{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline de Dataset No Supervisado ECG (Autoencoder LSTM)\n",
        "\n",
        "Este notebook crea un dataset específico para entrenamiento de autoencoders LSTM:\n",
        "\n",
        "**Características clave:**\n",
        "1. **Train**: Solo ECG normales (label == 0) - para entrenamiento del autoencoder\n",
        "2. **Val/Test**: Mezcla de normales y anómalos (con labels) - para evaluación\n",
        "3. **Guardado en disco**: Todo se guarda directamente sin cargar arrays grandes en memoria\n",
        "4. **Funciones de carga**: Funciones reutilizables para el notebook de entrenamiento\n",
        "\n",
        "**Diferencias con el dataset supervisado:**\n",
        "- Train solo contiene normales (el autoencoder aprende el patrón normal)\n",
        "- Val/Test mantienen labels para evaluación de detección de anomalías\n",
        "- No se balancea el dataset (mantenemos la distribución natural)\n",
        "\n",
        "**Pipeline:**\n",
        "1. Carga datos del pipeline supervisado (ya procesados)\n",
        "2. Separa en train (solo normales) y val/test (normales + anómalos)\n",
        "3. Guarda todo en `/data/Datos_no_supervisados/`\n",
        "4. Genera funciones de carga reutilizables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuración e Importaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CONFIGURACIÓN\n",
            "================================================================================\n",
            "✓ Directorio de datos supervisados: S:\\Proyecto final\\data\\Datos_supervisados\n",
            "✓ Directorio de salida: s:\\Proyecto final\\data\\Datos_no_supervisados\n",
            "✓ Proporción Val: 15%\n",
            "✓ Proporción Test: 15%\n",
            "✓ Random state: 42\n",
            "✓ Label normal: 0\n",
            "✓ Label anómalo: 1\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Recargar módulo si es necesario\n",
        "import importlib\n",
        "import supervised_ecg_pipeline\n",
        "importlib.reload(supervised_ecg_pipeline)\n",
        "\n",
        "# Importar módulos del pipeline supervisado (reutilizamos las funciones de procesamiento)\n",
        "from supervised_ecg_pipeline import (\n",
        "    OUTPUT_DIR as SUPERVISED_OUTPUT_DIR,\n",
        "    PTB_ROOT,\n",
        "    MIMIC_ROOT,\n",
        "    ensure_dir,\n",
        ")\n",
        "\n",
        "# ==================== CONFIGURACIÓN ====================\n",
        "# Directorios (usar la misma lógica que el pipeline supervisado)\n",
        "PROJECT_ROOT = Path.cwd().parent  # Asume que estamos en Books/\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "UNSUPERVISED_OUTPUT_DIR = DATA_DIR / \"Datos_no_supervisados\"\n",
        "\n",
        "# Rutas de datos de entrada (del pipeline supervisado)\n",
        "SUPERVISED_DATA_DIR = SUPERVISED_OUTPUT_DIR  # Usar los datos ya procesados\n",
        "\n",
        "# Parámetros de splits\n",
        "VAL_RATIO = 0.15  # 15% para validación\n",
        "TEST_RATIO = 0.15  # 15% para test\n",
        "# Train será el resto de los normales (después de separar val/test)\n",
        "RANDOM_STATE = 42  # Para reproducibilidad\n",
        "\n",
        "# Parámetros de etiquetado (mismos que el supervisado)\n",
        "LABEL_COL = \"label\"  # Nombre de la columna de etiquetas\n",
        "NORMAL_LABEL_VALUE = 0  # Valor que indica ECG normal\n",
        "ANOMALY_LABEL_VALUE = 1  # Valor que indica ECG anómalo\n",
        "\n",
        "# Crear directorio de salida\n",
        "ensure_dir(UNSUPERVISED_OUTPUT_DIR)\n",
        "ensure_dir(UNSUPERVISED_OUTPUT_DIR / \"numpy\")\n",
        "ensure_dir(UNSUPERVISED_OUTPUT_DIR / \"metadata\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURACIÓN\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"✓ Directorio de datos supervisados: {SUPERVISED_DATA_DIR}\")\n",
        "print(f\"✓ Directorio de salida: {UNSUPERVISED_OUTPUT_DIR}\")\n",
        "print(f\"✓ Proporción Val: {VAL_RATIO*100:.0f}%\")\n",
        "print(f\"✓ Proporción Test: {TEST_RATIO*100:.0f}%\")\n",
        "print(f\"✓ Random state: {RANDOM_STATE}\")\n",
        "print(f\"✓ Label normal: {NORMAL_LABEL_VALUE}\")\n",
        "print(f\"✓ Label anómalo: {ANOMALY_LABEL_VALUE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cargar Datos del Pipeline Supervisado\n",
        "\n",
        "**Nota**: Si los datos del pipeline supervisado aún no están procesados, primero ejecuta `build_supervised_ecg_dataset.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 1: Cargando datos del pipeline supervisado\n",
            "================================================================================\n",
            "\n",
            "  Cargando datos desde: S:\\Proyecto final\\data\\Datos_supervisados\n",
            "  Cargando arrays (memoria mapeada)...\n",
            "  Cargando metadata...\n",
            "\n",
            "✓ Datos cargados exitosamente (tiempo: 0.47s)\n",
            "\n",
            "  Datos supervisados cargados:\n",
            "    Train: 270668 registros (shape: (270668, 5000, 3))\n",
            "      Normales: 135334\n",
            "      Anómalos: 135334\n",
            "    Val: 58001 registros (shape: (58001, 5000, 3))\n",
            "      Normales: 29000\n",
            "      Anómalos: 29001\n",
            "    Test: 58001 registros (shape: (58001, 5000, 3))\n",
            "      Normales: 29001\n",
            "      Anómalos: 29000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PASO 1: Cargando datos del pipeline supervisado\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Rutas de los archivos guardados por el pipeline supervisado\n",
        "    numpy_dir = SUPERVISED_DATA_DIR / \"numpy\"\n",
        "    metadata_dir = SUPERVISED_DATA_DIR / \"metadata\"\n",
        "    \n",
        "    # Verificar que los archivos existan\n",
        "    required_files = [\n",
        "        numpy_dir / \"X_train.npy\",\n",
        "        numpy_dir / \"y_train.npy\",\n",
        "        numpy_dir / \"X_val.npy\",\n",
        "        numpy_dir / \"y_val.npy\",\n",
        "        numpy_dir / \"X_test.npy\",\n",
        "        numpy_dir / \"y_test.npy\",\n",
        "        metadata_dir / \"master_labels_full.csv\",\n",
        "    ]\n",
        "    \n",
        "    missing_files = [f for f in required_files if not f.exists()]\n",
        "    if missing_files:\n",
        "        print(f\"\\n✗ ERROR: Archivos faltantes del pipeline supervisado:\")\n",
        "        for f in missing_files:\n",
        "            print(f\"  - {f}\")\n",
        "        print(f\"\\n⚠ Por favor, ejecuta primero build_supervised_ecg_dataset.ipynb\")\n",
        "        raise FileNotFoundError(\"Datos supervisados no encontrados\")\n",
        "    \n",
        "    print(f\"\\n  Cargando datos desde: {SUPERVISED_DATA_DIR}\")\n",
        "    \n",
        "    # Cargar datos usando memoria mapeada para eficiencia\n",
        "    print(f\"  Cargando arrays (memoria mapeada)...\")\n",
        "    X_train_sup = np.load(numpy_dir / \"X_train.npy\", mmap_mode='r')\n",
        "    y_train_sup = np.load(numpy_dir / \"y_train.npy\")\n",
        "    X_val_sup = np.load(numpy_dir / \"X_val.npy\", mmap_mode='r')\n",
        "    y_val_sup = np.load(numpy_dir / \"y_val.npy\")\n",
        "    X_test_sup = np.load(numpy_dir / \"X_test.npy\", mmap_mode='r')\n",
        "    y_test_sup = np.load(numpy_dir / \"y_test.npy\")\n",
        "    \n",
        "    # Cargar metadata\n",
        "    print(f\"  Cargando metadata...\")\n",
        "    metadata_full = pd.read_csv(metadata_dir / \"master_labels_full.csv\")\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ Datos cargados exitosamente (tiempo: {elapsed:.2f}s)\")\n",
        "    print(f\"\\n  Datos supervisados cargados:\")\n",
        "    print(f\"    Train: {len(X_train_sup)} registros (shape: {X_train_sup.shape})\")\n",
        "    print(f\"      Normales: {(y_train_sup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "    print(f\"      Anómalos: {(y_train_sup == ANOMALY_LABEL_VALUE).sum()}\")\n",
        "    print(f\"    Val: {len(X_val_sup)} registros (shape: {X_val_sup.shape})\")\n",
        "    print(f\"      Normales: {(y_val_sup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "    print(f\"      Anómalos: {(y_val_sup == ANOMALY_LABEL_VALUE).sum()}\")\n",
        "    print(f\"    Test: {len(X_test_sup)} registros (shape: {X_test_sup.shape})\")\n",
        "    print(f\"      Normales: {(y_test_sup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "    print(f\"      Anómalos: {(y_test_sup == ANOMALY_LABEL_VALUE).sum()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR cargando datos: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Separar Datos para Entrenamiento No Supervisado\n",
        "\n",
        "**Estrategia:**\n",
        "- **Train**: Solo normales de train_sup (70% de los normales del train supervisado)\n",
        "- **Val**: Combinar val_sup completo (normales + anómalos) + 15% de normales del train_sup\n",
        "- **Test**: Usar test_sup completo (normales + anómalos) + 15% de normales del train_sup\n",
        "\n",
        "**Nota**: El train del supervisado ya está balanceado, así que tiene tanto normales como anómalos. Necesitamos extraer solo los normales para el entrenamiento no supervisado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 2: Separando datos para entrenamiento no supervisado\n",
            "================================================================================\n",
            "\n",
            "  Extrayendo normales del train supervisado...\n",
            "    Normales en train supervisado: 135334\n",
            "    Normales para train no-supervisado: 94733\n",
            "    Normales para val/test pool: 40601\n",
            "\n",
            "  Combinando datos para val (normales + anómalos)...\n",
            "    Val combinado: 78301 registros\n",
            "      Normales: 49300\n",
            "      Anómalos: 29001\n",
            "\n",
            "  Combinando datos para test (normales + anómalos)...\n",
            "    Test combinado: 78302 registros\n",
            "      Normales: 49302\n",
            "      Anómalos: 29000\n",
            "\n",
            "  Preparando train no-supervisado (solo normales)...\n",
            "    Train no-supervisado: 94733 registros\n",
            "      Todos normales: 94733\n",
            "\n",
            "✓ Separación completada (tiempo: 52.03s)\n",
            "\n",
            "  Resumen:\n",
            "    Train (solo normales): 94733 ((94733, 5000, 3))\n",
            "    Val (normales + anómalos): 78301 ((78301, 5000, 3))\n",
            "    Test (normales + anómalos): 78302 ((78302, 5000, 3))\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PASO 2: Separando datos para entrenamiento no supervisado\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # ===== 1. Extraer solo normales del train supervisado =====\n",
        "    print(f\"\\n  Extrayendo normales del train supervisado...\")\n",
        "    train_normal_mask = (y_train_sup == NORMAL_LABEL_VALUE)\n",
        "    train_normal_indices = np.where(train_normal_mask)[0]\n",
        "    \n",
        "    print(f\"    Normales en train supervisado: {len(train_normal_indices)}\")\n",
        "    \n",
        "    # Separar normales de train_sup en: train (70%) y el resto para val/test (30%)\n",
        "    # Primero separamos val/test del conjunto de normales, luego el resto va a train\n",
        "    n_normales_train_sup = len(train_normal_indices)\n",
        "    \n",
        "    # Calcular cuántos normales necesitamos para val/test\n",
        "    # Queremos que val y test tengan una proporción razonable de normales\n",
        "    # Usamos una proporción 70/30: 70% de normales para train, 30% para val/test\n",
        "    train_normals_ratio = 0.70\n",
        "    val_test_normals_ratio = 1.0 - train_normals_ratio\n",
        "    \n",
        "    # Separar normales en train (70%) y val_test_pool (30%)\n",
        "    sss_normales = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        test_size=val_test_normals_ratio,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    \n",
        "    # Crear array dummy de labels (todos son normales, pero necesitamos esto para el split)\n",
        "    y_normales_dummy = np.zeros(len(train_normal_indices), dtype=int)\n",
        "    \n",
        "    train_normal_idx_local, val_test_normal_idx_local = next(\n",
        "        sss_normales.split(np.arange(len(train_normal_indices)), y_normales_dummy)\n",
        "    )\n",
        "    \n",
        "    # Mapear índices locales a índices globales\n",
        "    train_normal_idx = train_normal_indices[train_normal_idx_local]\n",
        "    val_test_normal_idx = train_normal_indices[val_test_normal_idx_local]\n",
        "    \n",
        "    print(f\"    Normales para train no-supervisado: {len(train_normal_idx)}\")\n",
        "    print(f\"    Normales para val/test pool: {len(val_test_normal_idx)}\")\n",
        "    \n",
        "    # ===== 2. Combinar datos para val =====\n",
        "    print(f\"\\n  Combinando datos para val (normales + anómalos)...\")\n",
        "    # Val tendrá: val_sup completo + una parte de normales del train_sup\n",
        "    \n",
        "    # Separar val_test_normal_idx en val y test (50/50 del pool)\n",
        "    val_normal_idx_local, test_normal_idx_local = next(\n",
        "        StratifiedShuffleSplit(\n",
        "            n_splits=1,\n",
        "            test_size=0.5,\n",
        "            random_state=RANDOM_STATE\n",
        "        ).split(np.arange(len(val_test_normal_idx)), np.zeros(len(val_test_normal_idx), dtype=int))\n",
        "    )\n",
        "    \n",
        "    val_normal_from_train = val_test_normal_idx[val_normal_idx_local]\n",
        "    test_normal_from_train = val_test_normal_idx[test_normal_idx_local]\n",
        "    \n",
        "    # Combinar val: val_sup completo + normales adicionales del train_sup\n",
        "    # Primero cargamos val_sup en memoria (es pequeño)\n",
        "    X_val_combined_list = []\n",
        "    y_val_combined_list = []\n",
        "    \n",
        "    # Agregar val_sup completo\n",
        "    X_val_sup_array = np.array(X_val_sup)  # Cargar en memoria\n",
        "    X_val_combined_list.append(X_val_sup_array)\n",
        "    y_val_combined_list.append(y_val_sup)\n",
        "    \n",
        "    # Agregar normales adicionales del train_sup\n",
        "    X_val_normal_array = np.array(X_train_sup[val_normal_from_train])\n",
        "    X_val_combined_list.append(X_val_normal_array)\n",
        "    y_val_normal_array = y_train_sup[val_normal_from_train]\n",
        "    y_val_combined_list.append(y_val_normal_array)\n",
        "    \n",
        "    # Concatenar\n",
        "    X_val_unsup = np.concatenate(X_val_combined_list, axis=0)\n",
        "    y_val_unsup = np.concatenate(y_val_combined_list, axis=0)\n",
        "    \n",
        "    print(f\"    Val combinado: {len(X_val_unsup)} registros\")\n",
        "    print(f\"      Normales: {(y_val_unsup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "    print(f\"      Anómalos: {(y_val_unsup == ANOMALY_LABEL_VALUE).sum()}\")\n",
        "    \n",
        "    # ===== 3. Combinar datos para test =====\n",
        "    print(f\"\\n  Combinando datos para test (normales + anómalos)...\")\n",
        "    \n",
        "    X_test_combined_list = []\n",
        "    y_test_combined_list = []\n",
        "    \n",
        "    # Agregar test_sup completo\n",
        "    X_test_sup_array = np.array(X_test_sup)  # Cargar en memoria\n",
        "    X_test_combined_list.append(X_test_sup_array)\n",
        "    y_test_combined_list.append(y_test_sup)\n",
        "    \n",
        "    # Agregar normales adicionales del train_sup\n",
        "    X_test_normal_array = np.array(X_train_sup[test_normal_from_train])\n",
        "    X_test_combined_list.append(X_test_normal_array)\n",
        "    y_test_normal_array = y_train_sup[test_normal_from_train]\n",
        "    y_test_combined_list.append(y_test_normal_array)\n",
        "    \n",
        "    # Concatenar\n",
        "    X_test_unsup = np.concatenate(X_test_combined_list, axis=0)\n",
        "    y_test_unsup = np.concatenate(y_test_combined_list, axis=0)\n",
        "    \n",
        "    print(f\"    Test combinado: {len(X_test_unsup)} registros\")\n",
        "    print(f\"      Normales: {(y_test_unsup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "    print(f\"      Anómalos: {(y_test_unsup == ANOMALY_LABEL_VALUE).sum()}\")\n",
        "    \n",
        "    # ===== 4. Train no supervisado (solo normales) =====\n",
        "    print(f\"\\n  Preparando train no-supervisado (solo normales)...\")\n",
        "    \n",
        "    # Cargar train normales en memoria\n",
        "    X_train_unsup = np.array(X_train_sup[train_normal_idx])\n",
        "    y_train_unsup = y_train_sup[train_normal_idx].copy()  # Todos son normales\n",
        "    \n",
        "    print(f\"    Train no-supervisado: {len(X_train_unsup)} registros\")\n",
        "    print(f\"      Todos normales: {(y_train_unsup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "    \n",
        "    # Limpiar variables intermedias\n",
        "    del X_val_combined_list, y_val_combined_list, X_test_combined_list, y_test_combined_list\n",
        "    del X_val_sup_array, X_test_sup_array, X_val_normal_array, X_test_normal_array\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ Separación completada (tiempo: {elapsed:.2f}s)\")\n",
        "    print(f\"\\n  Resumen:\")\n",
        "    print(f\"    Train (solo normales): {len(X_train_unsup)} ({X_train_unsup.shape})\")\n",
        "    print(f\"    Val (normales + anómalos): {len(X_val_unsup)} ({X_val_unsup.shape})\")\n",
        "    print(f\"    Test (normales + anómalos): {len(X_test_unsup)} ({X_test_unsup.shape})\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR separando datos: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 3: Creando metadatos para cada split\n",
            "================================================================================\n",
            "\n",
            "  Creando metadata para train...\n",
            "    Train metadata: 94733 registros\n",
            "\n",
            "  Creando metadata para val...\n",
            "    Val metadata: 78301 registros\n",
            "      De val_sup: 58001\n",
            "      De train_sup (normales): 20300\n",
            "\n",
            "  Creando metadata para test...\n",
            "    Test metadata: 78302 registros\n",
            "      De test_sup: 58001\n",
            "      De train_sup (normales): 20301\n",
            "\n",
            "✓ Metadatos creados (tiempo: 0.20s)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PASO 3: Creando metadatos para cada split\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Cargar metadata completa del supervisado\n",
        "    metadata_train_sup = metadata_full[metadata_full['split'] == 'train'].copy().reset_index(drop=True)\n",
        "    metadata_val_sup = metadata_full[metadata_full['split'] == 'val'].copy().reset_index(drop=True)\n",
        "    metadata_test_sup = metadata_full[metadata_full['split'] == 'test'].copy().reset_index(drop=True)\n",
        "    \n",
        "    # ===== 1. Metadata para train no-supervisado =====\n",
        "    print(f\"\\n  Creando metadata para train...\")\n",
        "    metadata_train_unsup = metadata_train_sup.iloc[train_normal_idx].copy()\n",
        "    metadata_train_unsup.reset_index(drop=True, inplace=True)\n",
        "    metadata_train_unsup['split'] = 'train'\n",
        "    metadata_train_unsup['unsupervised_split'] = 'train_normal'\n",
        "    \n",
        "    print(f\"    Train metadata: {len(metadata_train_unsup)} registros\")\n",
        "    \n",
        "    # ===== 2. Metadata para val no-supervisado =====\n",
        "    print(f\"\\n  Creando metadata para val...\")\n",
        "    \n",
        "    # Metadata de val_sup\n",
        "    metadata_val_sup_copy = metadata_val_sup.copy()\n",
        "    metadata_val_sup_copy['unsupervised_split'] = 'val_mixed'\n",
        "    \n",
        "    # Metadata de normales adicionales del train_sup\n",
        "    metadata_val_normal = metadata_train_sup.iloc[val_normal_from_train].copy()\n",
        "    metadata_val_normal.reset_index(drop=True, inplace=True)\n",
        "    metadata_val_normal['split'] = 'val'\n",
        "    metadata_val_normal['unsupervised_split'] = 'val_normal_from_train'\n",
        "    \n",
        "    # Combinar\n",
        "    metadata_val_unsup = pd.concat([metadata_val_sup_copy, metadata_val_normal], ignore_index=True)\n",
        "    metadata_val_unsup['split'] = 'val'\n",
        "    \n",
        "    print(f\"    Val metadata: {len(metadata_val_unsup)} registros\")\n",
        "    print(f\"      De val_sup: {len(metadata_val_sup_copy)}\")\n",
        "    print(f\"      De train_sup (normales): {len(metadata_val_normal)}\")\n",
        "    \n",
        "    # ===== 3. Metadata para test no-supervisado =====\n",
        "    print(f\"\\n  Creando metadata para test...\")\n",
        "    \n",
        "    # Metadata de test_sup\n",
        "    metadata_test_sup_copy = metadata_test_sup.copy()\n",
        "    metadata_test_sup_copy['unsupervised_split'] = 'test_mixed'\n",
        "    \n",
        "    # Metadata de normales adicionales del train_sup\n",
        "    metadata_test_normal = metadata_train_sup.iloc[test_normal_from_train].copy()\n",
        "    metadata_test_normal.reset_index(drop=True, inplace=True)\n",
        "    metadata_test_normal['split'] = 'test'\n",
        "    metadata_test_normal['unsupervised_split'] = 'test_normal_from_train'\n",
        "    \n",
        "    # Combinar\n",
        "    metadata_test_unsup = pd.concat([metadata_test_sup_copy, metadata_test_normal], ignore_index=True)\n",
        "    metadata_test_unsup['split'] = 'test'\n",
        "    \n",
        "    print(f\"    Test metadata: {len(metadata_test_unsup)} registros\")\n",
        "    print(f\"      De test_sup: {len(metadata_test_sup_copy)}\")\n",
        "    print(f\"      De train_sup (normales): {len(metadata_test_normal)}\")\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ Metadatos creados (tiempo: {elapsed:.2f}s)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR creando metadatos: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Guardar Dataset en Disco\n",
        "\n",
        "**Importante**: Guardamos todo directamente en disco para evitar problemas de memoria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 4: Guardando dataset en disco\n",
            "================================================================================\n",
            "\n",
            "  Guardando arrays numpy...\n",
            "    Guardando X_train.npy ((94733, 5000, 3))...\n",
            "    ✓ X_train.npy guardado\n",
            "    Guardando y_train.npy ((94733,))...\n",
            "    ✓ y_train.npy guardado (todos son normales, pero lo guardamos por consistencia)\n",
            "    Guardando X_val.npy ((78301, 5000, 3))...\n",
            "    ✓ X_val.npy guardado\n",
            "    Guardando y_val.npy ((78301,))...\n",
            "    ✓ y_val.npy guardado\n",
            "    Guardando X_test.npy ((78302, 5000, 3))...\n",
            "    ✓ X_test.npy guardado\n",
            "    Guardando y_test.npy ((78302,))...\n",
            "    ✓ y_test.npy guardado\n",
            "\n",
            "  Guardando metadatos...\n",
            "    Guardando metadata_train.csv...\n",
            "    ✓ metadata_train.csv guardado\n",
            "    Guardando metadata_val.csv...\n",
            "    ✓ metadata_val.csv guardado\n",
            "    Guardando metadata_test.csv...\n",
            "    ✓ metadata_test.csv guardado\n",
            "    Guardando metadata_full.csv...\n",
            "    ✓ metadata_full.csv guardado\n",
            "    Guardando summary_stats.txt...\n",
            "    ✓ summary_stats.txt guardado\n",
            "\n",
            "✓ Dataset guardado exitosamente (tiempo: 77.52s)\n",
            "  Ubicación: s:\\Proyecto final\\data\\Datos_no_supervisados\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PASO 4: Guardando dataset en disco\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    numpy_dir = UNSUPERVISED_OUTPUT_DIR / \"numpy\"\n",
        "    metadata_dir = UNSUPERVISED_OUTPUT_DIR / \"metadata\"\n",
        "    \n",
        "    print(f\"\\n  Guardando arrays numpy...\")\n",
        "    \n",
        "    # Guardar train (solo normales)\n",
        "    print(f\"    Guardando X_train.npy ({X_train_unsup.shape})...\")\n",
        "    np.save(numpy_dir / \"X_train.npy\", X_train_unsup)\n",
        "    print(f\"    ✓ X_train.npy guardado\")\n",
        "    \n",
        "    print(f\"    Guardando y_train.npy ({y_train_unsup.shape})...\")\n",
        "    np.save(numpy_dir / \"y_train.npy\", y_train_unsup)\n",
        "    print(f\"    ✓ y_train.npy guardado (todos son normales, pero lo guardamos por consistencia)\")\n",
        "    \n",
        "    # Guardar val (normales + anómalos)\n",
        "    print(f\"    Guardando X_val.npy ({X_val_unsup.shape})...\")\n",
        "    np.save(numpy_dir / \"X_val.npy\", X_val_unsup)\n",
        "    print(f\"    ✓ X_val.npy guardado\")\n",
        "    \n",
        "    print(f\"    Guardando y_val.npy ({y_val_unsup.shape})...\")\n",
        "    np.save(numpy_dir / \"y_val.npy\", y_val_unsup)\n",
        "    print(f\"    ✓ y_val.npy guardado\")\n",
        "    \n",
        "    # Guardar test (normales + anómalos)\n",
        "    print(f\"    Guardando X_test.npy ({X_test_unsup.shape})...\")\n",
        "    np.save(numpy_dir / \"X_test.npy\", X_test_unsup)\n",
        "    print(f\"    ✓ X_test.npy guardado\")\n",
        "    \n",
        "    print(f\"    Guardando y_test.npy ({y_test_unsup.shape})...\")\n",
        "    np.save(numpy_dir / \"y_test.npy\", y_test_unsup)\n",
        "    print(f\"    ✓ y_test.npy guardado\")\n",
        "    \n",
        "    print(f\"\\n  Guardando metadatos...\")\n",
        "    \n",
        "    # Guardar metadata por split\n",
        "    print(f\"    Guardando metadata_train.csv...\")\n",
        "    metadata_train_unsup.to_csv(metadata_dir / \"metadata_train.csv\", index=False)\n",
        "    print(f\"    ✓ metadata_train.csv guardado\")\n",
        "    \n",
        "    print(f\"    Guardando metadata_val.csv...\")\n",
        "    metadata_val_unsup.to_csv(metadata_dir / \"metadata_val.csv\", index=False)\n",
        "    print(f\"    ✓ metadata_val.csv guardado\")\n",
        "    \n",
        "    print(f\"    Guardando metadata_test.csv...\")\n",
        "    metadata_test_unsup.to_csv(metadata_dir / \"metadata_test.csv\", index=False)\n",
        "    print(f\"    ✓ metadata_test.csv guardado\")\n",
        "    \n",
        "    # Guardar metadata combinada\n",
        "    print(f\"    Guardando metadata_full.csv...\")\n",
        "    metadata_full_unsup = pd.concat([\n",
        "        metadata_train_unsup,\n",
        "        metadata_val_unsup,\n",
        "        metadata_test_unsup,\n",
        "    ], ignore_index=True)\n",
        "    metadata_full_unsup.to_csv(metadata_dir / \"metadata_full.csv\", index=False)\n",
        "    print(f\"    ✓ metadata_full.csv guardado\")\n",
        "    \n",
        "    # Guardar resumen de estadísticas\n",
        "    print(f\"    Guardando summary_stats.txt...\")\n",
        "    summary_path = metadata_dir / \"summary_stats.txt\"\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(\"RESUMEN DEL DATASET NO SUPERVISADO\\n\")\n",
        "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "        f.write(f\"Directorio: {UNSUPERVISED_OUTPUT_DIR}\\n\\n\")\n",
        "        f.write(f\"TRAIN (solo normales):\\n\")\n",
        "        f.write(f\"  Total registros: {len(X_train_unsup)}\\n\")\n",
        "        f.write(f\"  Shape: {X_train_unsup.shape}\\n\")\n",
        "        f.write(f\"  Normales: {(y_train_unsup == NORMAL_LABEL_VALUE).sum()}\\n\")\n",
        "        f.write(f\"  Memoria: {X_train_unsup.nbytes / 1024**3:.2f} GB\\n\\n\")\n",
        "        f.write(f\"VAL (normales + anómalos):\\n\")\n",
        "        f.write(f\"  Total registros: {len(X_val_unsup)}\\n\")\n",
        "        f.write(f\"  Shape: {X_val_unsup.shape}\\n\")\n",
        "        f.write(f\"  Normales: {(y_val_unsup == NORMAL_LABEL_VALUE).sum()}\\n\")\n",
        "        f.write(f\"  Anómalos: {(y_val_unsup == ANOMALY_LABEL_VALUE).sum()}\\n\")\n",
        "        f.write(f\"  Memoria: {X_val_unsup.nbytes / 1024**3:.2f} GB\\n\\n\")\n",
        "        f.write(f\"TEST (normales + anómalos):\\n\")\n",
        "        f.write(f\"  Total registros: {len(X_test_unsup)}\\n\")\n",
        "        f.write(f\"  Shape: {X_test_unsup.shape}\\n\")\n",
        "        f.write(f\"  Normales: {(y_test_unsup == NORMAL_LABEL_VALUE).sum()}\\n\")\n",
        "        f.write(f\"  Anómalos: {(y_test_unsup == ANOMALY_LABEL_VALUE).sum()}\\n\")\n",
        "        f.write(f\"  Memoria: {X_test_unsup.nbytes / 1024**3:.2f} GB\\n\")\n",
        "    print(f\"    ✓ summary_stats.txt guardado\")\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ Dataset guardado exitosamente (tiempo: {elapsed:.2f}s)\")\n",
        "    print(f\"  Ubicación: {UNSUPERVISED_OUTPUT_DIR}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR guardando dataset: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Funciones de Carga Reutilizables\n",
        "\n",
        "Estas funciones pueden ser copiadas/usadas en el notebook de entrenamiento del autoencoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FUNCIONES DE CARGA DEFINIDAS\n",
            "================================================================================\n",
            "\n",
            "Funciones disponibles:\n",
            "  1. load_unsupervised_train_data(path_base, mmap_mode='r')\n",
            "     → Carga train (solo normales)\n",
            "  2. load_unsupervised_val_test_data(path_base, split='both', mmap_mode='r')\n",
            "     → Carga val y/o test (normales + anómalos)\n",
            "  3. load_unsupervised_metadata(path_base, split='all')\n",
            "     → Carga metadatos\n",
            "\n",
            "Ejemplo de uso en notebook de entrenamiento:\n",
            "  ```python\n",
            "  X_train, y_train = load_unsupervised_train_data('s:\\Proyecto final\\data\\Datos_no_supervisados')\n",
            "  ```\n"
          ]
        }
      ],
      "source": [
        "# ==================== FUNCIONES DE CARGA ====================\n",
        "\n",
        "def load_unsupervised_train_data(\n",
        "    path_base: str | Path,\n",
        "    mmap_mode: str = 'r'\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Carga los datos preprocesados de entrenamiento (solo normales) desde disco.\n",
        "    \n",
        "    Args:\n",
        "        path_base: Ruta base del dataset (ej: 'data/Datos_no_supervisados')\n",
        "        mmap_mode: Modo de mapeo de memoria ('r'=readonly, None=cargar completo)\n",
        "        \n",
        "    Returns:\n",
        "        Tupla (X_train, y_train)\n",
        "        - X_train: Array de señales normales [N, T, C]\n",
        "        - y_train: Array de etiquetas (todos son normales, pero se incluye por consistencia)\n",
        "    \"\"\"\n",
        "    path_base = Path(path_base)\n",
        "    numpy_dir = path_base / \"numpy\"\n",
        "    \n",
        "    X_train = np.load(numpy_dir / \"X_train.npy\", mmap_mode=mmap_mode)\n",
        "    y_train = np.load(numpy_dir / \"y_train.npy\")\n",
        "    \n",
        "    return X_train, y_train\n",
        "\n",
        "\n",
        "def load_unsupervised_val_test_data(\n",
        "    path_base: str | Path,\n",
        "    split: str = \"both\",  # 'val', 'test', o 'both'\n",
        "    mmap_mode: str = 'r'\n",
        "):\n",
        "    \"\"\"\n",
        "    Carga los datos preprocesados de validación y/o test (normales y anómalos).\n",
        "    \n",
        "    Args:\n",
        "        path_base: Ruta base del dataset (ej: 'data/Datos_no_supervisados')\n",
        "        split: Qué split cargar ('val', 'test', o 'both')\n",
        "        mmap_mode: Modo de mapeo de memoria ('r'=readonly, None=cargar completo)\n",
        "        \n",
        "    Returns:\n",
        "        Si split='both': Tupla (X_val, y_val, X_test, y_test)\n",
        "        Si split='val': Tupla (X_val, y_val)\n",
        "        Si split='test': Tupla (X_test, y_test)\n",
        "    \"\"\"\n",
        "    path_base = Path(path_base)\n",
        "    numpy_dir = path_base / \"numpy\"\n",
        "    \n",
        "    if split == \"both\":\n",
        "        X_val = np.load(numpy_dir / \"X_val.npy\", mmap_mode=mmap_mode)\n",
        "        y_val = np.load(numpy_dir / \"y_val.npy\")\n",
        "        X_test = np.load(numpy_dir / \"X_test.npy\", mmap_mode=mmap_mode)\n",
        "        y_test = np.load(numpy_dir / \"y_test.npy\")\n",
        "        return X_val, y_val, X_test, y_test\n",
        "    elif split == \"val\":\n",
        "        X_val = np.load(numpy_dir / \"X_val.npy\", mmap_mode=mmap_mode)\n",
        "        y_val = np.load(numpy_dir / \"y_val.npy\")\n",
        "        return X_val, y_val\n",
        "    elif split == \"test\":\n",
        "        X_test = np.load(numpy_dir / \"X_test.npy\", mmap_mode=mmap_mode)\n",
        "        y_test = np.load(numpy_dir / \"y_test.npy\")\n",
        "        return X_test, y_test\n",
        "    else:\n",
        "        raise ValueError(f\"split debe ser 'val', 'test' o 'both', recibido: {split}\")\n",
        "\n",
        "\n",
        "def load_unsupervised_metadata(\n",
        "    path_base: str | Path,\n",
        "    split: str = \"all\"  # 'train', 'val', 'test', o 'all'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carga los metadatos del dataset no supervisado.\n",
        "    \n",
        "    Args:\n",
        "        path_base: Ruta base del dataset (ej: 'data/Datos_no_supervisados')\n",
        "        split: Qué split cargar ('train', 'val', 'test', o 'all')\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame con metadatos\n",
        "    \"\"\"\n",
        "    path_base = Path(path_base)\n",
        "    metadata_dir = path_base / \"metadata\"\n",
        "    \n",
        "    if split == \"all\":\n",
        "        return pd.read_csv(metadata_dir / \"metadata_full.csv\")\n",
        "    elif split == \"train\":\n",
        "        return pd.read_csv(metadata_dir / \"metadata_train.csv\")\n",
        "    elif split == \"val\":\n",
        "        return pd.read_csv(metadata_dir / \"metadata_val.csv\")\n",
        "    elif split == \"test\":\n",
        "        return pd.read_csv(metadata_dir / \"metadata_test.csv\")\n",
        "    else:\n",
        "        raise ValueError(f\"split debe ser 'train', 'val', 'test' o 'all', recibido: {split}\")\n",
        "\n",
        "\n",
        "# Ejemplo de uso:\n",
        "print(\"=\" * 80)\n",
        "print(\"FUNCIONES DE CARGA DEFINIDAS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nFunciones disponibles:\")\n",
        "print(\"  1. load_unsupervised_train_data(path_base, mmap_mode='r')\")\n",
        "print(\"     → Carga train (solo normales)\")\n",
        "print(\"  2. load_unsupervised_val_test_data(path_base, split='both', mmap_mode='r')\")\n",
        "print(\"     → Carga val y/o test (normales + anómalos)\")\n",
        "print(\"  3. load_unsupervised_metadata(path_base, split='all')\")\n",
        "print(\"     → Carga metadatos\")\n",
        "print(f\"\\nEjemplo de uso en notebook de entrenamiento:\")\n",
        "print(f\"  ```python\")\n",
        "print(f\"  X_train, y_train = load_unsupervised_train_data('{UNSUPERVISED_OUTPUT_DIR}')\")\n",
        "print(f\"  ```\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Resumen Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RESUMEN FINAL\n",
            "================================================================================\n",
            "\n",
            "✓ Pipeline no supervisado completado exitosamente\n",
            "\n",
            "Dataset guardado en: s:\\Proyecto final\\data\\Datos_no_supervisados\n",
            "\n",
            "Estructura:\n",
            "  s:\\Proyecto final\\data\\Datos_no_supervisados/\n",
            "    numpy/\n",
            "      X_train.npy, y_train.npy  (solo normales)\n",
            "      X_val.npy, y_val.npy      (normales + anómalos)\n",
            "      X_test.npy, y_test.npy    (normales + anómalos)\n",
            "    metadata/\n",
            "      metadata_train.csv\n",
            "      metadata_val.csv\n",
            "      metadata_test.csv\n",
            "      metadata_full.csv\n",
            "      summary_stats.txt\n",
            "\n",
            "Estadísticas:\n",
            "  Train (solo normales):\n",
            "    Total: 94733 registros\n",
            "    Shape: (94733, 5000, 3) (N, T, C)\n",
            "      T=5000 muestras (10 segundos)\n",
            "      C=3 leads (II, V1, V5)\n",
            "    Normales: 94733\n",
            "    Memoria: 5.29 GB\n",
            "\n",
            "  Val (normales + anómalos):\n",
            "    Total: 78301 registros\n",
            "    Shape: (78301, 5000, 3) (N, T, C)\n",
            "    Normales: 49300\n",
            "    Anómalos: 29001\n",
            "    Memoria: 4.38 GB\n",
            "\n",
            "  Test (normales + anómalos):\n",
            "    Total: 78302 registros\n",
            "    Shape: (78302, 5000, 3) (N, T, C)\n",
            "    Normales: 49302\n",
            "    Anómalos: 29000\n",
            "    Memoria: 4.38 GB\n",
            "\n",
            "Funciones de carga disponibles:\n",
            "  - load_unsupervised_train_data(path_base)\n",
            "  - load_unsupervised_val_test_data(path_base, split='both')\n",
            "  - load_unsupervised_metadata(path_base, split='all')\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RESUMEN FINAL\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n✓ Pipeline no supervisado completado exitosamente\")\n",
        "print(f\"\\nDataset guardado en: {UNSUPERVISED_OUTPUT_DIR}\")\n",
        "print(f\"\\nEstructura:\")\n",
        "print(f\"  {UNSUPERVISED_OUTPUT_DIR}/\")\n",
        "print(f\"    numpy/\")\n",
        "print(f\"      X_train.npy, y_train.npy  (solo normales)\")\n",
        "print(f\"      X_val.npy, y_val.npy      (normales + anómalos)\")\n",
        "print(f\"      X_test.npy, y_test.npy    (normales + anómalos)\")\n",
        "print(f\"    metadata/\")\n",
        "print(f\"      metadata_train.csv\")\n",
        "print(f\"      metadata_val.csv\")\n",
        "print(f\"      metadata_test.csv\")\n",
        "print(f\"      metadata_full.csv\")\n",
        "print(f\"      summary_stats.txt\")\n",
        "print(f\"\\nEstadísticas:\")\n",
        "print(f\"  Train (solo normales):\")\n",
        "print(f\"    Total: {len(X_train_unsup)} registros\")\n",
        "print(f\"    Shape: {X_train_unsup.shape} (N, T, C)\")\n",
        "print(f\"      T={X_train_unsup.shape[1]} muestras (10 segundos)\")\n",
        "print(f\"      C={X_train_unsup.shape[2]} leads (II, V1, V5)\")\n",
        "print(f\"    Normales: {(y_train_unsup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "print(f\"    Memoria: {X_train_unsup.nbytes / 1024**3:.2f} GB\")\n",
        "print(f\"\\n  Val (normales + anómalos):\")\n",
        "print(f\"    Total: {len(X_val_unsup)} registros\")\n",
        "print(f\"    Shape: {X_val_unsup.shape} (N, T, C)\")\n",
        "print(f\"    Normales: {(y_val_unsup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "print(f\"    Anómalos: {(y_val_unsup == ANOMALY_LABEL_VALUE).sum()}\")\n",
        "print(f\"    Memoria: {X_val_unsup.nbytes / 1024**3:.2f} GB\")\n",
        "print(f\"\\n  Test (normales + anómalos):\")\n",
        "print(f\"    Total: {len(X_test_unsup)} registros\")\n",
        "print(f\"    Shape: {X_test_unsup.shape} (N, T, C)\")\n",
        "print(f\"    Normales: {(y_test_unsup == NORMAL_LABEL_VALUE).sum()}\")\n",
        "print(f\"    Anómalos: {(y_test_unsup == ANOMALY_LABEL_VALUE).sum()}\")\n",
        "print(f\"    Memoria: {X_test_unsup.nbytes / 1024**3:.2f} GB\")\n",
        "print(f\"\\nFunciones de carga disponibles:\")\n",
        "print(f\"  - load_unsupervised_train_data(path_base)\")\n",
        "print(f\"  - load_unsupervised_val_test_data(path_base, split='both')\")\n",
        "print(f\"  - load_unsupervised_metadata(path_base, split='all')\")\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
