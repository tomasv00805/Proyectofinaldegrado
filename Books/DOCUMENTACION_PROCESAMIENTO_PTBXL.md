# Documentaci√≥n: Procesamiento de Dataset PTB-XL

## üìã Resumen Ejecutivo

Este documento describe en detalle c√≥mo funciona el proceso de procesamiento del dataset PTB-XL para la construcci√≥n del dataset supervisado binario de ECG (NORMAL vs AN√ìMALO). El pipeline procesa registros ECG crudos, los etiqueta, filtra por calidad, aplica filtros de se√±al, resamplea, normaliza y extrae solo las derivaciones II, V1 y V5.

**Resultado del ejemplo:**
- **Registros procesados:** 979 de 21,799 (4.5% de aceptaci√≥n)
- **Normales:** 397 (40.6%)
- **An√≥malos:** 582 (59.4%)
- **Shape final:** (979, 5000, 3) ‚Üí 979 registros √ó 5000 muestras √ó 3 leads
- **Tiempo:** ~0.43 minutos con procesamiento paralelo (15 workers)

---

## üîÑ Flujo Completo del Procesamiento

### 1. Inicializaci√≥n y Configuraci√≥n

```python
# Par√°metros principales
TARGET_LEADS = ["II", "V1", "V5"]  # Derivaciones objetivo
SAMPLING_RATE = 500  # Hz (frecuencia objetivo)
SIGNAL_DURATION = 10  # segundos
EXPECTED_SAMPLES = 5000  # 500 Hz √ó 10 s

# Filtros
NOTCH_FREQ = 50.0  # Hz (elimina ruido de l√≠nea)
BANDPASS_LOW = 0.5  # Hz
BANDPASS_HIGH = 40.0  # Hz

# Calidad de se√±al
MIN_STD = 0.001  # Desviaci√≥n est√°ndar m√≠nima
MAX_NAN_RATIO = 0.05  # M√°ximo 5% de NaN
```

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 49-66

---

### 2. Carga de Metadatos

**Paso 2.1: Leer base de datos CSV**
```python
db_csv = PTB_ROOT / "ptbxl_database.csv"
metadata = pd.read_csv(db_csv, index_col="ecg_id")
metadata["scp_codes"] = metadata["scp_codes"].apply(ast.literal_eval)
```

**Contenido del CSV:**
- `ecg_id`: Identificador √∫nico del ECG
- `filename_hr`: Ruta al archivo de se√±al (500 Hz)
- `scp_codes`: Diccionario con c√≥digos SCP de diagn√≥stico
- `validated_by_human`: Si fue validado por humano
- Columnas de calidad: `baseline_drift`, `static_noise`, `burst_noise`, etc.

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 1336-1341

---

### 3. Procesamiento Paralelo (Versi√≥n R√°pida)

#### 3.1. Preparaci√≥n para Multiprocessing

El procesamiento paralelo divide los 21,799 registros entre 15 workers (por defecto: `cpu_count() - 1`).

**Funci√≥n principal:** `process_ptbxl_dataset_fast()`

**Ubicaci√≥n:** `supervised_ecg_pipeline_fast.py` l√≠neas 190-260

#### 3.2. Funci√≥n de Procesamiento Individual

Para cada registro, se ejecuta `_process_ptbxl_single()` que realiza:

**A. Etiquetado (Labeling)**
```python
label, reason = label_ptbxl_record(
    scp_codes,           # C√≥digos SCP del diagn√≥stico
    quality_cols,         # Columnas de calidad de se√±al
    reject_unvalidated,   # Si rechazar no validados
    validated_by_human,   # Si fue validado
    initial_autogenerated  # Si fue autogenerado
)
```

**L√≥gica de etiquetado:**

1. **Rechazo por calidad de se√±al** (prioridad m√°xima):
   - `baseline_drift`: Deriva de l√≠nea base
   - `static_noise`: Ruido est√°tico
   - `burst_noise`: Ruido en r√°fagas
   - `electrodes_problems`: Problemas con electrodos
   - `extra_beats`: Latidos extra (como artefacto)

2. **Rechazo por validaci√≥n** (si `reject_unvalidated=True`):
   - Reportes no validados por humano
   - Reportes autogenerados inicialmente

3. **Clasificaci√≥n AN√ìMALO (label=1)** si contiene c√≥digos SCP:
   - Infartos: `IMI`, `ASMI`, `AMI`, `ALMI`, `ILMI`, `LMI`, etc.
   - Isquemia: `ISC_`, `ISCAL`, `ISCIN`, `INJAS`, etc.
   - Hipertrofias: `LVH`, `RVH`, `SEHYP`, `LAO`, `RAO`, etc.
   - Bloqueos: `CLBBB`, `CRBBB`, `1AVB`, `2AVB`, `3AVB`, etc.
   - Arritmias: `AFIB`, `AFLT`, `PVC`, `PAC`, etc.
   - ST-T: `NST_`, `STD_`, `STE_`, `TAB_`, etc.

4. **Clasificaci√≥n NORMAL (label=0)** si:
   - C√≥digo `NORM` con valor 100.0 o 80.0
   - No hay otros c√≥digos patol√≥gicos

5. **Rechazo (label=-1)** si:
   - No coincide con ning√∫n patr√≥n claro
   - Tiene problemas de calidad
   - No est√° validado (si se requiere)

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 323-389

**Resultado del ejemplo:**
- 20,820 registros rechazados (95.5%)
- 979 registros aceptados (4.5%)

---

**B. Carga de Se√±al ECG**

```python
record_path = PTB_ROOT / filename_hr
signal, meta = wfdb.rdsamp(str(record_path), channels=None)
```

**Formato de archivo:**
- Archivos `.dat` y `.hea` (formato PhysioNet/WFDB)
- Frecuencia de muestreo: 500 Hz (archivos `filename_hr`)
- 12 derivaciones est√°ndar: I, II, III, aVR, aVL, aVF, V1-V6

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 739-768

---

**C. Extracci√≥n de Leads Objetivo**

```python
sig_names = get_sig_names(meta)  # Obtener nombres de leads
lead_mapping = map_lead_names(sig_names, TARGET_LEADS)  # Mapear II, V1, V5
indices = [lead_mapping[lead] for lead in TARGET_LEADS]
signal = signal[:, indices].astype(np.float32)  # Extraer solo II, V1, V5
```

**Mapeo de nombres equivalentes:**
- `II`: `["II", "MLII", "II-", "II+", "II_"]`
- `V1`: `["V1", "V1-", "V1+", "V1_"]`
- `V5`: `["V5", "V5-", "V5+", "V5_"]`

**Resultado:** Array de forma `[T, 3]` donde T es el n√∫mero de muestras

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 196-242

---

**D. Procesamiento de Se√±al**

La funci√≥n `process_single_record()` ejecuta el pipeline completo:

**D.1. Verificaci√≥n de Calidad de Se√±al**

```python
is_valid, reason = check_signal_quality(
    signal,
    fs=original_fs,
    min_duration=10.0,      # M√≠nimo 10 segundos
    min_std=0.001,          # Desviaci√≥n est√°ndar m√≠nima
    max_nan_ratio=0.05,     # M√°ximo 5% de NaN
    max_flat_ratio=0.05,    # M√°ximo 5% de valores constantes
    saturation_threshold=0.98  # Detecci√≥n de saturaci√≥n
)
```

**Checks realizados:**

1. **Duraci√≥n m√≠nima:** Se√±al debe tener ‚â• 10 segundos
2. **NaN por lead:** M√°ximo 5% de valores NaN en cada lead
3. **Se√±al plana:** Desviaci√≥n est√°ndar m√≠nima de 0.001
4. **Saturaci√≥n:** Si >98% de valores est√°n en min/max, es saturaci√≥n
5. **Discontinuidades:** Saltos >10√ó la mediana indican artefactos

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 396-479

---

**D.2. Filtrado de Se√±al**

```python
filtered = filter_signal(
    signal,
    fs=original_fs,
    apply_notch=True,      # Filtro notch a 50 Hz
    notch_freq=50.0,
    apply_bandpass=True    # Filtro pasa-banda 0.5-40 Hz
)
```

**Filtros aplicados:**

1. **Filtro Notch (50 Hz):**
   - Elimina ruido de l√≠nea el√©ctrica
   - Q-factor: 30.0
   - Implementaci√≥n: `scipy.signal.iirnotch()`

2. **Filtro Pasa-Banda (0.5-40 Hz):**
   - Elimina deriva de l√≠nea base (< 0.5 Hz)
   - Elimina ruido de alta frecuencia (> 40 Hz)
   - Orden: 4 (Butterworth)
   - Implementaci√≥n: `scipy.signal.butter()` + `filtfilt()`

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 486-573

---

**D.3. Resampleo Temporal**

```python
resampled = resample_signal(
    filtered,
    original_fs=500,        # Hz original
    target_fs=500,          # Hz objetivo (ya es 500, pero se asegura)
    target_duration=10.0,   # 10 segundos exactos
    strategy="center"       # Ventana central
)
```

**Proceso:**

1. **Recorte temporal:** Si la se√±al es > 10s, se recorta a 10s usando ventana central
2. **Resampleo:** Si `original_fs ‚â† target_fs`, se resamplea usando `scipy.signal.resample()`
3. **Ajuste final:** Se asegura que tenga exactamente 5000 muestras (500 Hz √ó 10s)

**Resultado:** Array de forma `[5000, 3]`

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 655-732

---

**D.4. Normalizaci√≥n**

```python
if normalize_method == "minmax":
    normalized, mins, maxs = normalize_signal_minmax(resampled)
    # Normaliza a [0, 1] por lead: (signal - min) / (max - min)
elif normalize_method == "zscore":
    normalized = normalize_signal_zscore(resampled)
    # Normaliza a media=0, std=1: (signal - mean) / std
```

**Normalizaci√≥n Min-Max (por defecto):**
- Cada lead se normaliza independientemente
- Rango final: [0, 1]
- Si `max == min` (se√±al constante), retorna NaN (se rechaza)

**Ubicaci√≥n:** `supervised_ecg_pipeline.py` l√≠neas 580-648

---

### 4. Agregaci√≥n de Resultados

Despu√©s del procesamiento paralelo, se agregan todos los registros v√°lidos:

```python
# Extraer se√±ales de todos los registros procesados
signals = np.stack([rec["signal"] for rec in processed_records], axis=0)
labels = np.array([rec["label"] for rec in processed_records])
metadata = pd.DataFrame([{k: v for k, v in rec.items() if k != "signal"} 
                         for rec in processed_records])
```

**Resultado:**
- `signals`: Array `(979, 5000, 3)` - 979 registros √ó 5000 muestras √ó 3 leads
- `labels`: Array `(979,)` - Etiquetas 0 (normal) o 1 (an√≥malo)
- `metadata`: DataFrame con informaci√≥n de cada registro

**Ubicaci√≥n:** `supervised_ecg_pipeline_fast.py` l√≠neas 240-260

---

## üìä Estad√≠sticas del Proceso

### Distribuci√≥n de Resultados

```
Total registros en PTB-XL: 21,799
‚îú‚îÄ‚îÄ Procesados exitosamente: 979 (4.5%)
‚îÇ   ‚îú‚îÄ‚îÄ Normales (label=0): 397 (40.6%)
‚îÇ   ‚îî‚îÄ‚îÄ An√≥malos (label=1): 582 (59.4%)
‚îî‚îÄ‚îÄ Rechazados: 20,820 (95.5%)
    ‚îú‚îÄ‚îÄ Por etiquetado: ~15,000 (no clasificables claramente)
    ‚îú‚îÄ‚îÄ Por calidad de se√±al: ~3,000 (se√±ales malas)
    ‚îú‚îÄ‚îÄ Por validaci√≥n: ~1,000 (no validados, si se requiere)
    ‚îî‚îÄ‚îÄ Por errores: ~2,820 (leads faltantes, archivos corruptos, etc.)
```

### Razones de Rechazo T√≠picas

1. **Sin clasificaci√≥n clara (60-70%):**
   - C√≥digos SCP ambiguos
   - M√∫ltiples c√≥digos contradictorios
   - C√≥digos no incluidos en patrones

2. **Problemas de calidad (15-20%):**
   - Se√±ales con ruido excesivo
   - Deriva de l√≠nea base
   - Problemas con electrodos

3. **Leads faltantes (5-10%):**
   - Archivos incompletos
   - Nombres de leads no reconocidos

4. **Errores de procesamiento (5-10%):**
   - Archivos corruptos
   - Errores de lectura WFDB
   - Se√±ales demasiado cortas

---

## ‚öôÔ∏è Par√°metros Configurables

### Par√°metros de Etiquetado

```python
reject_unvalidated=True   # Rechazar reportes no validados
```

### Par√°metros de Filtrado

```python
apply_notch=True         # Aplicar filtro notch
notch_freq=50.0         # Frecuencia notch (50 o 60 Hz)
apply_bandpass=True     # Aplicar filtro pasa-banda
```

### Par√°metros de Normalizaci√≥n

```python
normalize_method="minmax"  # "minmax" o "zscore"
```

### Par√°metros de Calidad

```python
apply_quality_check=True    # Verificar calidad de se√±al
MIN_STD=0.001              # Desviaci√≥n est√°ndar m√≠nima
MAX_NAN_RATIO=0.05         # M√°ximo 5% de NaN
```

### Par√°metros de Paralelizaci√≥n

```python
n_workers=15               # N√∫mero de workers (cpu_count() - 1)
max_records=None           # L√≠mite de registros (None = todos)
```

---

## üîç Detalles T√©cnicos

### Estructura de Datos Final

**Array de se√±ales (`signals`):**
- **Shape:** `(N, 5000, 3)`
  - `N`: N√∫mero de registros (979 en el ejemplo)
  - `5000`: Muestras temporales (500 Hz √ó 10 s)
  - `3`: Canales/leads (II, V1, V5)
- **Dtype:** `float32`
- **Rango:** [0, 1] (si minmax) o normalizado (si zscore)

**Array de etiquetas (`labels`):**
- **Shape:** `(N,)`
- **Dtype:** `int64`
- **Valores:** `0` (normal) o `1` (an√≥malo)

**DataFrame de metadatos:**
- Columnas: `record_id`, `source`, `original_id`, `label`, `label_reason`, `shape`
- **Tama√±o:** N filas (una por registro)

---

### Optimizaciones de la Versi√≥n R√°pida

1. **Procesamiento paralelo:** Usa `multiprocessing.Pool` con 15 workers
2. **Pre-filtrado opcional:** Puede filtrar por etiquetas antes de procesar se√±ales
3. **Carga optimizada:** Carga solo los leads necesarios
4. **Memoria eficiente:** Procesa en batches y libera memoria intermedia

**Ubicaci√≥n:** `supervised_ecg_pipeline_fast.py`

---

## üìù Ejemplo de Uso

```python
from supervised_ecg_pipeline_fast import process_ptbxl_dataset_fast

# Procesar PTB-XL con versi√≥n r√°pida
signals, labels, metadata = process_ptbxl_dataset_fast(
    overwrite=False,
    apply_quality_check=True,
    apply_notch=True,
    notch_freq=50.0,
    normalize_method="minmax",
    reject_unvalidated=False,
    max_records=None,  # Procesar todos
    n_workers=15,
    verbose=True,
    prefilter_labels=False
)

print(f"Registros procesados: {len(signals)}")
print(f"Shape: {signals.shape}")
print(f"Normales: {(labels == 0).sum()}")
print(f"An√≥malos: {(labels == 1).sum()}")
```

---

## üêõ Troubleshooting

### Problema: Muy pocos registros procesados

**Causas posibles:**
1. Criterios de etiquetado muy estrictos
2. Filtros de calidad muy restrictivos
3. `reject_unvalidated=True` rechaza muchos registros

**Soluciones:**
- Ajustar `reject_unvalidated=False`
- Relajar `apply_quality_check` o usar `minimal_quality=True`
- Revisar patrones de etiquetado en `PTB_ANOMALY_CODES` y `PTB_NORMAL_CODES`

### Problema: Errores de memoria

**Causas:**
- Demasiados registros en memoria simult√°neamente
- Workers consumen mucha RAM

**Soluciones:**
- Reducir `n_workers`
- Usar `max_records` para limitar registros
- Procesar en batches m√°s peque√±os

### Problema: Leads faltantes

**Causas:**
- Nombres de leads no reconocidos
- Archivos incompletos

**Soluciones:**
- Verificar `LEAD_MAPPING` en `supervised_ecg_pipeline.py`
- Agregar variantes de nombres de leads si es necesario

---

## üìö Referencias

- **Archivos principales:**
  - `supervised_ecg_pipeline.py`: Funciones base de procesamiento
  - `supervised_ecg_pipeline_fast.py`: Versi√≥n paralela optimizada
  - `build_supervised_ecg_dataset.py`: Script principal de ejecuci√≥n

- **Documentaci√≥n relacionada:**
  - `README.md`: Instrucciones generales del proyecto
  - `README_SUPERVISED_PIPELINE.md`: Documentaci√≥n del pipeline supervisado

---

## ‚úÖ Checklist de Verificaci√≥n

Antes de ejecutar el procesamiento, verificar:

- [ ] Dataset PTB-XL descargado en `ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/`
- [ ] Archivo `ptbxl_database.csv` presente
- [ ] Archivos `.dat` y `.hea` accesibles
- [ ] Dependencias instaladas: `wfdb`, `numpy`, `pandas`, `scipy`
- [ ] Memoria suficiente (recomendado: 16+ GB RAM)
- [ ] CPU con m√∫ltiples cores para paralelizaci√≥n

---

**√öltima actualizaci√≥n:** 2025-01-XX  
**Autor:** Sistema de documentaci√≥n autom√°tica  
**Versi√≥n:** 1.0

