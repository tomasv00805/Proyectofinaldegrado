{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü´Ä LSTM para Clasificaci√≥n Binaria Supervisada de ECG\n",
        "\n",
        "Este notebook implementa un **LSTM puro** para clasificaci√≥n binaria supervisada (ECG normal vs an√≥malo).\n",
        "\n",
        "**Caracter√≠sticas principales:**\n",
        "- Arquitectura LSTM pura (sin CNN) para captura de dependencias temporales\n",
        "- Entrenamiento supervisado con etiquetas (0=normal, 1=an√≥malo)\n",
        "- Datos preprocesados desde `Datos_supervisados/tensors_200hz` (archivos .pt)\n",
        "- Integraci√≥n con MLflow para tracking de experimentos\n",
        "- Orquestaci√≥n con Prefect 2.x\n",
        "- Soporte autom√°tico para GPU (RTX 5080 compatible)\n",
        "\n",
        "> ‚ö†Ô∏è **IMPORTANTE EN WINDOWS:** Ejecuta la celda de **Setup DLLs CUDA** (celda 2) **ANTES** de la celda de imports. Esto es necesario para que PyTorch pueda cargar las DLLs de CUDA correctamente.\n",
        "\n",
        "> ‚ñ∂Ô∏è **Instrucciones:** \n",
        "> 1. Ejecuta la celda de **Setup DLLs CUDA** primero\n",
        "> 2. Configura los par√°metros en la secci√≥n de **CONFIGURACI√ìN GENERAL**\n",
        "> 3. Ajusta la ruta `DATA_DIR` a tu carpeta de datos (debe apuntar a `Datos_supervisados/tensors_200hz`)\n",
        "> 4. Ejecuta todas las dem√°s celdas en orden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã √çndice\n",
        "\n",
        "1. **Setup CUDA y dependencias** - Configuraci√≥n de DLLs y librer√≠as\n",
        "2. **Configuraci√≥n general** - Imports, semillas, dispositivo, hiperpar√°metros\n",
        "3. **Carga y preparaci√≥n de datos** - Funciones para cargar desde `Datos_supervisados`\n",
        "4. **Definici√≥n del modelo LSTM** - Arquitectura del modelo de clasificaci√≥n\n",
        "5. **Funciones de entrenamiento y evaluaci√≥n** - Loops de entrenamiento y validaci√≥n\n",
        "6. **Integraci√≥n con MLflow** - Configuraci√≥n y logging\n",
        "7. **Orquestaci√≥n con Prefect** - Flujo principal con Prefect\n",
        "8. **Ejecuci√≥n del flujo completo** - Celda final para ejecutar todo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. ‚öôÔ∏è Setup CUDA y Dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: c:\\Python311\\python.exe\n",
            "Working dir: S:\\Proyecto final\\Books\n",
            "DLL directories a√±adidos:\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp\n",
            "‚è≥ Instalando mlflow>=2.16 ...\n",
            "‚è≥ Instalando prefect>=3 ...\n",
            "‚è≥ Instalando scikit-learn ...\n",
            "‚úî matplotlib ya instalado\n",
            "‚úî pandas ya instalado\n",
            "‚úî numpy ya instalado\n",
            "\n",
            "Torch info:\n",
            "  - torch_version: 2.10.0.dev20251121+cu128\n",
            "  - cuda_version: 12.8\n",
            "  - cuda_available: True\n",
            "GPU detectada: NVIDIA GeForce RTX 5080 | SM 120\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üîß Setup RTX 5080 ‚Äî dependencias + CUDA DLL\n",
        "# Ejecuta una sola vez (o tras actualizar drivers/librer√≠as)\n",
        "# ========================================\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from textwrap import dedent\n",
        "\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"Working dir: {Path.cwd().resolve()}\")\n",
        "\n",
        "# Rutas candidatas para DLLs de CUDA\n",
        "CUDA_CANDIDATES = [\n",
        "    os.environ.get(\"CUDA_PATH\"),\n",
        "    os.environ.get(\"CUDA_PATH_V12_8\"),\n",
        "    r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\",\n",
        "    r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\",\n",
        "    r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp\",\n",
        "    r\"C:\\Program Files\\NVIDIA\\CUDNN\",\n",
        "]\n",
        "\n",
        "# A√±adir rutas DLL en Windows (necesario antes de importar torch)\n",
        "added = []\n",
        "if hasattr(os, \"add_dll_directory\"):\n",
        "    for candidate in CUDA_CANDIDATES:\n",
        "        if not candidate:\n",
        "            continue\n",
        "        path = Path(candidate)\n",
        "        if path.is_dir():\n",
        "            try:\n",
        "                os.add_dll_directory(str(path))\n",
        "                added.append(str(path))\n",
        "            except (FileNotFoundError, OSError):\n",
        "                pass\n",
        "\n",
        "if added:\n",
        "    print(\"DLL directories a√±adidos:\")\n",
        "    for path in added:\n",
        "        print(f\"  - {path}\")\n",
        "\n",
        "# Instalar dependencias base si no est√°n instaladas\n",
        "BASE_PACKAGES = [\n",
        "    \"mlflow>=2.16\",\n",
        "    \"prefect>=3\",\n",
        "    \"scikit-learn\",\n",
        "    \"matplotlib\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "]\n",
        "\n",
        "def pip_install(spec: str) -> None:\n",
        "    module_name = spec.split(\"==\")[0].split(\"[\")[0].replace(\"-\", \"_\")\n",
        "    try:\n",
        "        __import__(module_name)\n",
        "        print(f\"‚úî {spec} ya instalado\")\n",
        "    except Exception:\n",
        "        print(f\"‚è≥ Instalando {spec} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", spec])\n",
        "\n",
        "for pkg in BASE_PACKAGES:\n",
        "    pip_install(pkg)\n",
        "\n",
        "# Comando para instalar PyTorch nightly con CUDA 12.8 (para RTX 5080)\n",
        "TORCH_INSTALL_CMD = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"pip\",\n",
        "    \"install\",\n",
        "    \"--upgrade\",\n",
        "    \"--pre\",\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"torchaudio\",\n",
        "    \"--index-url\",\n",
        "    \"https://download.pytorch.org/whl/nightly/cu128\",\n",
        "]\n",
        "\n",
        "def ensure_torch_cuda() -> \"tuple[object | None, dict]\":\n",
        "    \"\"\"Importa torch, o instala la nightly cu128 si hace falta.\"\"\"\n",
        "    info: dict[str, str | float | bool] = {}\n",
        "    try:\n",
        "        import torch  # type: ignore\n",
        "        info[\"torch_version\"] = getattr(torch, \"__version__\", \"desconocida\")\n",
        "        info[\"cuda_version\"] = getattr(getattr(torch, \"version\", object()), \"cuda\", \"desconocida\")\n",
        "        info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "        if \"cu128\" not in info[\"torch_version\"] and not str(info[\"cuda_version\"]).startswith(\"12.8\"):\n",
        "            raise RuntimeError(\n",
        "                f\"Build {info['torch_version']} no es cu128. Se reinstalar√° la nightly para RTX 5080.\"\n",
        "            )\n",
        "        return torch, info\n",
        "    except Exception as err:\n",
        "        print(\"‚ö†Ô∏è Torch no usable todav√≠a:\", err)\n",
        "        print(\"   Desinstalando PyTorch corrupto...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\", \"torchaudio\"])\n",
        "        print(\"   Instalando nightly cu128 desde PyTorch (puede tardar).\")\n",
        "        subprocess.check_call(TORCH_INSTALL_CMD)\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚ö†Ô∏è IMPORTANTE: PyTorch fue reinstalado.\")\n",
        "        print(\"   DEBES REINICIAR EL KERNEL DE JUPYTER ahora:\")\n",
        "        print(\"   Kernel ‚Üí Restart Kernel\")\n",
        "        print(\"   Luego ejecuta esta celda de nuevo.\")\n",
        "        print(\"=\"*60)\n",
        "        import importlib\n",
        "        import time\n",
        "        time.sleep(2)\n",
        "        importlib.invalidate_caches()\n",
        "        try:\n",
        "            import torch  # type: ignore\n",
        "            info[\"torch_version\"] = getattr(torch, \"__version__\", \"desconocida\")\n",
        "            info[\"cuda_version\"] = getattr(getattr(torch, \"version\", object()), \"cuda\", \"desconocida\")\n",
        "            info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "            return torch, info\n",
        "        except Exception as e2:\n",
        "            print(f\"\\n‚ùå No se pudo importar PyTorch despu√©s de reinstalar: {e2}\")\n",
        "            print(\"   Por favor, REINICIA EL KERNEL y ejecuta esta celda de nuevo.\")\n",
        "            raise RuntimeError(\"Reinicia el kernel de Jupyter y ejecuta esta celda de nuevo.\") from e2\n",
        "\n",
        "# Intentar importar/instalar PyTorch\n",
        "torch, torch_info = ensure_torch_cuda()\n",
        "\n",
        "print(\"\\nTorch info:\")\n",
        "for k, v in torch_info.items():\n",
        "    print(f\"  - {k}: {v}\")\n",
        "\n",
        "if torch_info.get(\"cuda_available\"):\n",
        "    try:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        cc = torch.cuda.get_device_properties(0)\n",
        "        print(f\"GPU detectada: {gpu_name} | SM {cc.major}{cc.minor}\")\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è CUDA disponible pero no se pudo consultar GPU:\", e)\n",
        "else:\n",
        "    print(dedent(\n",
        "        \"\"\"\n",
        "        ‚ö†Ô∏è CUDA sigue inactiva. Revisa drivers / reinicia kernel tras la instalaci√≥n.\n",
        "        Si el problema contin√∫a, ejecuta manualmente:\n",
        "          pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
        "        \"\"\"\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Todos los imports completados\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Imports y dependencias\n",
        "# ========================================\n",
        "# ‚ö†Ô∏è IMPORTANTE: Ejecuta la celda anterior (Setup DLLs) antes de esta celda\n",
        "# torch ya est√° importado en la celda anterior\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# torch ya est√° importado en la celda anterior, solo importamos los subm√≥dulos\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        ")\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from prefect import task, flow\n",
        "from prefect.tasks import NO_CACHE\n",
        "\n",
        "print(\"‚úì Todos los imports completados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. ‚öôÔ∏è Configuraci√≥n General\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Configuraci√≥n cargada:\n",
            "{\n",
            "  \"DATA_DIR\": \"..\\\\data\\\\Datos_supervisados\\\\tensors_200hz\",\n",
            "  \"EXPERIMENT_NAME\": \"ecg_lstm_supervisado\",\n",
            "  \"RUN_NAME\": \"lstm_ecg_v1\",\n",
            "  \"OUTPUT_DIR\": \"outputs\",\n",
            "  \"N_CHANNELS\": 3,\n",
            "  \"SEQ_LEN\": 2000,\n",
            "  \"INPUT_SIZE\": 3,\n",
            "  \"HIDDEN_SIZE\": 64,\n",
            "  \"NUM_LAYERS\": 2,\n",
            "  \"DROPOUT\": 0.2,\n",
            "  \"BIDIRECTIONAL\": false,\n",
            "  \"FC_UNITS\": 32,\n",
            "  \"FC_DROPOUT\": 0.3,\n",
            "  \"BATCH_SIZE\": 8,\n",
            "  \"LEARNING_RATE\": 0.001,\n",
            "  \"NUM_EPOCHS\": 50,\n",
            "  \"WEIGHT_DECAY\": 1e-05,\n",
            "  \"USE_SCHEDULER\": true,\n",
            "  \"SCHEDULER_PATIENCE\": 3,\n",
            "  \"SCHEDULER_FACTOR\": 0.5,\n",
            "  \"SCHEDULER_MIN_LR\": 1e-06,\n",
            "  \"SCHEDULER_MODE\": \"max\",\n",
            "  \"CLIP_GRAD_NORM\": 1.0,\n",
            "  \"SEED\": 42,\n",
            "  \"USE_CUDA\": true,\n",
            "  \"ENABLE_CUDNN_BENCHMARK\": true,\n",
            "  \"MLFLOW_TRACKING_URI\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CONFIGURACI√ìN GENERAL\n",
        "# ========================================\n",
        "\n",
        "# --- Rutas y nombres ---\n",
        "DATA_DIR = Path(\"../data/Datos_supervisados/tensors_200hz\")   # TODO: cambiar por la carpeta donde est√°n train/val/test\n",
        "EXPERIMENT_NAME = \"ecg_lstm_supervisado\"\n",
        "RUN_NAME = \"lstm_ecg_v1\"\n",
        "OUTPUT_DIR = Path(\"./outputs\")                 # Directorio para guardar artefactos\n",
        "\n",
        "# --- Datos de entrada ---\n",
        "N_CHANNELS = 3          # derivaciones de ECG (3 canales)\n",
        "SEQ_LEN = 2000          # TODO: timesteps por ejemplo (ej: 10 s a 200 Hz => 2000)\n",
        "INPUT_SIZE = N_CHANNELS # features por timestep (3 si uso las 3 derivaciones como canales)\n",
        "\n",
        "# --- Arquitectura LSTM ---\n",
        "HIDDEN_SIZE = 64        # neuronas en la LSTM\n",
        "NUM_LAYERS = 2          # cantidad de capas LSTM apiladas\n",
        "DROPOUT = 0.2           # dropout entre capas LSTM (si NUM_LAYERS > 1)\n",
        "BIDIRECTIONAL = False   # usar LSTM bidireccional o no\n",
        "\n",
        "# --- Capa totalmente conectada ---\n",
        "FC_UNITS = 32           # tama√±o de la capa lineal antes de la salida\n",
        "FC_DROPOUT = 0.3        # dropout en la parte fully-connected\n",
        "\n",
        "# --- Entrenamiento ---\n",
        "# ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è PROBLEMA DE MEMORIA ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
        "# Los archivos .pt cargan TODO en RAM (~6GB para X_train.pt).\n",
        "# Si tienes errores de memoria:\n",
        "#   1. Reduce BATCH_SIZE a 8, 4 o incluso 2\n",
        "#   2. O convierte a HDF5 usando: convert_pt_to_hdf5(DATA_DIR)\n",
        "#   3. O aumenta la RAM disponible (se recomienda al menos 16GB)\n",
        "BATCH_SIZE = 8          # ‚ö†Ô∏è MUY REDUCIDO para evitar problemas de memoria\n",
        "                        # Si a√∫n falla, reduce a 4 o 2\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 50\n",
        "WEIGHT_DECAY = 1e-5     # regularizaci√≥n L2 (0.0 si no se quiere)\n",
        "\n",
        "# --- Learning Rate Scheduler ---\n",
        "USE_SCHEDULER = True\n",
        "SCHEDULER_PATIENCE = 3\n",
        "SCHEDULER_FACTOR = 0.5\n",
        "SCHEDULER_MIN_LR = 1e-6\n",
        "SCHEDULER_MODE = 'max'  # Monitorear val_f1_macro (maximizar)\n",
        "\n",
        "# --- Gradient Clipping ---\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "\n",
        "# --- Otros ---\n",
        "SEED = 42\n",
        "USE_CUDA = True         # si hay GPU disponible, usarla\n",
        "\n",
        "# --- Optimizaciones GPU ---\n",
        "ENABLE_CUDNN_BENCHMARK = True\n",
        "\n",
        "# --- MLflow ---\n",
        "MLFLOW_TRACKING_URI = None  # None = usa el directorio local (sqlite:///mlflow.db)\n",
        "\n",
        "# Crear directorio de salida\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Diccionario de configuraci√≥n (para pasar a funciones)\n",
        "CONFIG = {\n",
        "    \"DATA_DIR\": DATA_DIR,\n",
        "    \"EXPERIMENT_NAME\": EXPERIMENT_NAME,\n",
        "    \"RUN_NAME\": RUN_NAME,\n",
        "    \"OUTPUT_DIR\": OUTPUT_DIR,\n",
        "    \"N_CHANNELS\": N_CHANNELS,\n",
        "    \"SEQ_LEN\": SEQ_LEN,\n",
        "    \"INPUT_SIZE\": INPUT_SIZE,\n",
        "    \"HIDDEN_SIZE\": HIDDEN_SIZE,\n",
        "    \"NUM_LAYERS\": NUM_LAYERS,\n",
        "    \"DROPOUT\": DROPOUT,\n",
        "    \"BIDIRECTIONAL\": BIDIRECTIONAL,\n",
        "    \"FC_UNITS\": FC_UNITS,\n",
        "    \"FC_DROPOUT\": FC_DROPOUT,\n",
        "    \"BATCH_SIZE\": BATCH_SIZE,\n",
        "    \"LEARNING_RATE\": LEARNING_RATE,\n",
        "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
        "    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
        "    \"USE_SCHEDULER\": USE_SCHEDULER,\n",
        "    \"SCHEDULER_PATIENCE\": SCHEDULER_PATIENCE,\n",
        "    \"SCHEDULER_FACTOR\": SCHEDULER_FACTOR,\n",
        "    \"SCHEDULER_MIN_LR\": SCHEDULER_MIN_LR,\n",
        "    \"SCHEDULER_MODE\": SCHEDULER_MODE,\n",
        "    \"CLIP_GRAD_NORM\": CLIP_GRAD_NORM,\n",
        "    \"SEED\": SEED,\n",
        "    \"USE_CUDA\": USE_CUDA,\n",
        "    \"ENABLE_CUDNN_BENCHMARK\": ENABLE_CUDNN_BENCHMARK,\n",
        "    \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_URI,\n",
        "}\n",
        "\n",
        "print(\"‚úì Configuraci√≥n cargada:\")\n",
        "print(json.dumps({k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()}, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì cuDNN Benchmark: Habilitado\n",
            "‚úì Semilla fijada: 42\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Configuraci√≥n de semillas aleatorias y optimizaciones GPU\n",
        "# ========================================\n",
        "def set_seed_everywhere(seed: int = 42, enable_cudnn_benchmark: bool = True) -> None:\n",
        "    \"\"\"Fija semillas para reproducibilidad y optimiza GPU.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = False  # ‚¨ÜÔ∏è Cambiado a False para mejor rendimiento\n",
        "        torch.backends.cudnn.benchmark = enable_cudnn_benchmark  # ‚¨ÜÔ∏è NUEVO: Acelera entrenamiento\n",
        "        # Limpiar cach√© de GPU\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"‚úì cuDNN Benchmark: {'Habilitado' if enable_cudnn_benchmark else 'Deshabilitado'}\")\n",
        "\n",
        "set_seed_everywhere(SEED, enable_cudnn_benchmark=CONFIG.get(\"ENABLE_CUDNN_BENCHMARK\", True))\n",
        "print(f\"‚úì Semilla fijada: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì GPU detectada: NVIDIA GeForce RTX 5080\n",
            "  CUDA Version: 12.8\n",
            "  PyTorch Version: 2.10.0.dev20251121+cu128\n",
            "Dispositivo seleccionado: cuda\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Configuraci√≥n de dispositivo (GPU/CPU)\n",
        "# ========================================\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Detecta y configura el dispositivo (GPU si est√° disponible).\"\"\"\n",
        "    if USE_CUDA and torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"‚úì GPU detectada: {gpu_name}\")\n",
        "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"  PyTorch Version: {torch.__version__}\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"‚ö† GPU no disponible, usando CPU\")\n",
        "    return device\n",
        "\n",
        "DEVICE = get_device()\n",
        "print(f\"Dispositivo seleccionado: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. üìÇ Carga y Preparaci√≥n de Datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ö†Ô∏è SOLUCI√ìN AL PROBLEMA DE MEMORIA\n",
        "\n",
        "Si tienes errores de memoria al cargar los datos, **convierte los archivos .pt a HDF5**:\n",
        "\n",
        "```python\n",
        "# Ejecuta esto ANTES de entrenar:\n",
        "convert_pt_to_hdf5(Path(\"../data/Datos_supervisados/tensors_200hz\"))\n",
        "```\n",
        "\n",
        "Esto crear√° archivos .h5 en `tensors_200hz/hdf5/` que permiten acceso aleatorio sin cargar todo en memoria.\n",
        "\n",
        "El c√≥digo detectar√° autom√°ticamente los archivos HDF5 y los usar√° si est√°n disponibles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ‚ö†Ô∏è SOLUCI√ìN ALTERNATIVA: Convertir archivos .pt a HDF5 para acceso eficiente\n",
        "# ========================================\n",
        "# Si tienes problemas de memoria con archivos .pt, puedes convertir a HDF5\n",
        "# que permite acceso aleatorio sin cargar todo en memoria.\n",
        "#\n",
        "# Para usar esta funci√≥n, ejecuta:\n",
        "#   convert_pt_to_hdf5(Path(\"../data/Datos_supervisados/tensors_200hz\"))\n",
        "#\n",
        "# Luego modifica DATA_DIR para apuntar a la carpeta con archivos .h5\n",
        "\n",
        "def convert_pt_to_hdf5(data_dir: Path, output_dir: Path = None):\n",
        "    \"\"\"\n",
        "    Convierte archivos .pt a HDF5 para acceso eficiente en memoria.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Ruta a la carpeta con archivos .pt\n",
        "        output_dir: Ruta donde guardar archivos .h5 (por defecto: data_dir / \"hdf5\")\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import h5py\n",
        "    except ImportError:\n",
        "        print(\"‚ùå h5py no est√° instalado. Inst√°lalo con: pip install h5py\")\n",
        "        return\n",
        "    \n",
        "    if output_dir is None:\n",
        "        output_dir = data_dir / \"hdf5\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(\"üîÑ Convirtiendo archivos .pt a HDF5...\")\n",
        "    print(f\"   Entrada: {data_dir}\")\n",
        "    print(f\"   Salida: {output_dir}\")\n",
        "    \n",
        "    files_to_convert = [\"X_train.pt\", \"X_val.pt\", \"X_test.pt\", \"y_train.pt\", \"y_val.pt\", \"y_test.pt\"]\n",
        "    \n",
        "    for filename in files_to_convert:\n",
        "        pt_file = data_dir / filename\n",
        "        h5_file = output_dir / filename.replace(\".pt\", \".h5\")\n",
        "        \n",
        "        if not pt_file.exists():\n",
        "            print(f\"  ‚ö†Ô∏è {filename} no existe, saltando...\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"  Convirtiendo {filename}...\")\n",
        "        \n",
        "        # Cargar tensor\n",
        "        tensor = torch.load(pt_file, map_location='cpu')\n",
        "        \n",
        "        # Guardar como HDF5\n",
        "        with h5py.File(h5_file, 'w') as f:\n",
        "            f.create_dataset('data', data=tensor.numpy(), compression='gzip', compression_opts=4)\n",
        "            f.attrs['shape'] = tensor.shape\n",
        "            f.attrs['dtype'] = str(tensor.dtype)\n",
        "        \n",
        "        print(f\"  ‚úì {filename} convertido a {h5_file.name}\")\n",
        "        del tensor\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    \n",
        "    print(\"‚úÖ Conversi√≥n completada!\")\n",
        "    print(f\"   Ahora puedes usar los archivos .h5 en lugar de .pt\")\n",
        "    print(f\"   Modifica el c√≥digo para usar HDF5Dataset en lugar de LazyTensorDataset\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Dataset HDF5 que permite acceso aleatorio sin cargar todo en memoria\n",
        "# ========================================\n",
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset que lee desde archivos HDF5 con acceso aleatorio eficiente.\n",
        "    NO carga todo en memoria, solo lee lo necesario cuando se accede.\n",
        "    Esta es la soluci√≥n recomendada para archivos grandes.\n",
        "    \"\"\"\n",
        "    def __init__(self, X_file: Path, y_file: Path):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X_file: Ruta al archivo .h5 con los features\n",
        "            y_file: Ruta al archivo .h5 con las etiquetas (o .pt si a√∫n no est√° convertido)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import h5py\n",
        "        except ImportError:\n",
        "            raise ImportError(\"h5py no est√° instalado. Inst√°lalo con: pip install h5py\")\n",
        "        \n",
        "        self.X_file = Path(X_file)\n",
        "        self.y_file = Path(y_file)\n",
        "        \n",
        "        # Cargar etiquetas (son peque√±as, pueden ser .pt o .h5)\n",
        "        if self.y_file.suffix == '.h5':\n",
        "            with h5py.File(self.y_file, 'r') as f:\n",
        "                self.y = torch.from_numpy(f['data'][:])\n",
        "        else:\n",
        "            # Si a√∫n es .pt, cargarlo normalmente (es peque√±o)\n",
        "            self.y = torch.load(self.y_file, map_location='cpu')\n",
        "        \n",
        "        self.len = len(self.y)\n",
        "        \n",
        "        # Abrir archivo HDF5 (solo para lectura, no carga todo)\n",
        "        self.h5_file = h5py.File(self.X_file, 'r')\n",
        "        self.X_dataset = self.h5_file['data']\n",
        "        \n",
        "        print(f\"  ‚úì {self.X_file.name} abierto en modo HDF5 (acceso aleatorio, sin cargar todo en memoria)\")\n",
        "        print(f\"     Shape: {self.X_dataset.shape}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Leer solo la muestra necesaria (muy eficiente)\n",
        "        x = torch.from_numpy(self.X_dataset[idx])\n",
        "        y = self.y[idx]\n",
        "        return x, y\n",
        "    \n",
        "    def __del__(self):\n",
        "        # Cerrar archivo HDF5 cuando se destruye el dataset\n",
        "        if hasattr(self, 'h5_file'):\n",
        "            self.h5_file.close()\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Dataset personalizado que carga datos bajo demanda (lazy loading)\n",
        "# ‚ö†Ô∏è ADVERTENCIA: Carga TODO en memoria. Usa HDF5Dataset si es posible.\n",
        "# ========================================\n",
        "class LazyTensorDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset que carga tensors desde archivos .pt bajo demanda (lazy loading).\n",
        "    Los datos se cargan solo cuando se accede por primera vez y se mantienen en CPU.\n",
        "    Los datos se transfieren a GPU solo cuando el DataLoader los necesita.\n",
        "    Esto reduce el uso de memoria inicial.\n",
        "    \n",
        "    ‚ö†Ô∏è ADVERTENCIA: Cuando se carga X por primera vez, se carga TODO el archivo en memoria.\n",
        "    Para archivos muy grandes (>4GB), esto puede causar problemas de memoria.\n",
        "    Considera reducir BATCH_SIZE o usar un formato m√°s eficiente (HDF5, etc.).\n",
        "    \"\"\"\n",
        "    def __init__(self, X_file: Path, y_file: Path, load_immediately: bool = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X_file: Ruta al archivo .pt con los features\n",
        "            y_file: Ruta al archivo .pt con las etiquetas\n",
        "            load_immediately: Si True, carga X inmediatamente (por defecto False para lazy loading)\n",
        "        \"\"\"\n",
        "        self.X_file = Path(X_file)\n",
        "        self.y_file = Path(y_file)\n",
        "        \n",
        "        # Cargar solo las etiquetas primero (son peque√±as)\n",
        "        print(f\"  Cargando {self.y_file.name}...\")\n",
        "        self.y = torch.load(self.y_file, map_location='cpu')\n",
        "        self.len = len(self.y)\n",
        "        \n",
        "        # Inicializar X como None - se cargar√° bajo demanda\n",
        "        self.X = None\n",
        "        self._X_loaded = False\n",
        "        \n",
        "        # Si se solicita carga inmediata, cargar ahora\n",
        "        if load_immediately:\n",
        "            self._load_X()\n",
        "    \n",
        "    def _load_X(self):\n",
        "        \"\"\"Carga el tensor X solo cuando se necesita por primera vez.\"\"\"\n",
        "        if not self._X_loaded:\n",
        "            file_size_mb = self.X_file.stat().st_size / (1024 * 1024)\n",
        "            file_size_gb = file_size_mb / 1024\n",
        "            print(f\"  ‚ö†Ô∏è Cargando {self.X_file.name} en CPU...\")\n",
        "            print(f\"     Tama√±o del archivo: ~{file_size_gb:.2f} GB ({file_size_mb:.1f} MB)\")\n",
        "            print(f\"     ‚ö†Ô∏è ADVERTENCIA: Esto cargar√° TODO el archivo en RAM.\")\n",
        "            print(f\"     Esto puede tardar y usar RAM. Los datos se mantendr√°n en CPU.\")\n",
        "            \n",
        "            try:\n",
        "                # Intentar cargar con gesti√≥n de memoria\n",
        "                import gc\n",
        "                gc.collect()  # Limpiar memoria antes de cargar\n",
        "                \n",
        "                # Mostrar memoria disponible si es posible\n",
        "                try:\n",
        "                    import psutil\n",
        "                    mem = psutil.virtual_memory()\n",
        "                    print(f\"     RAM disponible: {mem.available / (1024**3):.2f} GB / {mem.total / (1024**3):.2f} GB\")\n",
        "                    if mem.available < file_size_gb * 1024 * 1024 * 1024 * 1.5:  # Necesitamos ~1.5x el tama√±o del archivo\n",
        "                        print(f\"     ‚ö†Ô∏è ADVERTENCIA: Puede que no haya suficiente RAM disponible!\")\n",
        "                except ImportError:\n",
        "                    pass  # psutil no est√° instalado, continuar\n",
        "                \n",
        "                self.X = torch.load(self.X_file, map_location='cpu')\n",
        "                self._X_loaded = True\n",
        "                \n",
        "                print(f\"  ‚úì {self.X_file.name} cargado en CPU (shape: {self.X.shape})\")\n",
        "                print(f\"     Memoria usada: ~{self.X.element_size() * self.X.nelement() / (1024**3):.2f} GB\")\n",
        "            except RuntimeError as e:\n",
        "                if \"not enough memory\" in str(e):\n",
        "                    print(f\"\\n  ‚ùå ERROR CR√çTICO: No hay suficiente memoria para cargar {self.X_file.name}\")\n",
        "                    print(f\"     Tama√±o requerido: ~{file_size_gb:.2f} GB\")\n",
        "                    print(f\"\\n  üîß SOLUCIONES:\")\n",
        "                    print(f\"     1. ‚ö° Reduce BATCH_SIZE a 4 u 8 en la configuraci√≥n (actual: {CONFIG.get('BATCH_SIZE', 'N/A')})\")\n",
        "                    print(f\"     2. üíæ Cierra otras aplicaciones que usen mucha memoria\")\n",
        "                    print(f\"     3. üöÄ Aumenta la RAM disponible (se recomienda al menos {file_size_gb * 1.5:.1f} GB)\")\n",
        "                    print(f\"     4. üì¶ Convierte a HDF5 para acceso eficiente:\")\n",
        "                    print(f\"        - Ejecuta: convert_pt_to_hdf5(Path('{self.X_file.parent}'))\")\n",
        "                    print(f\"        - Luego modifica el c√≥digo para usar HDF5Dataset\")\n",
        "                    print(f\"\\n  üí° NOTA: Los archivos .pt cargan TODO en memoria. HDF5 permite acceso aleatorio.\")\n",
        "                    raise RuntimeError(\n",
        "                        f\"No hay suficiente memoria para cargar {self.X_file.name} (~{file_size_gb:.2f} GB). \"\n",
        "                        f\"Reduce BATCH_SIZE o convierte a HDF5.\"\n",
        "                    ) from e\n",
        "                else:\n",
        "                    raise\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Cargar X si a√∫n no est√° cargado (lazy loading)\n",
        "        if not self._X_loaded:\n",
        "            self._load_X()\n",
        "        \n",
        "        # Retornar datos desde CPU (el DataLoader los transferir√° a GPU si es necesario)\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Funci√≥n para cargar informaci√≥n de datos sin cargar X (solo etiquetas)\n",
        "# ========================================\n",
        "def load_tensor_data_info(\n",
        "    data_dir: Path,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Carga solo la informaci√≥n de los datos (estad√≠sticas de etiquetas) sin cargar X.\n",
        "    Usa la configuraci√≥n para las formas de X.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Ruta a la carpeta tensors_200hz\n",
        "        \n",
        "    Returns:\n",
        "        Diccionario con informaci√≥n de los datos\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"üìÇ CARGANDO INFORMACI√ìN DE DATOS DESDE tensors_200hz\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Directorio: {data_dir.resolve()}\")\n",
        "    \n",
        "    # Cargar solo las etiquetas (son peque√±as) para obtener estad√≠sticas\n",
        "    print(\"\\n‚è≥ Cargando informaci√≥n de datos (solo etiquetas, sin cargar X)...\")\n",
        "    y_train = torch.load(data_dir / \"y_train.pt\", map_location='cpu')\n",
        "    y_val = torch.load(data_dir / \"y_val.pt\", map_location='cpu')\n",
        "    y_test = torch.load(data_dir / \"y_test.pt\", map_location='cpu')\n",
        "    \n",
        "    # Usar la configuraci√≥n para las formas de X (no cargamos X para ahorrar memoria)\n",
        "    # La forma esperada es (n_samples, SEQ_LEN, N_CHANNELS)\n",
        "    X_shape_train = (len(y_train), CONFIG[\"SEQ_LEN\"], CONFIG[\"N_CHANNELS\"])\n",
        "    X_shape_val = (len(y_val), CONFIG[\"SEQ_LEN\"], CONFIG[\"N_CHANNELS\"])\n",
        "    X_shape_test = (len(y_test), CONFIG[\"SEQ_LEN\"], CONFIG[\"N_CHANNELS\"])\n",
        "    sample_shape = (CONFIG[\"SEQ_LEN\"], CONFIG[\"N_CHANNELS\"])\n",
        "    \n",
        "    # Informaci√≥n\n",
        "    info = {\n",
        "        \"X_shape_train\": X_shape_train,\n",
        "        \"X_shape_val\": X_shape_val,\n",
        "        \"X_shape_test\": X_shape_test,\n",
        "        \"sample_shape\": sample_shape,\n",
        "        \"n_train\": len(y_train),\n",
        "        \"n_val\": len(y_val),\n",
        "        \"n_test\": len(y_test),\n",
        "        \"y_train_normales\": (y_train == 0).sum().item(),\n",
        "        \"y_train_anomalos\": (y_train == 1).sum().item(),\n",
        "        \"y_val_normales\": (y_val == 0).sum().item(),\n",
        "        \"y_val_anomalos\": (y_val == 1).sum().item(),\n",
        "        \"y_test_normales\": (y_test == 0).sum().item(),\n",
        "        \"y_test_anomalos\": (y_test == 1).sum().item(),\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n‚úì Informaci√≥n de datos (usando configuraci√≥n para formas de X):\")\n",
        "    print(f\"  X_train: {X_shape_train} | y_train: ({info['n_train']},) (normales: {info['y_train_normales']}, an√≥malos: {info['y_train_anomalos']})\")\n",
        "    print(f\"  X_val:   {X_shape_val} | y_val:   ({info['n_val']},) (normales: {info['y_val_normales']}, an√≥malos: {info['y_val_anomalos']})\")\n",
        "    print(f\"  X_test:  {X_shape_test} | y_test:  ({info['n_test']},) (normales: {info['y_test_normales']}, an√≥malos: {info['y_test_anomalos']})\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return info\n",
        "\n",
        "\n",
        "def create_dataloaders_from_files(\n",
        "    data_dir: Path,\n",
        "    batch_size: int,\n",
        "    shuffle_train: bool = True,\n",
        "    load_train_immediately: bool = False,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Crea DataLoaders desde archivos .pt usando Dataset personalizado.\n",
        "    Los datos se mantienen en CPU y se transfieren a GPU solo cuando se necesitan.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Ruta a la carpeta tensors_200hz\n",
        "        batch_size: Tama√±o del batch\n",
        "        shuffle_train: Si True, mezcla los datos de entrenamiento\n",
        "        load_train_immediately: Si True, carga train inmediatamente (por defecto False para lazy loading)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple con (train_loader, val_loader, test_loader, y_val_np, y_test_np)\n",
        "    \"\"\"\n",
        "    print(\"\\nüì¶ Creando datasets (los datos se cargar√°n en CPU bajo demanda)...\")\n",
        "    print(\"‚ö†Ô∏è  NOTA: Los datos se cargar√°n cuando el DataLoader comience a iterar.\")\n",
        "    print(\"    Si tienes problemas de memoria, reduce BATCH_SIZE en la configuraci√≥n.\")\n",
        "    \n",
        "    # Crear datasets (cargar√°n los datos en CPU bajo demanda)\n",
        "    # Val y test son m√°s peque√±os, as√≠ que podemos cargarlos inmediatamente si queremos\n",
        "    train_dataset = LazyTensorDataset(\n",
        "        data_dir / \"X_train.pt\",\n",
        "        data_dir / \"y_train.pt\",\n",
        "        load_immediately=load_train_immediately,\n",
        "    )\n",
        "    val_dataset = LazyTensorDataset(\n",
        "        data_dir / \"X_val.pt\",\n",
        "        data_dir / \"y_val.pt\",\n",
        "        load_immediately=False,  # Val es m√°s peque√±o, pero a√∫n as√≠ lazy loading\n",
        "    )\n",
        "    test_dataset = LazyTensorDataset(\n",
        "        data_dir / \"X_test.pt\",\n",
        "        data_dir / \"y_test.pt\",\n",
        "        load_immediately=False,  # Test es m√°s peque√±o, pero a√∫n as√≠ lazy loading\n",
        "    )\n",
        "    \n",
        "    # Cargar etiquetas para m√©tricas (son peque√±as)\n",
        "    y_val = torch.load(data_dir / \"y_val.pt\", map_location='cpu')\n",
        "    y_test = torch.load(data_dir / \"y_test.pt\", map_location='cpu')\n",
        "    y_val_np = y_val.cpu().numpy()\n",
        "    y_test_np = y_test.cpu().numpy()\n",
        "    \n",
        "    # Crear dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle_train,\n",
        "        num_workers=0,  # 0 para Windows\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úì DataLoaders creados:\")\n",
        "    print(f\"  Train: {len(train_loader)} batches ({len(train_dataset)} muestras)\")\n",
        "    print(f\"  Val:   {len(val_loader)} batches ({len(val_dataset)} muestras)\")\n",
        "    print(f\"  Test:  {len(test_loader)} batches ({len(test_dataset)} muestras)\")\n",
        "    \n",
        "    return train_loader, val_loader, test_loader, y_val_np, y_test_np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Crear DataLoaders\n",
        "# ‚¨ÜÔ∏è SIMPLIFICADO: Ahora recibe datasets directamente\n",
        "# ========================================\n",
        "def create_dataloaders(\n",
        "    train_dataset: Dataset,\n",
        "    val_dataset: Dataset,\n",
        "    test_dataset: Dataset,\n",
        "    batch_size: int = None,  # ‚¨ÜÔ∏è Si es None, usa todo el dataset\n",
        "    shuffle_train: bool = True,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    \"\"\"\n",
        "    Crea DataLoaders para train, val y test.\n",
        "    Si batch_size es None, procesa todo el dataset de una vez (sin batches).\n",
        "    \n",
        "    Args:\n",
        "        train_dataset, val_dataset, test_dataset: Datasets de PyTorch\n",
        "        batch_size: Tama√±o de batch (None = todo el dataset de una vez)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple con (train_loader, val_loader, test_loader)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Si batch_size es None, usar todo el dataset (sin batches)\n",
        "    train_batch_size = len(train_dataset) if batch_size is None else batch_size\n",
        "    val_batch_size = len(val_dataset) if batch_size is None else batch_size\n",
        "    test_batch_size = len(test_dataset) if batch_size is None else batch_size\n",
        "    \n",
        "    # Crear dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=shuffle_train,\n",
        "        num_workers=0,  # 0 para Windows\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=val_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úì DataLoaders creados:\")\n",
        "    if batch_size is None:\n",
        "        print(f\"  Train: 1 batch ({len(train_dataset)} muestras) - SIN DIVISI√ìN EN BATCHES\")\n",
        "        print(f\"  Val:   1 batch ({len(val_dataset)} muestras) - SIN DIVISI√ìN EN BATCHES\")\n",
        "        print(f\"  Test:  1 batch ({len(test_dataset)} muestras) - SIN DIVISI√ìN EN BATCHES\")\n",
        "    else:\n",
        "        print(f\"  Train: {len(train_loader)} batches ({len(train_dataset)} muestras)\")\n",
        "        print(f\"  Val:   {len(val_loader)} batches ({len(val_dataset)} muestras)\")\n",
        "        print(f\"  Test:  {len(test_loader)} batches ({len(test_dataset)} muestras)\")\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. üß† Definici√≥n del Modelo LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Clase LSTM para Clasificaci√≥n Binaria\n",
        "# ========================================\n",
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM puro para clasificaci√≥n binaria de series temporales (ECG normal vs an√≥malo).\n",
        "    \n",
        "    Arquitectura:\n",
        "    - LSTM apilado (m√∫ltiples capas, opcionalmente bidireccional)\n",
        "    - Capa fully connected con dropout\n",
        "    - Salida binaria (sigmoid)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "        bidirectional: bool,\n",
        "        fc_units: int,\n",
        "        fc_dropout: float,\n",
        "    ):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        \n",
        "        # Tama√±o de salida de LSTM (doble si es bidireccional)\n",
        "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        \n",
        "        # Capa fully connected\n",
        "        self.fc1 = nn.Linear(lstm_output_size, fc_units)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(fc_dropout)\n",
        "        \n",
        "        # Capa de salida (binaria)\n",
        "        self.fc2 = nn.Linear(fc_units, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Tensor de forma (batch_size, seq_len, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor de forma (batch_size,) con probabilidades\n",
        "        \"\"\"\n",
        "        # LSTM\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        \n",
        "        # Usar el √∫ltimo hidden state de la √∫ltima capa\n",
        "        # Si es bidireccional, concatenar forward y backward\n",
        "        if self.bidirectional:\n",
        "            # hidden shape: (num_layers * 2, batch_size, hidden_size)\n",
        "            # Tomar la √∫ltima capa: forward y backward\n",
        "            forward_hidden = hidden[-2]  # (batch_size, hidden_size)\n",
        "            backward_hidden = hidden[-1]  # (batch_size, hidden_size)\n",
        "            last_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)  # (batch_size, hidden_size * 2)\n",
        "        else:\n",
        "            # hidden shape: (num_layers, batch_size, hidden_size)\n",
        "            last_hidden = hidden[-1]  # (batch_size, hidden_size)\n",
        "        \n",
        "        # Fully connected\n",
        "        out = self.fc1(last_hidden)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout_fc(out)\n",
        "        \n",
        "        # Salida binaria\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        return out.squeeze(-1)  # (batch_size,)\n",
        "    \n",
        "    def predict_proba(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Devuelve probabilidades (mismo que forward).\"\"\"\n",
        "        return self.forward(x)\n",
        "    \n",
        "    def predict(self, x: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
        "        \"\"\"Devuelve predicciones binarias.\"\"\"\n",
        "        proba = self.forward(x)\n",
        "        return (proba > threshold).long()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Instanciar modelo\n",
        "# ========================================\n",
        "def create_model(config: Dict) -> LSTMClassifier:\n",
        "    \"\"\"Crea e instancia el modelo LSTM.\"\"\"\n",
        "    model = LSTMClassifier(\n",
        "        input_size=config[\"INPUT_SIZE\"],\n",
        "        hidden_size=config[\"HIDDEN_SIZE\"],\n",
        "        num_layers=config[\"NUM_LAYERS\"],\n",
        "        dropout=config[\"DROPOUT\"],\n",
        "        bidirectional=config[\"BIDIRECTIONAL\"],\n",
        "        fc_units=config[\"FC_UNITS\"],\n",
        "        fc_dropout=config[\"FC_DROPOUT\"],\n",
        "    )\n",
        "    \n",
        "    # Contar par√°metros\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"‚úì Modelo creado:\")\n",
        "    print(f\"  Par√°metros totales: {total_params:,} ({total_params / 1e6:.2f}M)\")\n",
        "    print(f\"  Par√°metros entrenables: {trainable_params:,}\")\n",
        "    print(f\"  LSTM: {config['NUM_LAYERS']} capas, hidden_size={config['HIDDEN_SIZE']}, bidirectional={config['BIDIRECTIONAL']}\")\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n de entrenamiento por √©poca\n",
        "# ========================================\n",
        "def train_one_epoch(\n",
        "    model: LSTMClassifier,\n",
        "    train_loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    clip_grad_norm: Optional[float] = None,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Entrena el modelo por una √©poca.\n",
        "    \n",
        "    Returns:\n",
        "        Tupla con (loss_promedio, accuracy_promedio)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x = batch_x.to(device, non_blocking=True)\n",
        "        batch_y = batch_y.to(device, non_blocking=True).float()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping (antes de optimizer.step)\n",
        "        if clip_grad_norm is not None and clip_grad_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        # Acumular m√©tricas\n",
        "        total_loss += loss.item() * batch_x.size(0)\n",
        "        predictions = (outputs > 0.5).long()\n",
        "        correct += (predictions == batch_y.long()).sum().item()\n",
        "        total += batch_x.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / total if total > 0 else 0.0\n",
        "    avg_accuracy = correct / total if total > 0 else 0.0\n",
        "    \n",
        "    return avg_loss, avg_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n de evaluaci√≥n\n",
        "# ========================================\n",
        "def evaluate(\n",
        "    model: LSTMClassifier,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> Tuple[float, float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Eval√∫a el modelo en un dataloader.\n",
        "    \n",
        "    Returns:\n",
        "        Tupla con (loss, accuracy, y_true, y_pred)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x = batch_x.to(device, non_blocking=True)  # ‚¨ÜÔ∏è non_blocking para mejor rendimiento\n",
        "            batch_y = batch_y.to(device, non_blocking=True).float()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            # Acumular m√©tricas\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "            predictions = (outputs > 0.5).long()\n",
        "            correct += (predictions == batch_y.long()).sum().item()\n",
        "            total += batch_x.size(0)\n",
        "            \n",
        "            # Guardar predicciones y etiquetas\n",
        "            all_preds.append(predictions.cpu().numpy())\n",
        "            all_labels.append(batch_y.long().cpu().numpy())\n",
        "    \n",
        "    avg_loss = total_loss / total if total > 0 else 0.0\n",
        "    avg_accuracy = correct / total if total > 0 else 0.0\n",
        "    \n",
        "    y_true = np.concatenate(all_labels)\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    \n",
        "    return avg_loss, avg_accuracy, y_true, y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n para calcular m√©tricas completas\n",
        "# ========================================\n",
        "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calcula m√©tricas completas de clasificaci√≥n.\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con todas las m√©tricas\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    \n",
        "    # Calcular m√©tricas por clase usando classification_report\n",
        "    report = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"normal\", \"anomalo\"],\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "    \n",
        "    # M√©tricas para clase normal (0)\n",
        "    metrics_normal = report.get(\"normal\", {})\n",
        "    precision_normal = metrics_normal.get(\"precision\", 0.0)\n",
        "    recall_normal = metrics_normal.get(\"recall\", 0.0)\n",
        "    f1_normal = metrics_normal.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # M√©tricas para clase an√≥mala (1)\n",
        "    metrics_anom = report.get(\"anomalo\", {})\n",
        "    precision_anom = metrics_anom.get(\"precision\", 0.0)\n",
        "    recall_anom = metrics_anom.get(\"recall\", 0.0)\n",
        "    f1_anom = metrics_anom.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # M√©tricas generales (macro avg)\n",
        "    macro_avg = report.get(\"macro avg\", {})\n",
        "    precision_macro = macro_avg.get(\"precision\", 0.0)\n",
        "    recall_macro = macro_avg.get(\"recall\", 0.0)\n",
        "    f1_macro = macro_avg.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / max(1, tn + fp)  # TNR\n",
        "    sensitivity = tp / max(1, tp + fn)   # TPR (recall de clase an√≥mala)\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"specificity\": specificity,\n",
        "        \"sensitivity\": sensitivity,\n",
        "        \"precision_normal\": precision_normal,\n",
        "        \"recall_normal\": recall_normal,\n",
        "        \"f1_normal\": f1_normal,\n",
        "        \"precision_anom\": precision_anom,\n",
        "        \"recall_anom\": recall_anom,\n",
        "        \"f1_anom\": f1_anom,\n",
        "        \"precision_macro\": precision_macro,\n",
        "        \"recall_macro\": recall_macro,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"confusion_matrix\": cm,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. üìä Integraci√≥n con MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Configuraci√≥n de MLflow\n",
        "# ========================================\n",
        "def setup_mlflow(config: Dict) -> str:\n",
        "    \"\"\"\n",
        "    Configura MLflow y crea/obtiene el experimento.\n",
        "    \n",
        "    Returns:\n",
        "        ID del experimento\n",
        "    \"\"\"\n",
        "    # Configurar tracking URI\n",
        "    if config.get(\"MLFLOW_TRACKING_URI\") is not None:\n",
        "        mlflow.set_tracking_uri(config[\"MLFLOW_TRACKING_URI\"])\n",
        "    else:\n",
        "        # Usar sqlite en el directorio padre\n",
        "        PARENT_DIR = Path.cwd().parent.resolve()\n",
        "        TRACKING_DB = (PARENT_DIR / \"mlflow.db\").resolve()\n",
        "        mlflow.set_tracking_uri(f\"sqlite:///{TRACKING_DB.as_posix()}\")\n",
        "        print(f\"‚úì MLflow tracking URI: sqlite:///{TRACKING_DB.as_posix()}\")\n",
        "    \n",
        "    # Crear o obtener experimento\n",
        "    experiment_name = config[\"EXPERIMENT_NAME\"]\n",
        "    \n",
        "    try:\n",
        "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "        if experiment is None:\n",
        "            # Crear directorio de artefactos\n",
        "            PARENT_DIR = Path.cwd().parent.resolve()\n",
        "            ARTIFACT_ROOT = (PARENT_DIR / \"mlflow_artifacts\").resolve()\n",
        "            ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "            experiment_id = mlflow.create_experiment(experiment_name, artifact_location=ARTIFACT_ROOT.as_uri())\n",
        "            print(f\"‚úì Experimento MLflow creado: {experiment_name} (ID: {experiment_id})\")\n",
        "            print(f\"  Artifact root: {ARTIFACT_ROOT.as_uri()}\")\n",
        "        else:\n",
        "            experiment_id = experiment.experiment_id\n",
        "            print(f\"‚úì Experimento MLflow existente: {experiment_name} (ID: {experiment_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error al configurar MLflow: {e}\")\n",
        "        experiment_id = mlflow.set_experiment(experiment_name)\n",
        "    \n",
        "    return experiment_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n para guardar matriz de confusi√≥n como artefacto\n",
        "# ========================================\n",
        "def save_confusion_matrix(\n",
        "    cm: np.ndarray,\n",
        "    output_dir: Path,\n",
        "    tag: str,\n",
        ") -> Tuple[Path, Path]:\n",
        "    \"\"\"\n",
        "    Guarda la matriz de confusi√≥n como PNG y CSV.\n",
        "    \n",
        "    Returns:\n",
        "        Tupla con rutas (png_path, csv_path)\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Guardar como CSV\n",
        "    csv_path = output_dir / f\"confusion_matrix_{tag}.csv\"\n",
        "    df_cm = pd.DataFrame(cm, index=[\"Normal\", \"An√≥malo\"], columns=[\"Normal\", \"An√≥malo\"])\n",
        "    df_cm.to_csv(csv_path)\n",
        "    \n",
        "    # Guardar como PNG\n",
        "    png_path = output_dir / f\"confusion_matrix_{tag}.png\"\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    \n",
        "    # Etiquetas\n",
        "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]))\n",
        "    ax.set_xticklabels([\"Normal\", \"An√≥malo\"])\n",
        "    ax.set_yticklabels([\"Normal\", \"An√≥malo\"])\n",
        "    ax.set_xlabel(\"Predicci√≥n\")\n",
        "    ax.set_ylabel(\"Real\")\n",
        "    ax.set_title(f\"Matriz de Confusi√≥n - {tag.upper()}\")\n",
        "    \n",
        "    # A√±adir valores en las celdas\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(\n",
        "                j, i, f\"{cm[i, j]}\",\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
        "            )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(png_path, dpi=150)\n",
        "    plt.close()\n",
        "    \n",
        "    return png_path, csv_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n para guardar gr√°ficos de curvas de entrenamiento\n",
        "# ========================================\n",
        "def save_training_curves(\n",
        "    train_losses: List[float],\n",
        "    train_accuracies: List[float],\n",
        "    val_losses: List[float],\n",
        "    val_f1_scores: List[float],\n",
        "    output_dir: Path,\n",
        "    learning_rates: Optional[List[float]] = None,  # ‚¨ÜÔ∏è NUEVO: Curva de LR\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Guarda gr√°ficos de curvas de entrenamiento.\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo PNG guardado\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Usar 2x3 si hay learning rates, sino 2x2\n",
        "    if learning_rates:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    else:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\")\n",
        "    axes[0, 0].plot(epochs, val_losses, label=\"Val Loss\", color=\"red\")\n",
        "    axes[0, 0].set_xlabel(\"√âpoca\")\n",
        "    axes[0, 0].set_ylabel(\"Loss (BCE)\")\n",
        "    axes[0, 0].set_title(\"Loss de Entrenamiento y Validaci√≥n\")\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[0, 1].plot(epochs, train_accuracies, label=\"Train Accuracy\", color=\"blue\")\n",
        "    axes[0, 1].set_xlabel(\"√âpoca\")\n",
        "    axes[0, 1].set_ylabel(\"Accuracy\")\n",
        "    axes[0, 1].set_title(\"Accuracy de Entrenamiento\")\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    # F1 Score en validaci√≥n\n",
        "    if val_f1_scores:\n",
        "        axes[1, 0].plot(epochs, val_f1_scores, label=\"Val F1 (macro)\", color=\"red\")\n",
        "        axes[1, 0].set_xlabel(\"√âpoca\")\n",
        "        axes[1, 0].set_ylabel(\"F1-Score\")\n",
        "        axes[1, 0].set_title(\"F1-Score en Validaci√≥n\")\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].legend()\n",
        "    \n",
        "    # Learning Rate (si est√° disponible)\n",
        "    if learning_rates:\n",
        "        axes[0, 2].plot(epochs, learning_rates, label=\"Learning Rate\", color=\"green\")\n",
        "        axes[0, 2].set_xlabel(\"√âpoca\")\n",
        "        axes[0, 2].set_ylabel(\"Learning Rate\")\n",
        "        axes[0, 2].set_title(\"Learning Rate durante Entrenamiento\")\n",
        "        axes[0, 2].set_yscale('log')  # Escala logar√≠tmica para mejor visualizaci√≥n\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "        axes[0, 2].legend()\n",
        "        \n",
        "        # Comparaci√≥n Train vs Val Accuracy (necesitar√≠amos val_accuracies, pero no las tenemos)\n",
        "        axes[1, 1].plot(epochs, train_accuracies, label=\"Train Accuracy\", color=\"blue\", alpha=0.7)\n",
        "        axes[1, 1].set_xlabel(\"√âpoca\")\n",
        "        axes[1, 1].set_ylabel(\"Accuracy\")\n",
        "        axes[1, 1].set_title(\"Train Accuracy\")\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        axes[1, 1].legend()\n",
        "        \n",
        "        # Loss comparaci√≥n\n",
        "        axes[1, 2].plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", alpha=0.7)\n",
        "        axes[1, 2].plot(epochs, val_losses, label=\"Val Loss\", color=\"red\", alpha=0.7)\n",
        "        axes[1, 2].set_xlabel(\"√âpoca\")\n",
        "        axes[1, 2].set_ylabel(\"Loss\")\n",
        "        axes[1, 2].set_title(\"Train vs Val Loss\")\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "        axes[1, 2].legend()\n",
        "    else:\n",
        "        # Comparaci√≥n Train vs Val Accuracy (si no hay LR)\n",
        "        axes[1, 1].plot(epochs, train_accuracies, label=\"Train Accuracy\", color=\"blue\", alpha=0.7)\n",
        "        axes[1, 1].set_xlabel(\"√âpoca\")\n",
        "        axes[1, 1].set_ylabel(\"Accuracy\")\n",
        "        axes[1, 1].set_title(\"Accuracy de Entrenamiento\")\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        axes[1, 1].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    png_path = output_dir / \"training_curves.png\"\n",
        "    plt.savefig(png_path, dpi=150)\n",
        "    plt.close()\n",
        "    \n",
        "    return png_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. ü™Ñ Orquestaci√≥n con Prefect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Tarea Prefect: Cargar datos\n",
        "# ========================================\n",
        "@task(name=\"load_data\", log_prints=True, cache_policy=NO_CACHE)\n",
        "def task_load_data(config: Dict):\n",
        "    \"\"\"Tarea Prefect para cargar datos desde tensors_200hz.\"\"\"\n",
        "    print(\"üìÇ Cargando datos...\")\n",
        "    \n",
        "    # Cargar informaci√≥n de datos primero (para verificar formas)\n",
        "    load_tensor_data_info(config[\"DATA_DIR\"])\n",
        "    \n",
        "    # Crear DataLoaders usando Dataset personalizado (mantiene datos en CPU)\n",
        "    train_loader, val_loader, test_loader, y_val_np, y_test_np = create_dataloaders_from_files(\n",
        "        config[\"DATA_DIR\"],\n",
        "        batch_size=config[\"BATCH_SIZE\"],\n",
        "        shuffle_train=True,\n",
        "    )\n",
        "    \n",
        "    print(\"‚úì Datos cargados y preparados\")\n",
        "    return train_loader, val_loader, test_loader, y_val_np, y_test_np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Tarea Prefect: Entrenar modelo\n",
        "# ========================================\n",
        "@task(name=\"train_model\", log_prints=True, cache_policy=NO_CACHE)\n",
        "def task_train_model(\n",
        "    model: LSTMClassifier,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    y_val: np.ndarray,\n",
        "    config: Dict,\n",
        "    device: torch.device,\n",
        "    experiment_id: str,\n",
        "):\n",
        "    \"\"\"Tarea Prefect para entrenar el modelo.\"\"\"\n",
        "    print(\"üèãÔ∏è Iniciando entrenamiento...\")\n",
        "    print(f\"  üìä Verificando DataLoaders...\")\n",
        "    print(f\"    Train: {len(train_loader)} batches ({len(train_loader.dataset)} muestras)\")\n",
        "    print(f\"    Val: {len(val_loader)} batches ({len(val_loader.dataset)} muestras)\")\n",
        "    print(f\"    Device: {device}\")\n",
        "    \n",
        "    # Mover modelo a dispositivo\n",
        "    print(f\"  üîÑ Moviendo modelo a {device}...\")\n",
        "    model = model.to(device)\n",
        "    print(f\"  ‚úì Modelo en {device}\")\n",
        "    \n",
        "    # Optimizador y criterio\n",
        "    print(f\"  üîÑ Inicializando optimizador y criterio...\")\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config[\"LEARNING_RATE\"],\n",
        "        weight_decay=config[\"WEIGHT_DECAY\"],\n",
        "    )\n",
        "    criterion = nn.BCELoss()\n",
        "    print(f\"  ‚úì Optimizador y criterio listos\")\n",
        "    \n",
        "    # ‚¨ÜÔ∏è NUEVO: Learning Rate Scheduler\n",
        "    scheduler = None\n",
        "    if config.get(\"USE_SCHEDULER\", False):\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode=config.get(\"SCHEDULER_MODE\", \"max\"),\n",
        "            factor=config.get(\"SCHEDULER_FACTOR\", 0.5),\n",
        "            patience=config.get(\"SCHEDULER_PATIENCE\", 5),\n",
        "            min_lr=config.get(\"SCHEDULER_MIN_LR\", 1e-6),\n",
        "        )\n",
        "        print(f\"‚úì Learning Rate Scheduler configurado:\")\n",
        "        print(f\"  Modo: {config.get('SCHEDULER_MODE', 'max')}\")\n",
        "        print(f\"  Patience: {config.get('SCHEDULER_PATIENCE', 5)} √©pocas\")\n",
        "        print(f\"  Factor: {config.get('SCHEDULER_FACTOR', 0.5)} (reduce a la mitad)\")\n",
        "        print(f\"  LR m√≠nimo: {config.get('SCHEDULER_MIN_LR', 1e-6)}\")\n",
        "    \n",
        "    # Gradient clipping\n",
        "    clip_grad_norm = config.get(\"CLIP_GRAD_NORM\", None)\n",
        "    if clip_grad_norm is not None and clip_grad_norm > 0:\n",
        "        print(f\"‚úì Gradient Clipping habilitado: {clip_grad_norm}\")\n",
        "    \n",
        "    # Listas para tracking\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_f1_scores = []\n",
        "    learning_rates = []  # ‚¨ÜÔ∏è NUEVO: Track LR\n",
        "    best_f1 = 0.0\n",
        "    best_model_state = None\n",
        "    \n",
        "    # Iniciar run de MLflow\n",
        "    print(f\"  üîÑ Iniciando run de MLflow...\")\n",
        "    with mlflow.start_run(experiment_id=experiment_id, run_name=config[\"RUN_NAME\"]):\n",
        "        print(f\"  ‚úì Run de MLflow iniciado\")\n",
        "        # Log hiperpar√°metros\n",
        "        print(f\"  üîÑ Loggeando hiperpar√°metros en MLflow...\")\n",
        "        mlflow.log_params({\n",
        "            \"n_channels\": config[\"N_CHANNELS\"],\n",
        "            \"seq_len\": config[\"SEQ_LEN\"],\n",
        "            \"input_size\": config[\"INPUT_SIZE\"],\n",
        "            \"hidden_size\": config[\"HIDDEN_SIZE\"],\n",
        "            \"num_layers\": config[\"NUM_LAYERS\"],\n",
        "            \"dropout\": config[\"DROPOUT\"],\n",
        "            \"bidirectional\": config[\"BIDIRECTIONAL\"],\n",
        "            \"fc_units\": config[\"FC_UNITS\"],\n",
        "            \"fc_dropout\": config[\"FC_DROPOUT\"],\n",
        "            \"batch_size\": config[\"BATCH_SIZE\"],\n",
        "            \"learning_rate\": config[\"LEARNING_RATE\"],\n",
        "            \"num_epochs\": config[\"NUM_EPOCHS\"],\n",
        "            \"weight_decay\": config[\"WEIGHT_DECAY\"],\n",
        "            \"use_scheduler\": config.get(\"USE_SCHEDULER\", False),\n",
        "            \"scheduler_patience\": config.get(\"SCHEDULER_PATIENCE\", 3),\n",
        "            \"scheduler_factor\": config.get(\"SCHEDULER_FACTOR\", 0.5),\n",
        "            \"scheduler_min_lr\": config.get(\"SCHEDULER_MIN_LR\", 1e-6),\n",
        "            \"clip_grad_norm\": config.get(\"CLIP_GRAD_NORM\", None),\n",
        "            \"cudnn_benchmark\": config.get(\"ENABLE_CUDNN_BENCHMARK\", True),\n",
        "            \"seed\": config[\"SEED\"],\n",
        "        })\n",
        "        print(f\"  ‚úì Hiperpar√°metros loggeados\")\n",
        "        \n",
        "        # Loop de entrenamiento\n",
        "        print(f\"\\nüöÄ Iniciando loop de entrenamiento ({config['NUM_EPOCHS']} √©pocas)...\")\n",
        "        print(f\"  Sin mensajes de progreso por batch (solo resultados por √©poca)\\n\")\n",
        "        for epoch in range(1, config[\"NUM_EPOCHS\"] + 1):\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"üìÖ √âPOCA {epoch}/{config['NUM_EPOCHS']}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            \n",
        "            # Entrenar\n",
        "            print(f\"  üèãÔ∏è Entrenando...\")\n",
        "            train_loss, train_acc = train_one_epoch(\n",
        "                model, train_loader, optimizer, criterion, device,\n",
        "                clip_grad_norm=clip_grad_norm,\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "            print(f\"  ‚úì Entrenamiento completado: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
        "            \n",
        "            # Validar\n",
        "            print(f\"  üìä Validando...\")\n",
        "            val_loss, val_acc, y_val_true, y_val_pred = evaluate(\n",
        "                model, val_loader, criterion, device\n",
        "            )\n",
        "            val_losses.append(val_loss)\n",
        "            print(f\"  ‚úì Validaci√≥n completada: Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
        "            \n",
        "            # Calcular m√©tricas de validaci√≥n\n",
        "            val_metrics = compute_metrics(y_val_true, y_val_pred)\n",
        "            val_f1 = val_metrics[\"f1_macro\"]\n",
        "            val_f1_scores.append(val_f1)\n",
        "            \n",
        "            # ‚¨ÜÔ∏è NUEVO: Actualizar Learning Rate Scheduler\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            learning_rates.append(current_lr)\n",
        "            \n",
        "            if scheduler is not None:\n",
        "                # ReduceLROnPlateau usa la m√©trica (val_f1 para maximizar)\n",
        "                scheduler.step(val_f1)\n",
        "                new_lr = optimizer.param_groups[0]['lr']\n",
        "                if new_lr < current_lr:\n",
        "                    print(f\"  ‚¨áÔ∏è Learning Rate reducido: {current_lr:.6f} ‚Üí {new_lr:.6f}\")\n",
        "            \n",
        "            # Log m√©tricas en MLflow\n",
        "            mlflow.log_metrics({\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_accuracy\": val_metrics[\"accuracy\"],\n",
        "                \"val_f1_macro\": val_f1,\n",
        "                \"val_f1_normal\": val_metrics[\"f1_normal\"],\n",
        "                \"val_f1_anom\": val_metrics[\"f1_anom\"],\n",
        "                \"val_precision_macro\": val_metrics[\"precision_macro\"],\n",
        "                \"val_recall_macro\": val_metrics[\"recall_macro\"],\n",
        "                \"learning_rate\": current_lr,  # ‚¨ÜÔ∏è NUEVO: Log LR actual\n",
        "            }, step=epoch)\n",
        "            \n",
        "            # Guardar mejor modelo\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                best_model_state = model.state_dict().copy()\n",
        "            \n",
        "            # Print progreso\n",
        "            if epoch % 5 == 0 or epoch == 1:\n",
        "                print(\n",
        "                    f\"Epoch {epoch:03d}/{config['NUM_EPOCHS']} | \"\n",
        "                    f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "                    f\"Val Loss: {val_loss:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | \"\n",
        "                    f\"Val F1: {val_f1:.4f} | LR: {current_lr:.6f}\"  # ‚¨ÜÔ∏è NUEVO: Mostrar LR\n",
        "                )\n",
        "        \n",
        "        # Cargar mejor modelo\n",
        "        if best_model_state is not None:\n",
        "            model.load_state_dict(best_model_state)\n",
        "        \n",
        "        # Guardar curvas de entrenamiento\n",
        "        curves_path = save_training_curves(\n",
        "            train_losses, train_accuracies, val_losses, val_f1_scores, config[\"OUTPUT_DIR\"],\n",
        "            learning_rates=learning_rates if learning_rates else None,  # ‚¨ÜÔ∏è NUEVO: Incluir LR\n",
        "        )\n",
        "        mlflow.log_artifact(str(curves_path))\n",
        "        \n",
        "        # Guardar matriz de confusi√≥n de validaci√≥n\n",
        "        val_metrics_final = compute_metrics(y_val_true, y_val_pred)\n",
        "        cm_val_path, _ = save_confusion_matrix(\n",
        "            val_metrics_final[\"confusion_matrix\"], config[\"OUTPUT_DIR\"], \"val\"\n",
        "        )\n",
        "        mlflow.log_artifact(str(cm_val_path))\n",
        "        \n",
        "        # Guardar modelo\n",
        "        mlflow.pytorch.log_model(model, \"model\")\n",
        "        \n",
        "        print(f\"‚úì Entrenamiento completado. Mejor F1 (macro): {best_f1:.4f}\")\n",
        "    \n",
        "    return model, train_losses, train_accuracies, val_losses, val_f1_scores, best_f1, learning_rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Tarea Prefect: Evaluar en test\n",
        "# ========================================\n",
        "@task(name=\"evaluate_test\", log_prints=True, cache_policy=NO_CACHE)\n",
        "def task_evaluate_test(\n",
        "    model: LSTMClassifier,\n",
        "    test_loader: DataLoader,\n",
        "    y_test: np.ndarray,\n",
        "    device: torch.device,\n",
        "    config: Dict,\n",
        "    experiment_id: str,\n",
        "):\n",
        "    \"\"\"Tarea Prefect para evaluar en test.\"\"\"\n",
        "    print(\"üìä Evaluando en conjunto de test...\")\n",
        "    \n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    \n",
        "    # Evaluar\n",
        "    test_loss, test_acc, y_test_true, y_test_pred = evaluate(\n",
        "        model, test_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Calcular m√©tricas completas\n",
        "    test_metrics = compute_metrics(y_test_true, y_test_pred)\n",
        "    test_metrics[\"loss\"] = test_loss\n",
        "    test_metrics[\"accuracy\"] = test_acc\n",
        "    \n",
        "    # Log en MLflow\n",
        "    with mlflow.start_run(experiment_id=experiment_id, run_name=config[\"RUN_NAME\"]):\n",
        "        mlflow.log_metrics({\n",
        "            \"test_loss\": test_loss,\n",
        "            \"test_accuracy\": test_metrics[\"accuracy\"],\n",
        "            \"test_f1_macro\": test_metrics[\"f1_macro\"],\n",
        "            \"test_f1_normal\": test_metrics[\"f1_normal\"],\n",
        "            \"test_f1_anom\": test_metrics[\"f1_anom\"],\n",
        "            \"test_precision_macro\": test_metrics[\"precision_macro\"],\n",
        "            \"test_recall_macro\": test_metrics[\"recall_macro\"],\n",
        "            \"test_specificity\": test_metrics[\"specificity\"],\n",
        "            \"test_sensitivity\": test_metrics[\"sensitivity\"],\n",
        "        })\n",
        "        \n",
        "        # Guardar matriz de confusi√≥n de test\n",
        "        cm_test_path, _ = save_confusion_matrix(\n",
        "            test_metrics[\"confusion_matrix\"], config[\"OUTPUT_DIR\"], \"test\"\n",
        "        )\n",
        "        mlflow.log_artifact(str(cm_test_path))\n",
        "    \n",
        "    print(\"‚úì Evaluaci√≥n en test completada:\")\n",
        "    print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Precision (normal): {test_metrics['precision_normal']:.4f} | Recall: {test_metrics['recall_normal']:.4f} | F1: {test_metrics['f1_normal']:.4f}\")\n",
        "    print(f\"  Precision (an√≥malo): {test_metrics['precision_anom']:.4f} | Recall: {test_metrics['recall_anom']:.4f} | F1: {test_metrics['f1_anom']:.4f}\")\n",
        "    print(f\"  F1 Macro: {test_metrics['f1_macro']:.4f}\")\n",
        "    \n",
        "    return test_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Flujo principal de Prefect\n",
        "# ========================================\n",
        "@flow(name=\"lstm_classification_training_flow\", log_prints=True)\n",
        "def lstm_classification_training_flow(config: Dict = None):\n",
        "    \"\"\"\n",
        "    Flujo principal de Prefect que orquesta todo el proceso:\n",
        "    1. Carga y preparaci√≥n de datos\n",
        "    2. Creaci√≥n del modelo\n",
        "    3. Entrenamiento\n",
        "    4. Evaluaci√≥n en test\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = CONFIG\n",
        "    \n",
        "    print(\"üöÄ Iniciando flujo de entrenamiento LSTM Clasificaci√≥n...\")\n",
        "    print(f\"Experimento MLflow: {config['EXPERIMENT_NAME']}\")\n",
        "    \n",
        "    # Configurar MLflow\n",
        "    experiment_id = setup_mlflow(config)\n",
        "    \n",
        "    # Cargar y preparar datos\n",
        "    dataloaders = task_load_data(config)\n",
        "    train_loader, val_loader, test_loader, y_val, y_test = dataloaders\n",
        "    \n",
        "    # Crear modelo\n",
        "    print(\"üß† Creando modelo...\")\n",
        "    model = create_model(config)\n",
        "    \n",
        "    # Entrenar\n",
        "    model, train_losses, train_accs, val_losses, val_f1_scores, best_f1, learning_rates = task_train_model(\n",
        "        model, train_loader, val_loader, y_val, config, DEVICE, experiment_id\n",
        "    )\n",
        "    \n",
        "    # Evaluar en test\n",
        "    test_metrics = task_evaluate_test(\n",
        "        model, test_loader, y_test, DEVICE, config, experiment_id\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ FLUJO COMPLETADO\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Mejor F1 en validaci√≥n: {best_f1:.4f}\")\n",
        "    print(f\"F1 en test: {test_metrics['f1_macro']:.4f}\")\n",
        "    print(f\"\\nRevisa MLflow para ver todos los artefactos y m√©tricas.\")\n",
        "    \n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"best_f1\": best_f1,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. üöÄ Ejecuci√≥n del Flujo Completo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs] Beginning flow run 'bulky-gharial' for flow 'lstm_classification_training_flow'\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs] üöÄ Iniciando flujo de entrenamiento LSTM Clasificaci√≥n...\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs] Experimento MLflow: ecg_lstm_supervisado\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs] ‚úì MLflow tracking URI: sqlite:///S:/Proyecto final/mlflow.db\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs] ‚úì Experimento MLflow existente: ecg_lstm_supervisado (ID: 8)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] üìÇ Cargando datos...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ======================================================================\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] üìÇ CARGANDO INFORMACI√ìN DE DATOS DESDE tensors_200hz\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ======================================================================\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] Directorio: S:\\Proyecto final\\data\\Datos_supervisados\\tensors_200hz\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] \n",
            "‚è≥ Cargando informaci√≥n de datos (solo etiquetas, sin cargar X)...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] \n",
            "‚úì Informaci√≥n de datos (usando configuraci√≥n para formas de X):\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   X_train: (270668, 2000, 3) | y_train: (270668,) (normales: 135334, an√≥malos: 135334)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   X_val:   (58001, 2000, 3) | y_val:   (58001,) (normales: 29000, an√≥malos: 29001)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   X_test:  (58001, 2000, 3) | y_test:  (58001,) (normales: 29001, an√≥malos: 29000)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ======================================================================\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] \n",
            "üì¶ Creando datasets (los datos se cargar√°n en CPU bajo demanda)...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ‚ö†Ô∏è  NOTA: Los datos se cargar√°n cuando el DataLoader comience a iterar.\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]     Si tienes problemas de memoria, reduce BATCH_SIZE en la configuraci√≥n.\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Cargando y_train.pt...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Cargando y_val.pt...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Cargando y_test.pt...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] \n",
            "‚úì DataLoaders creados:\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Train: 33834 batches (270668 muestras)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Val:   7251 batches (58001 muestras)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Test:  7251 batches (58001 muestras)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ‚úì Datos cargados y preparados\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] Finished in state Completed()\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs] üß† Creando modelo...\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs] ‚úì Modelo creado:\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs]   Par√°metros totales: 53,057 (0.05M)\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs]   Par√°metros entrenables: 53,057\n",
            "2025-11-24 22:55:17 INFO  [prefect.flow_runs]   LSTM: 2 capas, hidden_size=64, bidirectional=False\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] üèãÔ∏è Iniciando entrenamiento...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   üìä Verificando DataLoaders...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]     Train: 33834 batches (270668 muestras)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]     Val: 7251 batches (58001 muestras)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]     Device: cuda\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   üîÑ Moviendo modelo a cuda...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   ‚úì Modelo en cuda\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   üîÑ Inicializando optimizador y criterio...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   ‚úì Optimizador y criterio listos\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ‚úì Learning Rate Scheduler configurado:\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Modo: max\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Patience: 3 √©pocas\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Factor: 0.5 (reduce a la mitad)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   LR m√≠nimo: 1e-06\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ‚úì Gradient Clipping habilitado: 1.0\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   üîÑ Iniciando run de MLflow...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   ‚úì Run de MLflow iniciado\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   üîÑ Loggeando hiperpar√°metros en MLflow...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   ‚úì Hiperpar√°metros loggeados\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] \n",
            "üöÄ Iniciando loop de entrenamiento (50 √©pocas)...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   Sin mensajes de progreso por batch (solo resultados por √©poca)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] \n",
            "============================================================\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] üìÖ √âPOCA 1/50\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs] ============================================================\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   üèãÔ∏è Entrenando...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]   ‚ö†Ô∏è Cargando X_train.pt en CPU...\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]      Tama√±o del archivo: ~6.05 GB (6195.1 MB)\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]      ‚ö†Ô∏è ADVERTENCIA: Esto cargar√° TODO el archivo en RAM.\n",
            "2025-11-24 22:55:17 INFO  [prefect.task_runs]      Esto puede tardar y usar RAM. Los datos se mantendr√°n en CPU.\n",
            "2025-11-24 22:55:18 INFO  [prefect.task_runs]      RAM disponible: 9.11 GB / 31.90 GB\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs] \n",
            "  ‚ùå ERROR CR√çTICO: No hay suficiente memoria para cargar X_train.pt\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs]      Tama√±o requerido: ~6.05 GB\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs] \n",
            "  üîß SOLUCIONES:\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs]      1. ‚ö° Reduce BATCH_SIZE a 4 u 8 en la configuraci√≥n (actual: 8)\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs]      2. üíæ Cierra otras aplicaciones que usen mucha memoria\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs]      3. üöÄ Aumenta la RAM disponible (se recomienda al menos 9.1 GB)\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs]      4. üì¶ Convierte a HDF5 para acceso eficiente:\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs]         - Ejecuta: convert_pt_to_hdf5(Path('..\\data\\Datos_supervisados\\tensors_200hz'))\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs]         - Luego modifica el c√≥digo para usar HDF5Dataset\n",
            "2025-11-24 22:55:39 INFO  [prefect.task_runs] \n",
            "  üí° NOTA: Los archivos .pt cargan TODO en memoria. HDF5 permite acceso aleatorio.\n",
            "2025-11-24 22:55:39 ERROR [prefect.task_runs] Task run failed with exception: RuntimeError('No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5.')\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\350961526.py\", line 119, in _load_X\n",
            "    self.X = torch.load(self.X_file, map_location='cpu')\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 1540, in load\n",
            "    return _load(\n",
            "           ^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 2143, in _load\n",
            "    result = unpickler.load()\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\_weights_only_unpickler.py\", line 537, in load\n",
            "    self.append(self.persistent_load(pid))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 2107, in persistent_load\n",
            "    typed_storage = load_tensor(\n",
            "                    ^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 2060, in load_tensor\n",
            "    zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)\n",
            "RuntimeError: [enforce fail at alloc_cpu.cpp:117] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6496032000 bytes.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 871, in run_context\n",
            "    yield self\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 1512, in run_task_sync\n",
            "    engine.call_task_fn(txn)\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 888, in call_task_fn\n",
            "    result = call_with_parameters(self.task.fn, parameters)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\3714320537.py\", line 106, in task_train_model\n",
            "    train_loss, train_acc = train_one_epoch(\n",
            "                            ^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\1631548253.py\", line 23, in train_one_epoch\n",
            "    for batch_x, batch_y in train_loader:\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 740, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 800, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\350961526.py\", line 149, in __getitem__\n",
            "    self._load_X()\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\350961526.py\", line 136, in _load_X\n",
            "    raise RuntimeError(\n",
            "RuntimeError: No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5.\n",
            "2025-11-24 22:55:39 ERROR [prefect.task_runs] Finished in state Failed('Task run encountered an exception RuntimeError: No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5.')\n",
            "2025-11-24 22:55:39 ERROR [prefect.flow_runs] Encountered exception during execution: RuntimeError('No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5.')\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\350961526.py\", line 119, in _load_X\n",
            "    self.X = torch.load(self.X_file, map_location='cpu')\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 1540, in load\n",
            "    return _load(\n",
            "           ^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 2143, in _load\n",
            "    result = unpickler.load()\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\_weights_only_unpickler.py\", line 537, in load\n",
            "    self.append(self.persistent_load(pid))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 2107, in persistent_load\n",
            "    typed_storage = load_tensor(\n",
            "                    ^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\serialization.py\", line 2060, in load_tensor\n",
            "    zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)\n",
            "RuntimeError: [enforce fail at alloc_cpu.cpp:117] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6496032000 bytes.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py\", line 788, in run_context\n",
            "    yield self\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py\", line 1409, in run_flow_sync\n",
            "    engine.call_flow_fn()\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py\", line 808, in call_flow_fn\n",
            "    result = call_with_parameters(self.flow.fn, self.parameters)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\3851895273.py\", line 31, in lstm_classification_training_flow\n",
            "    model, train_losses, train_accs, val_losses, val_f1_scores, best_f1, learning_rates = task_train_model(\n",
            "                                                                                          ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\tasks.py\", line 1139, in __call__\n",
            "    return run_task(\n",
            "           ^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 1739, in run_task\n",
            "    return run_task_sync(**kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 1514, in run_task_sync\n",
            "    return engine.state if return_type == \"state\" else engine.result()\n",
            "                                                       ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 493, in result\n",
            "    raise self._raised\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 871, in run_context\n",
            "    yield self\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 1512, in run_task_sync\n",
            "    engine.call_task_fn(txn)\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py\", line 888, in call_task_fn\n",
            "    result = call_with_parameters(self.task.fn, parameters)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\3714320537.py\", line 106, in task_train_model\n",
            "    train_loss, train_acc = train_one_epoch(\n",
            "                            ^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\1631548253.py\", line 23, in train_one_epoch\n",
            "    for batch_x, batch_y in train_loader:\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 740, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 800, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\350961526.py\", line 149, in __getitem__\n",
            "    self._load_X()\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_12624\\350961526.py\", line 136, in _load_X\n",
            "    raise RuntimeError(\n",
            "RuntimeError: No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5.\n",
            "2025-11-24 22:55:39 INFO  [prefect.flow_runs] Finished in state Failed('Flow run encountered an exception: RuntimeError: No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5.')\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mLazyTensorDataset._load_X\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# psutil no est√° instalado, continuar\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28mself\u001b[39m.X = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28mself\u001b[39m._X_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\serialization.py:1540\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\serialization.py:2143\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2142\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2144\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:537\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    534\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    535\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    536\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\serialization.py:2107\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2106\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2107\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\serialization.py:2060\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2054\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThis is a debug assert that was run as the `TORCH_SERIALIZATION_DEBUG` environment \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2056\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvariable was set: Incorrect offset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m expected \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2057\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file.get_record_offset(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2058\u001b[39m             )\n\u001b[32m   2059\u001b[39m     storage = (\n\u001b[32m-> \u001b[39m\u001b[32m2060\u001b[39m         \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2061\u001b[39m         ._typed_storage()\n\u001b[32m   2062\u001b[39m         ._untyped_storage\n\u001b[32m   2063\u001b[39m     )\n\u001b[32m   2064\u001b[39m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:117] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6496032000 bytes.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Ejecutar el flujo completo\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     results = \u001b[43mlstm_classification_training_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Proceso finalizado exitosamente\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flows.py:1702\u001b[39m, in \u001b[36mFlow.__call__\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m track_viz_task(\u001b[38;5;28mself\u001b[39m.isasync, \u001b[38;5;28mself\u001b[39m.name, parameters)\n\u001b[32m   1700\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mprefect\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflow_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_flow\n\u001b[32m-> \u001b[39m\u001b[32m1702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_flow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:1566\u001b[39m, in \u001b[36mrun_flow\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, error_logger, context)\u001b[39m\n\u001b[32m   1564\u001b[39m         ret_val = run_flow_async(**kwargs)\n\u001b[32m   1565\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1566\u001b[39m         ret_val = \u001b[43mrun_flow_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (Abort, Pause):\n\u001b[32m   1568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:1411\u001b[39m, in \u001b[36mrun_flow_sync\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1408\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m   1409\u001b[39m             engine.call_flow_fn()\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:361\u001b[39m, in \u001b[36mFlowRunEngine.result\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# This is a fall through case which leans on the existing state result mechanics to get the\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# return value. This is necessary because we currently will return a State object if the\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# the State was Prefect-created.\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# TODO: Remove the need to get the result from a State except in cases where the return value\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# is a State object.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:788\u001b[39m, in \u001b[36mFlowRunEngine.run_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m timeout_context(\n\u001b[32m    782\u001b[39m         seconds=\u001b[38;5;28mself\u001b[39m.flow.timeout_seconds,\n\u001b[32m    783\u001b[39m         timeout_exc_type=FlowRunTimeoutError,\n\u001b[32m    784\u001b[39m     ):\n\u001b[32m    785\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.debug(\n\u001b[32m    786\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuting flow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for flow run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow_run.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    787\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_timeout(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:1409\u001b[39m, in \u001b[36mrun_flow_sync\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1407\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m engine.is_running():\n\u001b[32m   1408\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m-> \u001b[39m\u001b[32m1409\u001b[39m             \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_flow_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:808\u001b[39m, in \u001b[36mFlowRunEngine.call_flow_fn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_flow_fn()\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m     result = \u001b[43mcall_with_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_success(result)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\utilities\\callables.py:210\u001b[39m, in \u001b[36mcall_with_parameters\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03mCall a function with parameters extracted with `get_call_parameters`\u001b[39;00m\n\u001b[32m    204\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \u001b[33;03mthe args/kwargs using `parameters_to_positional_and_keyword` directly\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mlstm_classification_training_flow\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     28\u001b[39m model = create_model(config)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m model, train_losses, train_accs, val_losses, val_f1_scores, best_f1, learning_rates = \u001b[43mtask_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_id\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Evaluar en test\u001b[39;00m\n\u001b[32m     36\u001b[39m test_metrics = task_evaluate_test(\n\u001b[32m     37\u001b[39m     model, test_loader, y_test, DEVICE, config, experiment_id\n\u001b[32m     38\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\tasks.py:1139\u001b[39m, in \u001b[36mTask.__call__\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m track_viz_task(\n\u001b[32m   1134\u001b[39m         \u001b[38;5;28mself\u001b[39m.isasync, \u001b[38;5;28mself\u001b[39m.name, parameters, \u001b[38;5;28mself\u001b[39m.viz_return_value\n\u001b[32m   1135\u001b[39m     )\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mprefect\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtask_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_task\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py:1739\u001b[39m, in \u001b[36mrun_task\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_task_async(**kwargs)\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_task_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py:1514\u001b[39m, in \u001b[36mrun_task_sync\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1507\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1508\u001b[39m             engine.asset_context(),\n\u001b[32m   1509\u001b[39m             engine.run_context(),\n\u001b[32m   1510\u001b[39m             engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1511\u001b[39m         ):\n\u001b[32m   1512\u001b[39m             engine.call_task_fn(txn)\n\u001b[32m-> \u001b[39m\u001b[32m1514\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py:493\u001b[39m, in \u001b[36mSyncTaskRunEngine.result\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[32m    491\u001b[39m     \u001b[38;5;66;03m# if the task raised an exception, raise it\u001b[39;00m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    495\u001b[39m     \u001b[38;5;66;03m# otherwise, return the exception\u001b[39;00m\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py:871\u001b[39m, in \u001b[36mSyncTaskRunEngine.run_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    868\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_cancelled():\n\u001b[32m    869\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m CancelledError(\u001b[33m\"\u001b[39m\u001b[33mTask run cancelled by the task runner\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_timeout(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py:1512\u001b[39m, in \u001b[36mrun_task_sync\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1506\u001b[39m         run_coro_as_sync(engine.wait_until_ready())\n\u001b[32m   1507\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1508\u001b[39m             engine.asset_context(),\n\u001b[32m   1509\u001b[39m             engine.run_context(),\n\u001b[32m   1510\u001b[39m             engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1511\u001b[39m         ):\n\u001b[32m-> \u001b[39m\u001b[32m1512\u001b[39m             \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_task_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\task_engine.py:888\u001b[39m, in \u001b[36mSyncTaskRunEngine.call_task_fn\u001b[39m\u001b[34m(self, transaction)\u001b[39m\n\u001b[32m    886\u001b[39m     result = transaction.read()\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m     result = \u001b[43mcall_with_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;28mself\u001b[39m.handle_success(result, transaction=transaction)\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\utilities\\callables.py:210\u001b[39m, in \u001b[36mcall_with_parameters\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03mCall a function with parameters extracted with `get_call_parameters`\u001b[39;00m\n\u001b[32m    204\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \u001b[33;03mthe args/kwargs using `parameters_to_positional_and_keyword` directly\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mtask_train_model\u001b[39m\u001b[34m(model, train_loader, val_loader, y_val, config, device, experiment_id)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  üèãÔ∏è Entrenando...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_grad_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m train_losses.append(train_loss)\n\u001b[32m    111\u001b[39m train_accuracies.append(train_acc)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion, device, clip_grad_norm)\u001b[39m\n\u001b[32m     20\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     21\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:740\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    738\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    743\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    744\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    746\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:800\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    799\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    802\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mLazyTensorDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# Cargar X si a√∫n no est√° cargado (lazy loading)\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._X_loaded:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# Retornar datos desde CPU (el DataLoader los transferir√° a GPU si es necesario)\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.X[idx], \u001b[38;5;28mself\u001b[39m.y[idx]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mLazyTensorDataset._load_X\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m        - Luego modifica el c√≥digo para usar HDF5Dataset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    135\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  üí° NOTA: Los archivos .pt cargan TODO en memoria. HDF5 permite acceso aleatorio.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    137\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo hay suficiente memoria para cargar \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.X_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size_gb\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReduce BATCH_SIZE o convierte a HDF5.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: No hay suficiente memoria para cargar X_train.pt (~6.05 GB). Reduce BATCH_SIZE o convierte a HDF5."
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Ejecutar el flujo completo\n",
        "# ========================================\n",
        "if __name__ == \"__main__\":\n",
        "    results = lstm_classification_training_flow(CONFIG)\n",
        "    print(\"\\n‚úì Proceso finalizado exitosamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Checklist Final\n",
        "\n",
        "Antes de ejecutar el notebook completo:\n",
        "\n",
        "1. ‚úÖ **Ajusta la ruta `DATA_DIR`** en la secci√≥n de configuraci√≥n (debe apuntar a `Datos_supervisados`)\n",
        "2. ‚úÖ **Verifica los par√°metros** en la secci√≥n de configuraci√≥n (INPUT_SIZE, SEQ_LEN, etc.)\n",
        "3. ‚úÖ **Cambia el nombre del experimento MLflow** (`EXPERIMENT_NAME`) si quieres crear uno nuevo\n",
        "4. ‚úÖ **Ejecuta todas las celdas en orden** (empezando por Setup CUDA)\n",
        "\n",
        "### üìù Notas importantes:\n",
        "\n",
        "- El modelo se entrena con **datos supervisados** (etiquetas 0=normal, 1=an√≥malo)\n",
        "- Los datos se cargan desde `Datos_supervisados/numpy/` (X_train.npy, y_train.npy, etc.)\n",
        "- Todos los artefactos (modelo, gr√°ficos, matrices de confusi√≥n) se guardan en `OUTPUT_DIR` y en MLflow\n",
        "- Las m√©tricas se registran en MLflow: `train_loss`, `train_accuracy`, `val_*`, `test_*`\n",
        "- El modelo usa **BCE Loss** (Binary Cross Entropy) para clasificaci√≥n binaria\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
