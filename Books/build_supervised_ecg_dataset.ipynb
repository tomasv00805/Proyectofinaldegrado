{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline de Dataset Supervisado ECG (NORMAL vs ANÓMALO)\n",
        "\n",
        "Este notebook ejecuta el pipeline completo paso a paso:\n",
        "1. Procesa datasets PTB-XL y MIMIC-IV-ECG\n",
        "2. Etiqueta registros (NORMAL vs ANÓMALO)\n",
        "3. Filtra señales de mala calidad\n",
        "4. Aplica filtrado y normalización\n",
        "5. Selecciona leads II, V1, V5\n",
        "6. Resamplea a 10s y 500 Hz\n",
        "7. Genera datasets balanceados\n",
        "8. Genera train/val/test + 10 folds\n",
        "9. Guarda todo en `/data/Datos_supervisados/`\n",
        "\n",
        "**Ventaja**: Puedes ejecutar celda por celda y ver exactamente dónde se bloquea.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuración e Importaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Versión rápida (paralela) disponible\n",
            "✓ Directorio de salida: S:\\Proyecto final\\data\\Datos_supervisados\n",
            "\n",
            "Configuración:\n",
            "  PTB-XL máximo: Todos\n",
            "  MIMIC máximo: Todos\n",
            "  Versión: RÁPIDA (paralela)\n",
            "  Workers: 15\n",
            "  Balancear: True\n",
            "  Calidad mínima: False\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Recargar módulo si es necesario (para incluir nuevas funciones)\n",
        "import importlib\n",
        "import supervised_ecg_pipeline\n",
        "importlib.reload(supervised_ecg_pipeline)\n",
        "\n",
        "# Importar módulos del pipeline\n",
        "from supervised_ecg_pipeline import (\n",
        "    OUTPUT_DIR,\n",
        "    create_splits,\n",
        "    create_splits_to_disk,\n",
        "    create_stratified_folds,\n",
        "    save_dataset,\n",
        "    balance_dataset,\n",
        "    ensure_dir,\n",
        "    PTB_ROOT,\n",
        "    MIMIC_ROOT,\n",
        ")\n",
        "\n",
        "# Intentar importar versión optimizada\n",
        "try:\n",
        "    from supervised_ecg_pipeline_fast import (\n",
        "        process_mimic_dataset_fast,\n",
        "        process_ptbxl_dataset_fast,\n",
        "    )\n",
        "    FAST_VERSION_AVAILABLE = True\n",
        "    print(\"✓ Versión rápida (paralela) disponible\")\n",
        "except ImportError:\n",
        "    from supervised_ecg_pipeline import (\n",
        "        process_mimic_dataset,\n",
        "        process_ptbxl_dataset,\n",
        "    )\n",
        "    FAST_VERSION_AVAILABLE = False\n",
        "    print(\"⚠ Usando versión lenta (secuencial)\")\n",
        "\n",
        "print(f\"✓ Directorio de salida: {OUTPUT_DIR}\")\n",
        "\n",
        "# ==================== CONFIGURACIÓN ====================\n",
        "# Modifica estos valores según necesites\n",
        "MAX_PTB = None  # None = todos, o poner número para pruebas (ej: 1000)\n",
        "MAX_MIMIC = None  # None = todos, o poner número para pruebas (ej: 1000)\n",
        "USE_FAST = FAST_VERSION_AVAILABLE\n",
        "N_WORKERS = None  # None = auto (cpu_count - 1)\n",
        "APPLY_QUALITY_CHECK = True\n",
        "APPLY_NOTCH = True\n",
        "NOTCH_FREQ = 50.0\n",
        "NORMALIZE_METHOD = \"minmax\"\n",
        "REJECT_UNVALIDATED = False\n",
        "DO_BALANCE = True\n",
        "MINIMAL_QUALITY = False  # True = deshabilitar checks menos críticos para más velocidad\n",
        "\n",
        "if N_WORKERS is None and USE_FAST:\n",
        "    import multiprocessing as mp\n",
        "    N_WORKERS = max(1, mp.cpu_count() - 1)\n",
        "\n",
        "print(f\"\\nConfiguración:\")\n",
        "print(f\"  PTB-XL máximo: {MAX_PTB if MAX_PTB else 'Todos'}\")\n",
        "print(f\"  MIMIC máximo: {MAX_MIMIC if MAX_MIMIC else 'Todos'}\")\n",
        "print(f\"  Versión: {'RÁPIDA (paralela)' if USE_FAST else 'LENTA (secuencial)'}\")\n",
        "if USE_FAST:\n",
        "    print(f\"  Workers: {N_WORKERS}\")\n",
        "print(f\"  Balancear: {DO_BALANCE}\")\n",
        "print(f\"  Calidad mínima: {MINIMAL_QUALITY}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Procesar PTB-XL\n",
        "\n",
        "**Ejecuta esta celda y espera a que termine. Si se bloquea, verás dónde.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 1: Procesando PTB-XL\n",
            "================================================================================\n",
            "\n",
            "[PTB-XL] Iniciando procesamiento paralelo...\n",
            "[PTB-XL] Procesando 21799 registros directamente en paralelo (15 workers)...\n",
            "  Iniciando procesamiento paralelo de 21799 registros...\n",
            "  ✓ Completado: 979 válidos, 20820 rechazados\n",
            "\n",
            "[PTB-XL] Completado:\n",
            "  Procesados exitosamente: 979\n",
            "  Rechazados: 20820\n",
            "  Construyendo arrays finales de 979 registros...\n",
            "  Extrayendo señales...\n",
            "  ✓ Señales extraídas: shape (979, 5000, 3)\n",
            "  Extrayendo labels...\n",
            "  ✓ Labels extraídos: 979 labels\n",
            "  Creando metadatos...\n",
            "  ✓ Metadatos creados: 979 registros\n",
            "  ✓ Arrays finales listos: signals=(979, 5000, 3), labels=(979,)\n",
            "\n",
            "✓ PTB-XL COMPLETADO\n",
            "  Tiempo: 0.43 minutos\n",
            "  Registros: 979\n",
            "  Normales: 397\n",
            "  Anómalos: 582\n",
            "  Shape: (979, 5000, 3)\n",
            "  Memoria: 0.05 GB\n"
          ]
        }
      ],
      "source": [
        "# Inicializar variables\n",
        "ptbxl_signals = None\n",
        "ptbxl_labels = None\n",
        "ptbxl_df = None\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PASO 1: Procesando PTB-XL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if USE_FAST:\n",
        "        print(f\"\\n[PTB-XL] Iniciando procesamiento paralelo...\")\n",
        "        ptbxl_signals, ptbxl_labels, ptbxl_df = process_ptbxl_dataset_fast(\n",
        "            overwrite=False,\n",
        "            apply_quality_check=APPLY_QUALITY_CHECK and not MINIMAL_QUALITY,\n",
        "            apply_notch=APPLY_NOTCH,\n",
        "            notch_freq=NOTCH_FREQ,\n",
        "            normalize_method=NORMALIZE_METHOD,\n",
        "            reject_unvalidated=REJECT_UNVALIDATED,\n",
        "            max_records=MAX_PTB,\n",
        "            n_workers=N_WORKERS,\n",
        "            verbose=True,\n",
        "            prefilter_labels=False,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"\\n[PTB-XL] Iniciando procesamiento secuencial...\")\n",
        "        ptbxl_df_full = process_ptbxl_dataset(\n",
        "            overwrite=False,\n",
        "            apply_quality_check=APPLY_QUALITY_CHECK,\n",
        "            apply_notch=APPLY_NOTCH,\n",
        "            notch_freq=NOTCH_FREQ,\n",
        "            normalize_method=NORMALIZE_METHOD,\n",
        "            reject_unvalidated=REJECT_UNVALIDATED,\n",
        "            max_records=MAX_PTB,\n",
        "            verbose=True,\n",
        "        )\n",
        "        \n",
        "        if len(ptbxl_df_full) > 0:\n",
        "            print(f\"  Extrayendo señales de DataFrame...\")\n",
        "            ptbxl_signals = np.stack(ptbxl_df_full[\"signal\"].values, axis=0)\n",
        "            ptbxl_labels = ptbxl_df_full[\"label\"].values\n",
        "            ptbxl_df = ptbxl_df_full.drop(columns=[\"signal\"])\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    if ptbxl_signals is None or len(ptbxl_signals) == 0:\n",
        "        print(\"\\n⚠ No se procesaron registros de PTB-XL\")\n",
        "        ptbxl_signals = None\n",
        "        ptbxl_labels = None\n",
        "        ptbxl_df = None\n",
        "    else:\n",
        "        print(f\"\\n✓ PTB-XL COMPLETADO\")\n",
        "        print(f\"  Tiempo: {elapsed/60:.2f} minutos\")\n",
        "        print(f\"  Registros: {len(ptbxl_signals)}\")\n",
        "        print(f\"  Normales: {(ptbxl_labels == 0).sum()}\")\n",
        "        print(f\"  Anómalos: {(ptbxl_labels == 1).sum()}\")\n",
        "        print(f\"  Shape: {ptbxl_signals.shape}\")\n",
        "        print(f\"  Memoria: {ptbxl_signals.nbytes / 1024**3:.2f} GB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR procesando PTB-XL: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    ptbxl_signals = None\n",
        "    ptbxl_labels = None\n",
        "    ptbxl_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Procesar MIMIC-IV-ECG\n",
        "\n",
        "**Ejecuta esta celda y espera a que termine. Si se bloquea, verás dónde.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 2: Procesando MIMIC-IV-ECG\n",
            "================================================================================\n",
            "\n",
            "[MIMIC] Iniciando procesamiento paralelo...\n",
            "[MIMIC] Leyendo registros y preparando procesamiento paralelo (15 workers)...\n",
            "[MIMIC] Procesando 800035 registros directamente en paralelo...\n",
            "  Iniciando procesamiento paralelo de 800035 registros...\n",
            "  ✓ Completado: 495265 válidos, 304770 rechazados\n",
            "\n",
            "[MIMIC] Completado:\n",
            "  Procesados exitosamente: 495265\n",
            "  Rechazados: 304770\n",
            "  Construyendo arrays finales de 495265 registros...\n",
            "  Extrayendo señales...\n",
            "  ✓ Señales extraídas: shape (495265, 5000, 3)\n",
            "  Extrayendo labels...\n",
            "  ✓ Labels extraídos: 495265 labels\n",
            "  Creando metadatos...\n",
            "  ✓ Metadatos creados: 495265 registros\n",
            "  ✓ Arrays finales listos: signals=(495265, 5000, 3), labels=(495265,)\n",
            "\n",
            "✓ MIMIC COMPLETADO\n",
            "  Tiempo: 54.07 minutos\n",
            "  Registros: 495265\n",
            "  Normales: 192938\n",
            "  Anómalos: 302327\n",
            "  Shape: (495265, 5000, 3)\n",
            "  Memoria: 27.68 GB\n"
          ]
        }
      ],
      "source": [
        "# Inicializar variables\n",
        "mimic_signals = None\n",
        "mimic_labels = None\n",
        "mimic_df = None\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PASO 2: Procesando MIMIC-IV-ECG\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if USE_FAST:\n",
        "        print(f\"\\n[MIMIC] Iniciando procesamiento paralelo...\")\n",
        "        mimic_signals, mimic_labels, mimic_df = process_mimic_dataset_fast(\n",
        "            overwrite=False,\n",
        "            apply_quality_check=APPLY_QUALITY_CHECK and not MINIMAL_QUALITY,\n",
        "            apply_notch=APPLY_NOTCH,\n",
        "            notch_freq=NOTCH_FREQ,\n",
        "            normalize_method=NORMALIZE_METHOD,\n",
        "            max_records=MAX_MIMIC,\n",
        "            n_workers=N_WORKERS,\n",
        "            verbose=True,\n",
        "            prefilter_labels=False,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"\\n[MIMIC] Iniciando procesamiento secuencial...\")\n",
        "        mimic_df_full = process_mimic_dataset(\n",
        "            overwrite=False,\n",
        "            report_column=\"report_1\",\n",
        "            apply_quality_check=APPLY_QUALITY_CHECK,\n",
        "            apply_notch=APPLY_NOTCH,\n",
        "            notch_freq=NOTCH_FREQ,\n",
        "            normalize_method=NORMALIZE_METHOD,\n",
        "            max_records=MAX_MIMIC,\n",
        "            verbose=True,\n",
        "        )\n",
        "        \n",
        "        if len(mimic_df_full) > 0:\n",
        "            print(f\"  Extrayendo señales de DataFrame...\")\n",
        "            signals_list = mimic_df_full[\"signal\"].values.tolist()\n",
        "            print(f\"  Haciendo stack de {len(signals_list)} señales (esto puede tardar)...\")\n",
        "            mimic_signals = np.stack(signals_list, axis=0)\n",
        "            del signals_list\n",
        "            mimic_labels = mimic_df_full[\"label\"].values\n",
        "            mimic_df = mimic_df_full.drop(columns=[\"signal\"])\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    if mimic_signals is None or len(mimic_signals) == 0:\n",
        "        print(\"\\n⚠ No se procesaron registros de MIMIC\")\n",
        "        mimic_signals = None\n",
        "        mimic_labels = None\n",
        "        mimic_df = None\n",
        "    else:\n",
        "        print(f\"\\n✓ MIMIC COMPLETADO\")\n",
        "        print(f\"  Tiempo: {elapsed/60:.2f} minutos\")\n",
        "        print(f\"  Registros: {len(mimic_signals)}\")\n",
        "        print(f\"  Normales: {(mimic_labels == 0).sum()}\")\n",
        "        print(f\"  Anómalos: {(mimic_labels == 1).sum()}\")\n",
        "        print(f\"  Shape: {mimic_signals.shape}\")\n",
        "        print(f\"  Memoria: {mimic_signals.nbytes / 1024**3:.2f} GB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR procesando MIMIC: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    mimic_signals = None\n",
        "    mimic_labels = None\n",
        "    mimic_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Construir Dataset Combinado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 3: Construyendo dataset combinado\n",
            "================================================================================\n",
            "\n",
            "  Agregando PTB-XL: 979 registros\n",
            "  Agregando MIMIC: 495265 registros\n",
            "\n",
            "  Combinando señales (esto puede tardar si hay muchas)...\n",
            "  ✓ Señales combinadas: (496244, 5000, 3)\n",
            "  Combinando labels...\n",
            "  ✓ Labels combinados: 496244 labels\n",
            "  Combinando metadatos...\n",
            "  ✓ Metadatos combinados: 496244 registros\n",
            "\n",
            "✓ Dataset combinado\n",
            "  Tiempo: 712.72 segundos\n",
            "  Total registros: 496244\n",
            "  Normales: 193335\n",
            "  Anómalos: 302909\n",
            "  Shape: (496244, 5000, 3)\n",
            "  Memoria: 27.73 GB\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PASO 3: Construyendo dataset combinado\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if (ptbxl_signals is None or len(ptbxl_signals) == 0) and (mimic_signals is None or len(mimic_signals) == 0):\n",
        "    print(\"\\n✗ ERROR: No hay datos para procesar\")\n",
        "    print(f\"  PTB-XL: {ptbxl_signals is not None and len(ptbxl_signals) > 0}\")\n",
        "    print(f\"  MIMIC: {mimic_signals is not None and len(mimic_signals) > 0}\")\n",
        "    raise ValueError(\"No hay datos para procesar\")\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Combinar señales y labels\n",
        "    all_signals = []\n",
        "    all_labels = []\n",
        "    all_metadata = []\n",
        "    \n",
        "    if ptbxl_signals is not None and len(ptbxl_signals) > 0:\n",
        "        print(f\"\\n  Agregando PTB-XL: {len(ptbxl_signals)} registros\")\n",
        "        all_signals.append(ptbxl_signals)\n",
        "        all_labels.append(ptbxl_labels)\n",
        "        if ptbxl_df is not None and len(ptbxl_df) > 0:\n",
        "            all_metadata.append(ptbxl_df)\n",
        "    \n",
        "    if mimic_signals is not None and len(mimic_signals) > 0:\n",
        "        print(f\"  Agregando MIMIC: {len(mimic_signals)} registros\")\n",
        "        all_signals.append(mimic_signals)\n",
        "        all_labels.append(mimic_labels)\n",
        "        if mimic_df is not None and len(mimic_df) > 0:\n",
        "            all_metadata.append(mimic_df)\n",
        "    \n",
        "    print(f\"\\n  Combinando señales (esto puede tardar si hay muchas)...\")\n",
        "    X = np.concatenate(all_signals, axis=0)\n",
        "    print(f\"  ✓ Señales combinadas: {X.shape}\")\n",
        "    \n",
        "    print(f\"  Combinando labels...\")\n",
        "    y = np.concatenate(all_labels, axis=0)\n",
        "    print(f\"  ✓ Labels combinados: {len(y)} labels\")\n",
        "    \n",
        "    if all_metadata:\n",
        "        print(f\"  Combinando metadatos...\")\n",
        "        metadata = pd.concat(all_metadata, ignore_index=True)\n",
        "        print(f\"  ✓ Metadatos combinados: {len(metadata)} registros\")\n",
        "    else:\n",
        "        metadata = pd.DataFrame()\n",
        "    \n",
        "    # Liberar memoria\n",
        "    del all_signals, all_labels, all_metadata\n",
        "    if ptbxl_signals is not None:\n",
        "        del ptbxl_signals, ptbxl_labels\n",
        "    if mimic_signals is not None:\n",
        "        del mimic_signals, mimic_labels\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ Dataset combinado\")\n",
        "    print(f\"  Tiempo: {elapsed:.2f} segundos\")\n",
        "    print(f\"  Total registros: {len(X)}\")\n",
        "    print(f\"  Normales: {(y == 0).sum()}\")\n",
        "    print(f\"  Anómalos: {(y == 1).sum()}\")\n",
        "    print(f\"  Shape: {X.shape}\")\n",
        "    print(f\"  Memoria: {X.nbytes / 1024**3:.2f} GB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR construyendo dataset: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Balancear Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 4: Balanceando dataset\n",
            "================================================================================\n",
            "\n",
            "  Balanceando dataset...\n",
            "  Antes: 496244 registros\n",
            "    Normales: 193335\n",
            "    Anómalos: 302909\n",
            "\n",
            "✓ Dataset balanceado\n",
            "  Tiempo: 1293.67 segundos\n",
            "  Después: 386670 registros\n",
            "    Normales: 193335\n",
            "    Anómalos: 193335\n"
          ]
        }
      ],
      "source": [
        "if DO_BALANCE:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"PASO 4: Balanceando dataset\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(f\"\\n  Balanceando dataset...\")\n",
        "        print(f\"  Antes: {len(X)} registros\")\n",
        "        print(f\"    Normales: {(y == 0).sum()}\")\n",
        "        print(f\"    Anómalos: {(y == 1).sum()}\")\n",
        "        \n",
        "        X_balanced, y_balanced, balanced_indices = balance_dataset(X, y, return_indices=True)\n",
        "        \n",
        "        # Mapear metadatos\n",
        "        metadata = metadata.iloc[balanced_indices].copy()\n",
        "        metadata.reset_index(drop=True, inplace=True)\n",
        "        \n",
        "        X = X_balanced\n",
        "        y = y_balanced\n",
        "        \n",
        "        del X_balanced, y_balanced, balanced_indices\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        \n",
        "        print(f\"\\n✓ Dataset balanceado\")\n",
        "        print(f\"  Tiempo: {elapsed:.2f} segundos\")\n",
        "        print(f\"  Después: {len(X)} registros\")\n",
        "        print(f\"    Normales: {(y == 0).sum()}\")\n",
        "        print(f\"    Anómalos: {(y == 1).sum()}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR balanceando dataset: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "else:\n",
        "    print(\"\\n⏭ Saltando balanceo (DO_BALANCE=False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Crear Splits Train/Val/Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 5: Creando splits train/val/test\n",
            "================================================================================\n",
            "\n",
            "  Creando splits estratificados (70/15/15)...\n",
            "\n",
            "✗ ERROR creando splits: Unable to allocate 3.24 GiB for an array with shape (58001, 5000, 3) and data type float32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_32336\\1155616389.py\", line 9, in <module>\n",
            "    X_train, y_train, X_val, y_val, X_test, y_test = create_splits(\n",
            "                                                     ^^^^^^^^^^^^^^\n",
            "  File \"s:\\Proyecto final\\Books\\supervised_ecg_pipeline.py\", line 950, in create_splits\n",
            "    # Crear test split primero (más pequeño, ~15%)\n",
            "                                    ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 2944, in train_test_split\n",
            "    return list(\n",
            "           ^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 2946, in <genexpr>\n",
            "    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 349, in _safe_indexing\n",
            "    return _array_indexing(X, indices, indices_dtype, axis=axis)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 40, in _array_indexing\n",
            "    return array[key, ...] if axis == 0 else array[:, key]\n",
            "           ~~~~~^^^^^^^^^^\n",
            "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 3.24 GiB for an array with shape (58001, 5000, 3) and data type float32\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 3.24 GiB for an array with shape (58001, 5000, 3) and data type float32",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m start_time = time.time()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  Creando splits estratificados (70/15/15)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_train, y_train, X_val, y_val, X_test, y_test = \u001b[43mcreate_splits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.70\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Crear metadata para cada split\u001b[39;00m\n\u001b[32m     17\u001b[39m train_indices = np.arange(\u001b[38;5;28mlen\u001b[39m(y_train))\n",
            "\u001b[36mFile \u001b[39m\u001b[32ms:\\Proyecto final\\Books\\supervised_ecg_pipeline.py:950\u001b[39m, in \u001b[36mcreate_splits\u001b[39m\u001b[34m(X, y, train_ratio, val_ratio, test_ratio, random_state)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;66;03m# Luego separar train y val del resto\u001b[39;00m\n\u001b[32m    949\u001b[39m val_size = val_ratio / (train_ratio + val_ratio)\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m X_train, X_val, y_train, y_val = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, y_train, X_val, y_val, X_test, y_test\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2944\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2940\u001b[39m     train, test = \u001b[38;5;28mnext\u001b[39m(cv.split(X=arrays[\u001b[32m0\u001b[39m], y=stratify))\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2946\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2948\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2946\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2940\u001b[39m     train, test = \u001b[38;5;28mnext\u001b[39m(cv.split(X=arrays[\u001b[32m0\u001b[39m], y=stratify))\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m-> \u001b[39m\u001b[32m2946\u001b[39m         (_safe_indexing(a, train), \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:349\u001b[39m, in \u001b[36m_safe_indexing\u001b[39m\u001b[34m(X, indices, axis)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m warnings.warn(\n\u001b[32m    342\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[33mA data object with support for the dataframe interchange protocol\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwas passed, but scikit-learn does currently not know how to handle this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkind of data. Some array/list indexing will be tried.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    345\u001b[39m         category=\u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    346\u001b[39m     )\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:40\u001b[39m, in \u001b[36m_array_indexing\u001b[39m\u001b[34m(array, key, key_dtype, axis)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m     39\u001b[39m     key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
            "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.24 GiB for an array with shape (58001, 5000, 3) and data type float32"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PASO 5: Creando splits train/val/test\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"\\n  Creando splits estratificados (70/15/15)...\")\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = create_splits(\n",
        "        X, y,\n",
        "        train_ratio=0.70,\n",
        "        val_ratio=0.15,\n",
        "        test_ratio=0.15,\n",
        "    )\n",
        "    \n",
        "    # Crear metadata para cada split\n",
        "    train_indices = np.arange(len(y_train))\n",
        "    val_indices = np.arange(len(y_train), len(y_train) + len(y_val))\n",
        "    test_indices = np.arange(len(y_train) + len(y_val), len(y_train) + len(y_val) + len(y_test))\n",
        "    \n",
        "    metadata_train = metadata.iloc[train_indices].copy()\n",
        "    metadata_val = metadata.iloc[val_indices].copy()\n",
        "    metadata_test = metadata.iloc[test_indices].copy()\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ Splits creados (tiempo: {elapsed:.2f}s)\")\n",
        "    print(f\"\\n  Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "    print(f\"    Normales: {(y_train == 0).sum()}\")\n",
        "    print(f\"    Anómalos: {(y_train == 1).sum()}\")\n",
        "    print(f\"    Memoria: {X_train.nbytes / 1024**3:.2f} GB\")\n",
        "    \n",
        "    print(f\"\\n  Val: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "    print(f\"    Normales: {(y_val == 0).sum()}\")\n",
        "    print(f\"    Anómalos: {(y_val == 1).sum()}\")\n",
        "    print(f\"    Memoria: {X_val.nbytes / 1024**3:.2f} GB\")\n",
        "    \n",
        "    print(f\"\\n  Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "    print(f\"    Normales: {(y_test == 0).sum()}\")\n",
        "    print(f\"    Anómalos: {(y_test == 1).sum()}\")\n",
        "    print(f\"    Memoria: {X_test.nbytes / 1024**3:.2f} GB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR creando splits: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recargar módulo\n",
        "import importlib\n",
        "import supervised_ecg_pipeline\n",
        "importlib.reload(supervised_ecg_pipeline)\n",
        "from supervised_ecg_pipeline import create_splits_to_disk\n",
        "\n",
        "# Ahora ejecutar create_splits_to_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 5: Creando splits train/val/test\n",
            "================================================================================\n",
            "\n",
            "  Creando splits estratificados (70/15/15) y guardando en disco...\n",
            "  NOTA: Usando guardado directo en disco para evitar problemas de memoria\n",
            "    Calculando índices de splits...\n",
            "    Guardando test split (58001 registros)...\n",
            "    Guardando val split (58001 registros)...\n",
            "    Guardando train split (270668 registros) en chunks...\n",
            "    Cargando arrays en memoria (solo para compatibilidad)...\n",
            "    ✓ Splits guardados en S:\\Proyecto final\\data\\Datos_supervisados\\numpy\n",
            "    ✓ Índices guardados en S:\\Proyecto final\\data\\Datos_supervisados\\numpy\\split_indices.npz\n",
            "\n",
            "  Creando metadata para cada split...\n",
            "\n",
            "✓ Splits creados y guardados (tiempo: 773.80s)\n",
            "\n",
            "  Train: 270668 (70.0%)\n",
            "    Normales: 135334\n",
            "    Anómalos: 135334\n",
            "    Guardado en: S:\\Proyecto final\\data\\Datos_supervisados\\numpy\\X_train.npy\n",
            "\n",
            "  Val: 58001 (15.0%)\n",
            "    Normales: 29000\n",
            "    Anómalos: 29001\n",
            "    Guardado en: S:\\Proyecto final\\data\\Datos_supervisados\\numpy\\X_val.npy\n",
            "\n",
            "  Test: 58001 (15.0%)\n",
            "    Normales: 29001\n",
            "    Anómalos: 29000\n",
            "    Guardado en: S:\\Proyecto final\\data\\Datos_supervisados\\numpy\\X_test.npy\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PASO 5: Creando splits train/val/test\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"\\n  Creando splits estratificados (70/15/15) y guardando en disco...\")\n",
        "    print(f\"  NOTA: Usando guardado directo en disco para evitar problemas de memoria\")\n",
        "    \n",
        "    # Importar la nueva función si no está importada\n",
        "    from supervised_ecg_pipeline import create_splits_to_disk\n",
        "    \n",
        "    # Usar función que guarda directamente en disco\n",
        "    result = create_splits_to_disk(\n",
        "        X, y,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        train_ratio=0.70,\n",
        "        val_ratio=0.15,\n",
        "        test_ratio=0.15,\n",
        "        chunk_size=10000,  # Procesar en chunks de 10k registros\n",
        "        random_state=42,  # Usar el mismo random_state\n",
        "    )\n",
        "    \n",
        "    X_train, y_train, X_val, y_val, X_test, y_test, train_idx, val_idx, test_idx = result\n",
        "    \n",
        "    # Crear metadata para cada split usando los índices retornados\n",
        "    print(f\"\\n  Creando metadata para cada split...\")\n",
        "    metadata_train = metadata.iloc[train_idx].copy().reset_index(drop=True)\n",
        "    metadata_val = metadata.iloc[val_idx].copy().reset_index(drop=True)\n",
        "    metadata_test = metadata.iloc[test_idx].copy().reset_index(drop=True)\n",
        "    \n",
        "    # Limpiar índices\n",
        "    del train_idx, val_idx, test_idx\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    n_train = len(y_train)\n",
        "    n_val = len(y_val)\n",
        "    n_test = len(y_test)\n",
        "    \n",
        "    print(f\"\\n✓ Splits creados y guardados (tiempo: {elapsed:.2f}s)\")\n",
        "    print(f\"\\n  Train: {n_train} ({n_train/len(X)*100:.1f}%)\")\n",
        "    print(f\"    Normales: {(y_train == 0).sum()}\")\n",
        "    print(f\"    Anómalos: {(y_train == 1).sum()}\")\n",
        "    print(f\"    Guardado en: {OUTPUT_DIR / 'numpy' / 'X_train.npy'}\")\n",
        "    \n",
        "    print(f\"\\n  Val: {n_val} ({n_val/len(X)*100:.1f}%)\")\n",
        "    print(f\"    Normales: {(y_val == 0).sum()}\")\n",
        "    print(f\"    Anómalos: {(y_val == 1).sum()}\")\n",
        "    print(f\"    Guardado en: {OUTPUT_DIR / 'numpy' / 'X_val.npy'}\")\n",
        "    \n",
        "    print(f\"\\n  Test: {n_test} ({n_test/len(X)*100:.1f}%)\")\n",
        "    print(f\"    Normales: {(y_test == 0).sum()}\")\n",
        "    print(f\"    Anómalos: {(y_test == 1).sum()}\")\n",
        "    print(f\"    Guardado en: {OUTPUT_DIR / 'numpy' / 'X_test.npy'}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR creando splits: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Crear Folds Estratificados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 6: Creando folds estratificados (10 folds)\n",
            "================================================================================\n",
            "\n",
            "  Creando 10 folds estratificados...\n",
            "\n",
            "✓ 10 folds creados (tiempo: 0.21s)\n",
            "  Fold 1: Train=243601, Val=27067\n",
            "  Fold 2: Train=243601, Val=27067\n",
            "  Fold 3: Train=243601, Val=27067\n",
            "  ... (mostrando solo primeros 3 folds)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PASO 6: Creando folds estratificados (10 folds)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"\\n  Creando 10 folds estratificados...\")\n",
        "    folds = create_stratified_folds(X_train, y_train, n_splits=10)\n",
        "    folds_train = [fold[0] for fold in folds]\n",
        "    folds_val = [fold[1] for fold in folds]\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ {len(folds)} folds creados (tiempo: {elapsed:.2f}s)\")\n",
        "    for i, (train_idx, val_idx) in enumerate(folds[:3]):  # Mostrar solo primeros 3\n",
        "        print(f\"  Fold {i+1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
        "    print(f\"  ... (mostrando solo primeros 3 folds)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR creando folds: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Guardar Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PASO 7: Guardando dataset\n",
            "================================================================================\n",
            "\n",
            "  Preparando guardado...\n",
            "  X_train: (270668, 5000, 3) (15.12 GB)\n",
            "  X_val: (58001, 5000, 3) (3.24 GB)\n",
            "  X_test: (58001, 5000, 3) (3.24 GB)\n",
            "\n",
            "Iniciando guardado de dataset...\n",
            "  Guardando arrays numpy...\n",
            "    ⏭ X_train.npy ya existe en disco, omitiendo guardado\n",
            "    ⏭ y_train.npy ya existe en disco, omitiendo guardado\n",
            "    ⏭ X_val.npy ya existe en disco, omitiendo guardado\n",
            "    ⏭ y_val.npy ya existe en disco, omitiendo guardado\n",
            "    ⏭ X_test.npy ya existe en disco, omitiendo guardado\n",
            "    ⏭ y_test.npy ya existe en disco, omitiendo guardado\n",
            "  Guardando metadatos...\n",
            "    Guardando master_labels.csv...\n",
            "    ✓ master_labels.csv guardado\n",
            "    Guardando master_labels_full.csv...\n",
            "    ✓ master_labels_full.csv guardado\n",
            "  Guardando folds...\n",
            "    Guardando folds_train_indices.npy...\n",
            "    ✓ folds_train_indices.npy guardado\n",
            "    Guardando folds_val_indices.npy...\n",
            "    ✓ folds_val_indices.npy guardado\n",
            "\n",
            "✓ Dataset guardado exitosamente en S:\\Proyecto final\\data\\Datos_supervisados\n",
            "  Train: 270668 muestras\n",
            "  Val: 58001 muestras\n",
            "  Test: 58001 muestras\n",
            "  Folds: 10 folds estratificados\n",
            "\n",
            "✓ Dataset guardado exitosamente\n",
            "  Tiempo: 0.04 minutos\n",
            "  Ubicación: S:\\Proyecto final\\data\\Datos_supervisados\n"
          ]
        }
      ],
      "source": [
        "# Recargar módulo\n",
        "import importlib\n",
        "import supervised_ecg_pipeline\n",
        "importlib.reload(supervised_ecg_pipeline)\n",
        "from supervised_ecg_pipeline import save_dataset\n",
        "print(\"=\" * 80)\n",
        "print(\"PASO 7: Guardando dataset\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"\\n  Preparando guardado...\")\n",
        "    print(f\"  X_train: {X_train.shape} ({X_train.nbytes / 1024**3:.2f} GB)\")\n",
        "    print(f\"  X_val: {X_val.shape} ({X_val.nbytes / 1024**3:.2f} GB)\")\n",
        "    print(f\"  X_test: {X_test.shape} ({X_test.nbytes / 1024**3:.2f} GB)\")\n",
        "    \n",
        "    save_dataset(\n",
        "        X_train, y_train,\n",
        "        X_val, y_val,\n",
        "        X_test, y_test,\n",
        "        metadata_train, metadata_val, metadata_test,\n",
        "        folds_train, folds_val,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n✓ Dataset guardado exitosamente\")\n",
        "    print(f\"  Tiempo: {elapsed/60:.2f} minutos\")\n",
        "    print(f\"  Ubicación: {OUTPUT_DIR}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR guardando dataset: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Verificar Datos Guardados (Opcional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "VERIFICACIÓN: Archivos guardados\n",
            "================================================================================\n",
            "  ✓ numpy/X_train.npy (15487.75 MB)\n",
            "  ✓ numpy/y_train.npy (2.07 MB)\n",
            "  ✓ numpy/X_val.npy (3318.84 MB)\n",
            "  ✓ numpy/y_val.npy (0.44 MB)\n",
            "  ✓ numpy/X_test.npy (3318.84 MB)\n",
            "  ✓ numpy/y_test.npy (0.44 MB)\n",
            "  ✓ metadata/master_labels.csv (23.29 MB)\n",
            "  ✓ metadata/master_labels_full.csv (35.31 MB)\n",
            "  ✓ metadata/folds_train_indices.npy (18.59 MB)\n",
            "  ✓ metadata/folds_val_indices.npy (2.07 MB)\n",
            "\n",
            "✓ Todos los archivos se guardaron correctamente\n"
          ]
        }
      ],
      "source": [
        "# Verificar que los archivos se guardaron correctamente\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VERIFICACIÓN: Archivos guardados\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import os\n",
        "\n",
        "files_to_check = [\n",
        "    \"numpy/X_train.npy\",\n",
        "    \"numpy/y_train.npy\",\n",
        "    \"numpy/X_val.npy\",\n",
        "    \"numpy/y_val.npy\",\n",
        "    \"numpy/X_test.npy\",\n",
        "    \"numpy/y_test.npy\",\n",
        "    \"metadata/master_labels.csv\",\n",
        "    \"metadata/master_labels_full.csv\",\n",
        "    \"metadata/folds_train_indices.npy\",\n",
        "    \"metadata/folds_val_indices.npy\",\n",
        "]\n",
        "\n",
        "all_ok = True\n",
        "for file_path in files_to_check:\n",
        "    full_path = OUTPUT_DIR / file_path\n",
        "    if full_path.exists():\n",
        "        size_mb = full_path.stat().st_size / 1024**2\n",
        "        print(f\"  ✓ {file_path} ({size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  ✗ {file_path} - NO ENCONTRADO\")\n",
        "        all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n✓ Todos los archivos se guardaron correctamente\")\n",
        "else:\n",
        "    print(\"\\n⚠ Algunos archivos no se encontraron\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualizar Ejemplos (Opcional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generando 3 ejemplos visuales...\n",
            "  Generando ejemplo 1/3: mimic_13744125_41662601...\n",
            "  Generando ejemplo 2/3: mimic_18832132_48870396...\n",
            "  Generando ejemplo 3/3: mimic_12182445_49330246...\n",
            "\n",
            "✓ 3 ejemplos visuales creados en S:\\Proyecto final\\data\\Datos_supervisados\\raw_examples\n"
          ]
        }
      ],
      "source": [
        "# Visualizar algunos ejemplos\n",
        "from supervised_ecg_pipeline import plot_ecg_comparison\n",
        "\n",
        "n_examples = 3  # Número de ejemplos a visualizar\n",
        "\n",
        "print(f\"\\nGenerando {n_examples} ejemplos visuales...\")\n",
        "\n",
        "ensure_dir(OUTPUT_DIR / \"raw_examples\")\n",
        "\n",
        "indices = np.random.choice(len(X_test), size=min(n_examples, len(X_test)), replace=False)\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    signal = X_test[idx]\n",
        "    label = y_test[idx]\n",
        "    record_id = metadata_test.iloc[idx][\"record_id\"]\n",
        "    \n",
        "    title = f\"{record_id} - {'NORMAL' if label == 0 else 'ANÓMALO'}\"\n",
        "    save_path = OUTPUT_DIR / \"raw_examples\" / f\"example_{i+1:03d}_{str(record_id).replace('/', '_')}.png\"\n",
        "    \n",
        "    print(f\"  Generando ejemplo {i+1}/{n_examples}: {record_id}...\")\n",
        "    plot_ecg_comparison(\n",
        "        raw=signal,\n",
        "        filtered=None,\n",
        "        normalized=signal,\n",
        "        title=title,\n",
        "        save_path=save_path,\n",
        "    )\n",
        "\n",
        "print(f\"\\n✓ {n_examples} ejemplos visuales creados en {OUTPUT_DIR / 'raw_examples'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RESUMEN FINAL\n",
            "================================================================================\n",
            "\n",
            "✓ Pipeline completado exitosamente\n",
            "\n",
            "Dataset guardado en: S:\\Proyecto final\\data\\Datos_supervisados\n",
            "\n",
            "Estructura:\n",
            "  S:\\Proyecto final\\data\\Datos_supervisados/\n",
            "    metadata/\n",
            "      master_labels.csv\n",
            "      master_labels_full.csv\n",
            "      folds_train_indices.npy\n",
            "      folds_val_indices.npy\n",
            "    numpy/\n",
            "      X_train.npy, y_train.npy\n",
            "      X_val.npy, y_val.npy\n",
            "      X_test.npy, y_test.npy\n",
            "    raw_examples/\n",
            "      (ejemplos visuales)\n",
            "\n",
            "Estadísticas:\n",
            "  Total: 386670 registros\n",
            "  Train: 270668 (70.0%)\n",
            "  Val: 58001 (15.0%)\n",
            "  Test: 58001 (15.0%)\n",
            "  Shape de señales: (386670, 5000, 3) (N, T, C)\n",
            "    T=5000 muestras (10 segundos)\n",
            "    C=3 leads (II, V1, V5)\n",
            "  Frecuencia de muestreo: 500 Hz\n",
            "  Duración: 10 segundos\n",
            "  Folds: 10 folds estratificados\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RESUMEN FINAL\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n✓ Pipeline completado exitosamente\")\n",
        "print(f\"\\nDataset guardado en: {OUTPUT_DIR}\")\n",
        "print(f\"\\nEstructura:\")\n",
        "print(f\"  {OUTPUT_DIR}/\")\n",
        "print(f\"    metadata/\")\n",
        "print(f\"      master_labels.csv\")\n",
        "print(f\"      master_labels_full.csv\")\n",
        "print(f\"      folds_train_indices.npy\")\n",
        "print(f\"      folds_val_indices.npy\")\n",
        "print(f\"    numpy/\")\n",
        "print(f\"      X_train.npy, y_train.npy\")\n",
        "print(f\"      X_val.npy, y_val.npy\")\n",
        "print(f\"      X_test.npy, y_test.npy\")\n",
        "print(f\"    raw_examples/\")\n",
        "print(f\"      (ejemplos visuales)\")\n",
        "print(f\"\\nEstadísticas:\")\n",
        "print(f\"  Total: {len(X)} registros\")\n",
        "print(f\"  Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Val: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Shape de señales: {X.shape} (N, T, C)\")\n",
        "print(f\"    T={X.shape[1]} muestras (10 segundos)\")\n",
        "print(f\"    C={X.shape[2]} leads (II, V1, V5)\")\n",
        "print(f\"  Frecuencia de muestreo: 500 Hz\")\n",
        "print(f\"  Duración: 10 segundos\")\n",
        "print(f\"  Folds: {len(folds_train)} folds estratificados\")\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
