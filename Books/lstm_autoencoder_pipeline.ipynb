{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü´Ä LSTM Autoencoder para Detecci√≥n de Anomal√≠as en Series Temporales\n",
        "\n",
        "Este notebook implementa un **LSTM Autoencoder** para detecci√≥n de anomal√≠as en series temporales (por ejemplo, se√±ales ECG).\n",
        "\n",
        "**Caracter√≠sticas principales:**\n",
        "- Entrenamiento no supervisado (solo con ejemplos normales)\n",
        "- Validaci√≥n y test con etiquetas para evaluaci√≥n\n",
        "- Integraci√≥n con MLflow para tracking de experimentos\n",
        "- Orquestaci√≥n con Prefect 2.x\n",
        "- Soporte autom√°tico para GPU (RTX 5080 compatible)\n",
        "\n",
        "> ‚ö†Ô∏è **IMPORTANTE EN WINDOWS:** Ejecuta la celda de **Setup DLLs CUDA** (celda 3) **ANTES** de la celda de imports. Esto es necesario para que PyTorch pueda cargar las DLLs de CUDA correctamente.\n",
        "\n",
        "> ‚ñ∂Ô∏è **Instrucciones:** \n",
        "> 1. Ejecuta la celda de **Setup DLLs CUDA** primero\n",
        "> 2. Luego ejecuta todas las dem√°s celdas en orden\n",
        "> 3. Completa la funci√≥n `load_raw_data()` con tu l√≥gica de carga de datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã √çndice\n",
        "\n",
        "1. **Configuraci√≥n general** - Imports, semillas, dispositivo, hiperpar√°metros\n",
        "2. **Carga y preparaci√≥n de datos** - Funciones para cargar y preparar datos\n",
        "3. **Definici√≥n del modelo LSTM Autoencoder** - Arquitectura del modelo\n",
        "4. **Funciones de entrenamiento y evaluaci√≥n** - Loops de entrenamiento y validaci√≥n\n",
        "5. **Integraci√≥n con MLflow** - Configuraci√≥n y logging\n",
        "6. **Orquestaci√≥n con Prefect** - Flujo principal con Prefect\n",
        "7. **Ejecuci√≥n del flujo completo** - Celda final para ejecutar todo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. ‚öôÔ∏è Configuraci√≥n General\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: c:\\Python311\\python.exe\n",
            "Working dir: S:\\Proyecto final\\Books\n",
            "DLL directories a√±adidos:\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp\n",
            "‚è≥ Instalando mlflow>=2.16 ...\n",
            "‚è≥ Instalando prefect>=3 ...\n",
            "‚è≥ Instalando scikit-learn ...\n",
            "‚úî matplotlib ya instalado\n",
            "‚úî pandas ya instalado\n",
            "‚úî numpy ya instalado\n",
            "\n",
            "Torch info:\n",
            "  - torch_version: 2.10.0.dev20251121+cu128\n",
            "  - cuda_version: 12.8\n",
            "  - cuda_available: True\n",
            "GPU detectada: NVIDIA GeForce RTX 5080 | SM 120\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üîß Setup RTX 5080 ‚Äî dependencias + CUDA DLL\n",
        "# Ejecuta una sola vez (o tras actualizar drivers/librer√≠as)\n",
        "# ========================================\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from textwrap import dedent\n",
        "\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"Working dir: {Path.cwd().resolve()}\")\n",
        "\n",
        "# Rutas candidatas para DLLs de CUDA\n",
        "CUDA_CANDIDATES = [\n",
        "    os.environ.get(\"CUDA_PATH\"),\n",
        "    os.environ.get(\"CUDA_PATH_V12_8\"),\n",
        "    r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\",\n",
        "    r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\",\n",
        "    r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp\",\n",
        "    r\"C:\\Program Files\\NVIDIA\\CUDNN\",\n",
        "]\n",
        "\n",
        "# A√±adir rutas DLL en Windows (necesario antes de importar torch)\n",
        "added = []\n",
        "if hasattr(os, \"add_dll_directory\"):\n",
        "    for candidate in CUDA_CANDIDATES:\n",
        "        if not candidate:\n",
        "            continue\n",
        "        path = Path(candidate)\n",
        "        if path.is_dir():\n",
        "            try:\n",
        "                os.add_dll_directory(str(path))\n",
        "                added.append(str(path))\n",
        "            except (FileNotFoundError, OSError):\n",
        "                pass\n",
        "\n",
        "if added:\n",
        "    print(\"DLL directories a√±adidos:\")\n",
        "    for path in added:\n",
        "        print(f\"  - {path}\")\n",
        "\n",
        "# Instalar dependencias base si no est√°n instaladas\n",
        "BASE_PACKAGES = [\n",
        "    \"mlflow>=2.16\",\n",
        "    \"prefect>=3\",\n",
        "    \"scikit-learn\",\n",
        "    \"matplotlib\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "]\n",
        "\n",
        "def pip_install(spec: str) -> None:\n",
        "    module_name = spec.split(\"==\")[0].split(\"[\")[0].replace(\"-\", \"_\")\n",
        "    try:\n",
        "        __import__(module_name)\n",
        "        print(f\"‚úî {spec} ya instalado\")\n",
        "    except Exception:\n",
        "        print(f\"‚è≥ Instalando {spec} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", spec])\n",
        "\n",
        "for pkg in BASE_PACKAGES:\n",
        "    pip_install(pkg)\n",
        "\n",
        "# Comando para instalar PyTorch nightly con CUDA 12.8 (para RTX 5080)\n",
        "TORCH_INSTALL_CMD = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"pip\",\n",
        "    \"install\",\n",
        "    \"--upgrade\",\n",
        "    \"--pre\",\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"torchaudio\",\n",
        "    \"--index-url\",\n",
        "    \"https://download.pytorch.org/whl/nightly/cu128\",\n",
        "]\n",
        "\n",
        "def ensure_torch_cuda() -> \"tuple[object | None, dict]\":\n",
        "    \"\"\"Importa torch, o instala la nightly cu128 si hace falta.\"\"\"\n",
        "    info: dict[str, str | float | bool] = {}\n",
        "    try:\n",
        "        import torch  # type: ignore\n",
        "        info[\"torch_version\"] = getattr(torch, \"__version__\", \"desconocida\")\n",
        "        info[\"cuda_version\"] = getattr(getattr(torch, \"version\", object()), \"cuda\", \"desconocida\")\n",
        "        info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "        if \"cu128\" not in info[\"torch_version\"] and not str(info[\"cuda_version\"]).startswith(\"12.8\"):\n",
        "            raise RuntimeError(\n",
        "                f\"Build {info['torch_version']} no es cu128. Se reinstalar√° la nightly para RTX 5080.\"\n",
        "            )\n",
        "        return torch, info\n",
        "    except Exception as err:\n",
        "        print(\"‚ö†Ô∏è Torch no usable todav√≠a:\", err)\n",
        "        print(\"   Desinstalando PyTorch corrupto...\")\n",
        "        # Desinstalar primero\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\", \"torchaudio\"])\n",
        "        print(\"   Instalando nightly cu128 desde PyTorch (puede tardar).\")\n",
        "        subprocess.check_call(TORCH_INSTALL_CMD)\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚ö†Ô∏è IMPORTANTE: PyTorch fue reinstalado.\")\n",
        "        print(\"   DEBES REINICIAR EL KERNEL DE JUPYTER ahora:\")\n",
        "        print(\"   Kernel ‚Üí Restart Kernel\")\n",
        "        print(\"   Luego ejecuta esta celda de nuevo.\")\n",
        "        print(\"=\"*60)\n",
        "        # Intentar importar de todas formas (puede fallar, pero al menos intentamos)\n",
        "        import importlib\n",
        "        import time\n",
        "        time.sleep(2)\n",
        "        importlib.invalidate_caches()\n",
        "        try:\n",
        "            import torch  # type: ignore\n",
        "            info[\"torch_version\"] = getattr(torch, \"__version__\", \"desconocida\")\n",
        "            info[\"cuda_version\"] = getattr(getattr(torch, \"version\", object()), \"cuda\", \"desconocida\")\n",
        "            info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "            return torch, info\n",
        "        except Exception as e2:\n",
        "            print(f\"\\n‚ùå No se pudo importar PyTorch despu√©s de reinstalar: {e2}\")\n",
        "            print(\"   Por favor, REINICIA EL KERNEL y ejecuta esta celda de nuevo.\")\n",
        "            raise RuntimeError(\"Reinicia el kernel de Jupyter y ejecuta esta celda de nuevo.\") from e2\n",
        "\n",
        "# Intentar importar/instalar PyTorch\n",
        "torch, torch_info = ensure_torch_cuda()\n",
        "\n",
        "print(\"\\nTorch info:\")\n",
        "for k, v in torch_info.items():\n",
        "    print(f\"  - {k}: {v}\")\n",
        "\n",
        "if torch_info.get(\"cuda_available\"):\n",
        "    try:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        cc = torch.cuda.get_device_properties(0)\n",
        "        print(f\"GPU detectada: {gpu_name} | SM {cc.major}{cc.minor}\")\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è CUDA disponible pero no se pudo consultar GPU:\", e)\n",
        "else:\n",
        "    print(dedent(\n",
        "        \"\"\"\n",
        "        ‚ö†Ô∏è CUDA sigue inactiva. Revisa drivers / reinicia kernel tras la instalaci√≥n.\n",
        "        Si el problema contin√∫a, ejecuta manualmente:\n",
        "          pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
        "        \"\"\"\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Todos los imports completados\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Imports y dependencias\n",
        "# ========================================\n",
        "# ‚ö†Ô∏è IMPORTANTE: Ejecuta la celda anterior (Setup DLLs) antes de esta celda\n",
        "# torch ya est√° importado en la celda anterior\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# torch ya est√° importado en la celda anterior, solo importamos los subm√≥dulos\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        ")\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from prefect import task, flow\n",
        "from prefect.tasks import NO_CACHE\n",
        "\n",
        "print(\"‚úì Todos los imports completados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# DIAGN√ìSTICO: Verificar datos despu√©s de cargar\n",
        "# ========================================\n",
        "def diagnosticar_datos(\n",
        "    X_train_norm: np.ndarray,\n",
        "    X_val_norm: np.ndarray,\n",
        "    X_test_norm: np.ndarray,\n",
        "    nombre: str = \"DATOS RAW\"\n",
        "):\n",
        "    \"\"\"Funci√≥n para diagnosticar el estado de los datos.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"üîç DIAGN√ìSTICO: {nombre}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for nombre_arr, arr in [\n",
        "        (\"X_train_norm\", X_train_norm),\n",
        "        (\"X_val_norm\", X_val_norm),\n",
        "        (\"X_test_norm\", X_test_norm),\n",
        "    ]:\n",
        "        if len(arr) == 0:\n",
        "            print(f\"\\n{nombre_arr}: Array vac√≠o\")\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n{nombre_arr} (shape: {arr.shape}):\")\n",
        "        print(f\"  - Min: {arr.min():.6f}, Max: {arr.max():.6f}\")\n",
        "        print(f\"  - Mean: {arr.mean():.6f}, Std: {arr.std():.6f}\")\n",
        "        \n",
        "        # Solo mostrar valores √∫nicos si no son demasiados\n",
        "        unique_vals = np.unique(arr.flatten())\n",
        "        if len(unique_vals) <= 10:\n",
        "            print(f\"  - Valores √∫nicos: {unique_vals[:10]}\")\n",
        "        else:\n",
        "            print(f\"  - Valores √∫nicos (primeros 5): {unique_vals[:5]}\")\n",
        "        \n",
        "        n_zeros = (arr == 0).sum()\n",
        "        n_nan = np.isnan(arr).sum()\n",
        "        n_inf = np.isinf(arr).sum()\n",
        "        total = arr.size\n",
        "        \n",
        "        print(f\"  - Zeros: {n_zeros:,} ({100*n_zeros/total:.2f}%)\")\n",
        "        print(f\"  - NaN: {n_nan:,} ({100*n_nan/total:.2f}%)\")\n",
        "        print(f\"  - Inf: {n_inf:,} ({100*n_inf/total:.2f}%)\")\n",
        "        \n",
        "        if arr.std() < 1e-6:\n",
        "            print(f\"  ‚ö† ADVERTENCIA: Std muy peque√±o ({arr.std():.6e}), datos podr√≠an estar constantes\")\n",
        "        if n_zeros == total:\n",
        "            print(f\"  ‚ùå ERROR: Todos los valores son cero!\")\n",
        "        if arr.max() == 0 and arr.min() == 0:\n",
        "            print(f\"  ‚ùå ERROR: Array completamente en cero!\")\n",
        "        if arr.std() == 0:\n",
        "            print(f\"  ‚ùå ERROR: Desviaci√≥n est√°ndar es cero! Datos constantes!\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Funci√≥n mejorada de limpieza de datos\n",
        "# ========================================\n",
        "def clean_data_smart(X: np.ndarray, fill_value: str = \"mean\") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Limpia NaN e Inf de forma inteligente.\n",
        "    \n",
        "    Args:\n",
        "        X: Array a limpiar\n",
        "        fill_value: Qu√© usar para reemplazar NaN/Inf (\"mean\", \"median\", o \"zero\")\n",
        "    \n",
        "    Returns:\n",
        "        Array limpio\n",
        "    \"\"\"\n",
        "    X = X.copy()  # No modificar el original\n",
        "    \n",
        "    # Verificar si hay NaN o Inf\n",
        "    has_nan = np.any(np.isnan(X))\n",
        "    has_inf = np.any(np.isinf(X))\n",
        "    \n",
        "    if not (has_nan or has_inf):\n",
        "        return X\n",
        "    \n",
        "    n_nan = np.isnan(X).sum()\n",
        "    n_inf = np.isinf(X).sum()\n",
        "    \n",
        "    print(f\"    ‚ö† Encontrados {n_nan} NaN y {n_inf} Inf\")\n",
        "    \n",
        "    # Calcular valor de reemplazo\n",
        "    if fill_value == \"mean\":\n",
        "        # Calcular media solo de valores v√°lidos\n",
        "        valid_mask = np.isfinite(X)\n",
        "        if valid_mask.any():\n",
        "            fill_val = np.mean(X[valid_mask])\n",
        "            print(f\"    ‚Üí Reemplazando con media de valores v√°lidos: {fill_val:.6f}\")\n",
        "        else:\n",
        "            fill_val = 0.0\n",
        "            print(f\"    ‚ö† No hay valores v√°lidos, usando 0\")\n",
        "    elif fill_value == \"median\":\n",
        "        valid_mask = np.isfinite(X)\n",
        "        if valid_mask.any():\n",
        "            fill_val = np.median(X[valid_mask])\n",
        "            print(f\"    ‚Üí Reemplazando con mediana de valores v√°lidos: {fill_val:.6f}\")\n",
        "        else:\n",
        "            fill_val = 0.0\n",
        "            print(f\"    ‚ö† No hay valores v√°lidos, usando 0\")\n",
        "    else:  # \"zero\"\n",
        "        fill_val = 0.0\n",
        "        print(f\"    ‚Üí Reemplazando con 0\")\n",
        "    \n",
        "    # Reemplazar NaN e Inf\n",
        "    X = np.nan_to_num(X, nan=fill_val, posinf=fill_val, neginf=fill_val)\n",
        "    \n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Semilla fijada: 42\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Configuraci√≥n de semillas aleatorias\n",
        "# ========================================\n",
        "def set_seed_everywhere(seed: int = 42) -> None:\n",
        "    \"\"\"Fija semillas para reproducibilidad.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 42\n",
        "set_seed_everywhere(SEED)\n",
        "print(f\"‚úì Semilla fijada: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì GPU detectada: NVIDIA GeForce RTX 5080\n",
            "  CUDA Version: 12.8\n",
            "  PyTorch Version: 2.10.0.dev20251121+cu128\n",
            "Dispositivo seleccionado: cuda\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Configuraci√≥n de dispositivo (GPU/CPU)\n",
        "# ========================================\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Detecta y configura el dispositivo (GPU si est√° disponible).\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"‚úì GPU detectada: {gpu_name}\")\n",
        "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"  PyTorch Version: {torch.__version__}\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"‚ö† GPU no disponible, usando CPU\")\n",
        "    return device\n",
        "\n",
        "DEVICE = get_device()\n",
        "print(f\"Dispositivo seleccionado: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Configuraci√≥n cargada:\n",
            "{\n",
            "  \"INPUT_SIZE\": 3,\n",
            "  \"HIDDEN_SIZES_ENCODER\": [\n",
            "    64,\n",
            "    32\n",
            "  ],\n",
            "  \"HIDDEN_SIZES_DECODER\": [\n",
            "    32,\n",
            "    64\n",
            "  ],\n",
            "  \"LATENT_DIM\": 16,\n",
            "  \"NUM_LAYERS\": 2,\n",
            "  \"DROPOUT\": 0.1,\n",
            "  \"EPOCHS\": 30,\n",
            "  \"BATCH_SIZE\": 64,\n",
            "  \"LEARNING_RATE\": 0.001,\n",
            "  \"WEIGHT_DECAY\": 1e-05,\n",
            "  \"CLIP_GRAD_NORM\": 1.0,\n",
            "  \"THRESHOLD_PERCENTILE\": 95.0,\n",
            "  \"MLFLOW_EXPERIMENT_NAME\": \"LSTM_AE_Anomalias_ECG_v1\",\n",
            "  \"MLFLOW_TRACKING_URI\": null,\n",
            "  \"OUTPUT_DIR\": \"./outputs\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# ‚öôÔ∏è CONFIGURACI√ìN DE HIPERPAR√ÅMETROS\n",
        "# ========================================\n",
        "# Modifica estos valores para ajustar la arquitectura y entrenamiento\n",
        "\n",
        "CONFIG = {\n",
        "    # ===== ARQUITECTURA DEL MODELO =====\n",
        "    \"INPUT_SIZE\": 3,  # Dimensi√≥n de entrada por timestep (ej: 3 derivaciones ECG)\n",
        "    \"HIDDEN_SIZES_ENCODER\": [64, 32],  # Neuronas por capa LSTM del encoder\n",
        "    \"HIDDEN_SIZES_DECODER\": [32, 64],  # Neuronas por capa LSTM del decoder\n",
        "    \"LATENT_DIM\": 16,  # Dimensi√≥n del espacio latente\n",
        "    \"NUM_LAYERS\": 2,  # N√∫mero de capas LSTM (debe coincidir con longitudes de HIDDEN_SIZES)\n",
        "    \"DROPOUT\": 0.1,  # Dropout en capas LSTM\n",
        "    \n",
        "    # ===== ENTRENAMIENTO =====\n",
        "    \"EPOCHS\": 30,\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"LEARNING_RATE\": 0.001,\n",
        "    \"WEIGHT_DECAY\": 1e-5,\n",
        "    \"CLIP_GRAD_NORM\": 1.0,  # None para desactivar gradient clipping\n",
        "    \n",
        "    # ===== DETECCI√ìN DE ANOMAL√çAS =====\n",
        "    \"THRESHOLD_PERCENTILE\": 95.0,  # Percentil para calcular umbral de anomal√≠as\n",
        "    \n",
        "    # ===== MLFLOW =====\n",
        "    \"MLFLOW_EXPERIMENT_NAME\": \"LSTM_AE_Anomalias_ECG_v1\",  # ‚ö†Ô∏è CAMBIA ESTE NOMBRE PARA CADA EXPERIMENTO\n",
        "    \"MLFLOW_TRACKING_URI\": None,  # None = usa el directorio local\n",
        "    \n",
        "    # ===== RUTAS =====\n",
        "    \"OUTPUT_DIR\": \"./outputs\",  # Directorio para guardar modelos y artefactos\n",
        "}\n",
        "\n",
        "# Crear directorio de salida\n",
        "Path(CONFIG[\"OUTPUT_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úì Configuraci√≥n cargada:\")\n",
        "print(json.dumps(CONFIG, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. üìÇ Carga y Preparaci√≥n de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# TODO: COMPLETA ESTA FUNCI√ìN CON TU L√ìGICA DE CARGA DE DATOS\n",
        "# ========================================\n",
        "def load_raw_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Carga los datos crudos y los divide en train/val/test.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple con:\n",
        "        - X_train_norm: Array numpy de forma (N_train, T, INPUT_SIZE) con ejemplos NORMALES para entrenamiento\n",
        "        - X_val_norm: Array numpy de forma (N_val, T, INPUT_SIZE) con ejemplos NORMALES para validaci√≥n\n",
        "        - X_val_anom: Array numpy de forma (N_val_anom, T, INPUT_SIZE) con ejemplos AN√ìMALOS para validaci√≥n\n",
        "        - y_val: Array numpy de forma (N_val_norm + N_val_anom,) con etiquetas 0=normal, 1=an√≥malo para validaci√≥n\n",
        "        - X_test_norm: Array numpy de forma (N_test, T, INPUT_SIZE) con ejemplos NORMALES para test\n",
        "        - X_test_anom: Array numpy de forma (N_test_anom, T, INPUT_SIZE) con ejemplos AN√ìMALOS para test\n",
        "        - y_test: Array numpy de forma (N_test_norm + N_test_anom,) con etiquetas 0=normal, 1=an√≥malo para test\n",
        "    \"\"\"\n",
        "    # ========================================\n",
        "    # TODO: REEMPLAZA ESTE C√ìDIGO CON TU L√ìGICA\n",
        "    # ========================================\n",
        "    \n",
        "    # EJEMPLO: Datos dummy para testing (ELIMINA ESTO Y PON TU C√ìDIGO)\n",
        "    print(\"‚ö†Ô∏è ADVERTENCIA: Usando datos dummy. Reemplaza esta funci√≥n con tu l√≥gica de carga.\")\n",
        "    \n",
        "    # Par√°metros de ejemplo\n",
        "    T = 5000  # Longitud de la secuencia temporal\n",
        "    INPUT_SIZE = CONFIG[\"INPUT_SIZE\"]\n",
        "    \n",
        "    # Generar datos dummy\n",
        "    np.random.seed(SEED)\n",
        "    \n",
        "    # Train: solo normales (1000 ejemplos)\n",
        "    X_train_norm = np.random.randn(1000, T, INPUT_SIZE).astype(np.float32)\n",
        "    \n",
        "    # Val: normales (200) + an√≥malos (50)\n",
        "    X_val_norm = np.random.randn(200, T, INPUT_SIZE).astype(np.float32)\n",
        "    X_val_anom = np.random.randn(50, T, INPUT_SIZE).astype(np.float32) + 2.0  # An√≥malos con offset\n",
        "    y_val = np.concatenate([np.zeros(len(X_val_norm)), np.ones(len(X_val_anom))]).astype(np.int64)\n",
        "    \n",
        "    # Test: normales (200) + an√≥malos (50)\n",
        "    X_test_norm = np.random.randn(200, T, INPUT_SIZE).astype(np.float32)\n",
        "    X_test_anom = np.random.randn(50, T, INPUT_SIZE).astype(np.float32) + 2.0  # An√≥malos con offset\n",
        "    y_test = np.concatenate([np.zeros(len(X_test_norm)), np.ones(len(X_test_anom))]).astype(np.int64)\n",
        "    \n",
        "    # ========================================\n",
        "    # FIN DEL C√ìDIGO DUMMY - PON TU L√ìGICA AQU√ç\n",
        "    # ========================================\n",
        "    \n",
        "    return X_train_norm, X_val_norm, X_val_anom, y_val, X_test_norm, X_test_anom, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Preparaci√≥n de datos (PASA DIRECTAMENTE - sin modificaci√≥n)\n",
        "# ========================================\n",
        "def normalize_data(\n",
        "    X_train: np.ndarray,\n",
        "    X_val_norm: np.ndarray,\n",
        "    X_val_anom: np.ndarray,\n",
        "    X_test_norm: np.ndarray,\n",
        "    X_test_anom: np.ndarray,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Pasa los datos directamente sin modificarlos.\n",
        "    Los datos de Datos_no_supervisados ya vienen procesados y listos para usar.\n",
        "    \n",
        "    Returns:\n",
        "        Tupla con los mismos datos (sin modificar) y diccionario con estad√≠sticas de referencia.\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"üìä PREPARACI√ìN DE DATOS\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"‚úì Los datos de 'Datos_no_supervisados' ya vienen procesados y listos.\")\n",
        "    print(\"  ‚Üí Pasando datos directamente SIN modificaciones (sin limpieza ni normalizaci√≥n)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Usar datos tal como est√°n - SIN MODIFICACIONES\n",
        "    X_train_norm = X_train\n",
        "    X_val_norm_norm = X_val_norm\n",
        "    X_val_anom_norm = X_val_anom\n",
        "    X_test_norm_norm = X_test_norm\n",
        "    X_test_anom_norm = X_test_anom\n",
        "    \n",
        "    # Calcular estad√≠sticas solo para referencia/logging (no para normalizar)\n",
        "    mean = np.mean(X_train, axis=(0, 1))\n",
        "    std = np.std(X_train, axis=(0, 1))\n",
        "    stats = {\n",
        "        \"mean\": mean,\n",
        "        \"std\": std,\n",
        "        \"normalized\": False,\n",
        "        \"cleaned\": False,\n",
        "        \"note\": \"Datos usados directamente sin modificaci√≥n\"\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nüìà Estad√≠sticas de referencia (solo para logging):\")\n",
        "    print(f\"  Mean: {mean}\")\n",
        "    print(f\"  Std: {std}\")\n",
        "    print(f\"\\n‚úì Datos listos para usar (sin modificaciones)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return X_train_norm, X_val_norm_norm, X_val_anom_norm, X_test_norm_norm, X_test_anom_norm, stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. üß† Definici√≥n del Modelo LSTM Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Clase LSTM Autoencoder\n",
        "# ========================================\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder basado en LSTM para detecci√≥n de anomal√≠as en series temporales.\n",
        "    \n",
        "    Arquitectura:\n",
        "    - Encoder: LSTM que comprime la secuencia a un espacio latente\n",
        "    - Decoder: LSTM que reconstruye la secuencia desde el espacio latente\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_sizes_encoder: List[int],\n",
        "        hidden_sizes_decoder: List[int],\n",
        "        latent_dim: int,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Validar que num_layers coincida con las longitudes de hidden_sizes\n",
        "        assert len(hidden_sizes_encoder) == num_layers, \\\n",
        "            f\"len(hidden_sizes_encoder)={len(hidden_sizes_encoder)} debe ser igual a num_layers={num_layers}\"\n",
        "        assert len(hidden_sizes_decoder) == num_layers, \\\n",
        "            f\"len(hidden_sizes_decoder)={len(hidden_sizes_decoder)} debe ser igual a num_layers={num_layers}\"\n",
        "        \n",
        "        # ===== ENCODER =====\n",
        "        encoder_layers = []\n",
        "        for i in range(num_layers):\n",
        "            in_size = input_size if i == 0 else hidden_sizes_encoder[i - 1]\n",
        "            out_size = hidden_sizes_encoder[i]\n",
        "            encoder_layers.append(\n",
        "                nn.LSTM(\n",
        "                    input_size=in_size,\n",
        "                    hidden_size=out_size,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    dropout=0.0 if i == num_layers - 1 else dropout,\n",
        "                )\n",
        "            )\n",
        "        self.encoder = nn.ModuleList(encoder_layers)\n",
        "        \n",
        "        # Capa lineal para mapear al espacio latente\n",
        "        self.latent_projection = nn.Linear(hidden_sizes_encoder[-1], latent_dim)\n",
        "        \n",
        "        # ===== DECODER =====\n",
        "        # Capa para expandir desde el espacio latente\n",
        "        self.latent_expansion = nn.Linear(latent_dim, hidden_sizes_decoder[0])\n",
        "        \n",
        "        decoder_layers = []\n",
        "        for i in range(num_layers):\n",
        "            in_size = hidden_sizes_decoder[i]\n",
        "            out_size = hidden_sizes_decoder[i + 1] if i < num_layers - 1 else input_size\n",
        "            decoder_layers.append(\n",
        "                nn.LSTM(\n",
        "                    input_size=in_size,\n",
        "                    hidden_size=out_size,\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    dropout=0.0 if i == num_layers - 1 else dropout,\n",
        "                )\n",
        "            )\n",
        "        self.decoder = nn.ModuleList(decoder_layers)\n",
        "        \n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Codifica la secuencia de entrada al espacio latente.\n",
        "        \n",
        "        Args:\n",
        "            x: Tensor de forma (batch_size, seq_len, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor latente de forma (batch_size, latent_dim)\n",
        "        \"\"\"\n",
        "        h = x\n",
        "        for lstm_layer in self.encoder:\n",
        "            h, (hidden, cell) = lstm_layer(h)\n",
        "            # Usar el √∫ltimo hidden state\n",
        "            h = hidden[-1]  # (batch_size, hidden_size)\n",
        "            h = h.unsqueeze(1)  # (batch_size, 1, hidden_size) para siguiente capa\n",
        "        \n",
        "        # Tomar el √∫ltimo hidden state de la √∫ltima capa\n",
        "        last_hidden = h.squeeze(1)  # (batch_size, hidden_size)\n",
        "        \n",
        "        # Proyectar al espacio latente\n",
        "        latent = self.latent_projection(last_hidden)  # (batch_size, latent_dim)\n",
        "        return latent\n",
        "    \n",
        "    def decode(self, latent: torch.Tensor, seq_len: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Decodifica desde el espacio latente a la secuencia reconstruida.\n",
        "        \n",
        "        Args:\n",
        "            latent: Tensor latente de forma (batch_size, latent_dim)\n",
        "            seq_len: Longitud de la secuencia a reconstruir\n",
        "        \n",
        "        Returns:\n",
        "            Tensor reconstruido de forma (batch_size, seq_len, input_size)\n",
        "        \"\"\"\n",
        "        # Expandir el espacio latente\n",
        "        h = self.latent_expansion(latent)  # (batch_size, hidden_size_decoder[0])\n",
        "        h = h.unsqueeze(1)  # (batch_size, 1, hidden_size_decoder[0])\n",
        "        \n",
        "        # Repetir para toda la secuencia\n",
        "        h = h.repeat(1, seq_len, 1)  # (batch_size, seq_len, hidden_size_decoder[0])\n",
        "        \n",
        "        # Decodificar capa por capa\n",
        "        for lstm_layer in self.decoder:\n",
        "            h, (hidden, cell) = lstm_layer(h)\n",
        "        \n",
        "        return h  # (batch_size, seq_len, input_size)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass completo: encode -> decode.\n",
        "        \n",
        "        Args:\n",
        "            x: Tensor de forma (batch_size, seq_len, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor reconstruido de forma (batch_size, seq_len, input_size)\n",
        "        \"\"\"\n",
        "        latent = self.encode(x)\n",
        "        seq_len = x.size(1)\n",
        "        reconstructed = self.decode(latent, seq_len)\n",
        "        return reconstructed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n para calcular error de reconstrucci√≥n\n",
        "# ========================================\n",
        "def compute_reconstruction_error(\n",
        "    model: LSTMAutoencoder,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calcula el error de reconstrucci√≥n (MSE) para cada muestra en el dataloader.\n",
        "    \n",
        "    Returns:\n",
        "        Array numpy con un error por muestra.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # El dataloader devuelve una tupla con un tensor\n",
        "            x = batch[0].to(device)  # (batch_size, seq_len, input_size)\n",
        "            \n",
        "            # Reconstruir\n",
        "            x_recon = model(x)\n",
        "            \n",
        "            # Calcular MSE por muestra (promedio sobre timesteps y features)\n",
        "            mse = torch.mean((x_recon - x) ** 2, dim=(1, 2))  # (batch_size,)\n",
        "            errors.append(mse.cpu().numpy())\n",
        "    \n",
        "    return np.concatenate(errors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Instanciar modelo\n",
        "# ========================================\n",
        "def create_model(config: Dict) -> LSTMAutoencoder:\n",
        "    \"\"\"Crea e instancia el modelo LSTM Autoencoder.\"\"\"\n",
        "    model = LSTMAutoencoder(\n",
        "        input_size=config[\"INPUT_SIZE\"],\n",
        "        hidden_sizes_encoder=config[\"HIDDEN_SIZES_ENCODER\"],\n",
        "        hidden_sizes_decoder=config[\"HIDDEN_SIZES_DECODER\"],\n",
        "        latent_dim=config[\"LATENT_DIM\"],\n",
        "        num_layers=config[\"NUM_LAYERS\"],\n",
        "        dropout=config[\"DROPOUT\"],\n",
        "    )\n",
        "    \n",
        "    # Contar par√°metros\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"‚úì Modelo creado:\")\n",
        "    print(f\"  Par√°metros totales: {total_params:,} ({total_params / 1e6:.2f}M)\")\n",
        "    print(f\"  Par√°metros entrenables: {trainable_params:,}\")\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. üèãÔ∏è Funciones de Entrenamiento y Evaluaci√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Diagn√≥stico de errores de reconstrucci√≥n y umbral\n",
        "# ========================================\n",
        "def diagnosticar_errores_umbral(\n",
        "    errors_norm: np.ndarray,\n",
        "    errors_anom: np.ndarray,\n",
        "    threshold: float,\n",
        "    nombre: str = \"DIAGN√ìSTICO\"\n",
        "):\n",
        "    \"\"\"Muestra estad√≠sticas de errores de reconstrucci√≥n y umbral.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"üîç {nombre}: ERRORES DE RECONSTRUCCI√ìN Y UMBRAL\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if len(errors_norm) > 0:\n",
        "        print(f\"\\nüìä Errores NORMALES ({len(errors_norm)} muestras):\")\n",
        "        print(f\"  - Min: {errors_norm.min():.8f}\")\n",
        "        print(f\"  - Max: {errors_norm.max():.8f}\")\n",
        "        print(f\"  - Mean: {errors_norm.mean():.8f}\")\n",
        "        print(f\"  - Median: {np.median(errors_norm):.8f}\")\n",
        "        print(f\"  - Std: {errors_norm.std():.8f}\")\n",
        "        print(f\"  - Percentil 95: {np.percentile(errors_norm, 95):.8f}\")\n",
        "        print(f\"  - Percentil 99: {np.percentile(errors_norm, 99):.8f}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è No hay errores normales disponibles\")\n",
        "    \n",
        "    if len(errors_anom) > 0:\n",
        "        print(f\"\\nüìä Errores AN√ìMALOS ({len(errors_anom)} muestras):\")\n",
        "        print(f\"  - Min: {errors_anom.min():.8f}\")\n",
        "        print(f\"  - Max: {errors_anom.max():.8f}\")\n",
        "        print(f\"  - Mean: {errors_anom.mean():.8f}\")\n",
        "        print(f\"  - Median: {np.median(errors_anom):.8f}\")\n",
        "        print(f\"  - Std: {errors_anom.std():.8f}\")\n",
        "        print(f\"  - Percentil 95: {np.percentile(errors_anom, 95):.8f}\")\n",
        "        print(f\"  - Percentil 99: {np.percentile(errors_anom, 99):.8f}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è No hay errores an√≥malos disponibles\")\n",
        "    \n",
        "    print(f\"\\nüéØ UMBRAL DE DETECCI√ìN: {threshold:.8f}\")\n",
        "    \n",
        "    if len(errors_norm) > 0 and len(errors_anom) > 0:\n",
        "        # Predicciones esperadas\n",
        "        normales_detectados = (errors_norm > threshold).sum()\n",
        "        anomalos_detectados = (errors_anom > threshold).sum()\n",
        "        \n",
        "        print(f\"\\nüìà CLASIFICACI√ìN CON ESTE UMBRAL:\")\n",
        "        print(f\"  - Normales > umbral (predicho como an√≥malos): {normales_detectados}/{len(errors_norm)} ({100*normales_detectados/len(errors_norm):.2f}%)\")\n",
        "        print(f\"  - An√≥malos > umbral (predicho como an√≥malos): {anomalos_detectados}/{len(errors_anom)} ({100*anomalos_detectados/len(errors_anom):.2f}%)\")\n",
        "        \n",
        "        # Separabilidad\n",
        "        mean_norm = errors_norm.mean()\n",
        "        mean_anom = errors_anom.mean()\n",
        "        separabilidad = abs(mean_anom - mean_norm) / (errors_norm.std() + errors_anom.std() + 1e-8)\n",
        "        \n",
        "        print(f\"\\nüìä SEPARABILIDAD:\")\n",
        "        print(f\"  - Diferencia de medias: {abs(mean_anom - mean_norm):.8f}\")\n",
        "        print(f\"  - Separabilidad (Cohen's d aproximado): {separabilidad:.4f}\")\n",
        "        \n",
        "        if separabilidad < 0.5:\n",
        "            print(f\"  ‚ö†Ô∏è ADVERTENCIA: Separabilidad muy baja (< 0.5). El modelo no est√° diferenciando bien normales vs an√≥malos.\")\n",
        "        if mean_anom <= mean_norm:\n",
        "            print(f\"  ‚ùå ERROR: Los an√≥malos tienen errores MENORES que los normales! El modelo est√° aprendiendo al rev√©s.\")\n",
        "        \n",
        "        # Sugerencias de umbral\n",
        "        if threshold < errors_norm.max() and threshold < errors_anom.max():\n",
        "            percentil_optimo = None\n",
        "            for p in [90, 95, 98, 99]:\n",
        "                umbral_candidato = np.percentile(errors_norm, p)\n",
        "                tp = (errors_anom > umbral_candidato).sum()\n",
        "                fp = (errors_norm > umbral_candidato).sum()\n",
        "                fn = (errors_anom <= umbral_candidato).sum()\n",
        "                tn = (errors_norm <= umbral_candidato).sum()\n",
        "                \n",
        "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "                \n",
        "                if f1 > 0.3:  # Solo sugerir si tiene algo de F1\n",
        "                    percentil_optimo = p\n",
        "                    print(f\"\\nüí° SUGERENCIA: Percentil {p} da umbral {umbral_candidato:.8f} -> F1={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")\n",
        "                    break\n",
        "    \n",
        "    print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n de entrenamiento por √©poca\n",
        "# ========================================\n",
        "def train_one_epoch(\n",
        "    model: LSTMAutoencoder,\n",
        "    train_loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    config: Dict,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Entrena el modelo por una √©poca.\n",
        "    \n",
        "    Returns:\n",
        "        Tupla con (loss_promedio, reconstruction_error_promedio)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_recon_error = 0.0\n",
        "    n_samples = 0\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        x = batch[0].to(device)  # (batch_size, seq_len, input_size)\n",
        "        \n",
        "        # Verificar que los datos sean finitos\n",
        "        if not torch.isfinite(x).all():\n",
        "            print(\"‚ö†Ô∏è ADVERTENCIA: Datos de entrada contienen valores no finitos (NaN/Inf)\")\n",
        "            continue\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        x_recon = model(x)\n",
        "        \n",
        "        # Verificar que la reconstrucci√≥n sea finita\n",
        "        if not torch.isfinite(x_recon).all():\n",
        "            print(\"‚ö†Ô∏è ADVERTENCIA: Reconstrucci√≥n contiene valores no finitos (NaN/Inf)\")\n",
        "            continue\n",
        "        \n",
        "        # Calcular p√©rdida (MSE)\n",
        "        loss = criterion(x_recon, x)\n",
        "        \n",
        "        # Verificar que la p√©rdida sea finita\n",
        "        if not torch.isfinite(loss):\n",
        "            print(f\"‚ö†Ô∏è ADVERTENCIA: P√©rdida no finita: {loss.item()}\")\n",
        "            continue\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping (opcional)\n",
        "        if config.get(\"CLIP_GRAD_NORM\") is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"CLIP_GRAD_NORM\"])\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        # Acumular m√©tricas\n",
        "        batch_size = x.size(0)\n",
        "        loss_value = loss.item()\n",
        "        total_loss += loss_value * batch_size\n",
        "        \n",
        "        # Error de reconstrucci√≥n promedio por muestra (MSE por muestra, luego promedio)\n",
        "        # Calcular MSE por muestra: (batch_size,)\n",
        "        mse_per_sample = torch.mean((x_recon - x) ** 2, dim=(1, 2))  # (batch_size,)\n",
        "        # Promedio sobre todas las muestras del batch\n",
        "        recon_error = mse_per_sample.mean().item()\n",
        "        \n",
        "        # Verificar que el error de reconstrucci√≥n sea finito\n",
        "        if not np.isfinite(recon_error):\n",
        "            print(f\"‚ö†Ô∏è ADVERTENCIA: Error de reconstrucci√≥n no finito: {recon_error}\")\n",
        "            recon_error = loss_value  # Usar la p√©rdida como fallback\n",
        "        \n",
        "        total_recon_error += recon_error * batch_size\n",
        "        n_samples += batch_size\n",
        "    \n",
        "    avg_loss = total_loss / n_samples if n_samples > 0 else 0.0\n",
        "    avg_recon_error = total_recon_error / n_samples if n_samples > 0 else 0.0\n",
        "    \n",
        "    # Verificar que los promedios sean finitos\n",
        "    if not np.isfinite(avg_loss):\n",
        "        print(f\"‚ö†Ô∏è ADVERTENCIA: P√©rdida promedio no finita: {avg_loss}\")\n",
        "        avg_loss = 0.0\n",
        "    \n",
        "    if not np.isfinite(avg_recon_error):\n",
        "        print(f\"‚ö†Ô∏è ADVERTENCIA: Error de reconstrucci√≥n promedio no finito: {avg_recon_error}\")\n",
        "        avg_recon_error = avg_loss  # Usar la p√©rdida como fallback\n",
        "    \n",
        "    return avg_loss, avg_recon_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n de validaci√≥n (con m√©tricas para ambas clases)\n",
        "# ========================================\n",
        "def validate(\n",
        "    model: LSTMAutoencoder,\n",
        "    val_norm_loader: DataLoader,\n",
        "    val_anom_loader: DataLoader,\n",
        "    y_val: np.ndarray,\n",
        "    device: torch.device,\n",
        "    threshold: float,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Eval√∫a el modelo en el conjunto de validaci√≥n.\n",
        "    Calcula precision, recall y f1 para ambas clases (normal y an√≥malo).\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con m√©tricas completas y matriz de confusi√≥n\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Calcular errores de reconstrucci√≥n\n",
        "    errors_norm = compute_reconstruction_error(model, val_norm_loader, device)\n",
        "    errors_anom = compute_reconstruction_error(model, val_anom_loader, device) if val_anom_loader else np.array([])\n",
        "    \n",
        "    # Combinar errores y etiquetas\n",
        "    all_errors = np.concatenate([errors_norm, errors_anom]) if len(errors_anom) > 0 else errors_norm\n",
        "    \n",
        "    # Predicciones basadas en umbral\n",
        "    y_pred = (all_errors > threshold).astype(int)\n",
        "    \n",
        "    # Calcular m√©tricas generales\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    \n",
        "    # Calcular m√©tricas por clase usando classification_report\n",
        "    report = classification_report(y_val, y_pred, target_names=[\"normal\", \"anomalo\"], output_dict=True, zero_division=0)\n",
        "    \n",
        "    # M√©tricas para clase normal (0)\n",
        "    metrics_normal = report.get(\"normal\", {})\n",
        "    precision_normal = metrics_normal.get(\"precision\", 0.0)\n",
        "    recall_normal = metrics_normal.get(\"recall\", 0.0)\n",
        "    f1_normal = metrics_normal.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # M√©tricas para clase an√≥mala (1)\n",
        "    metrics_anom = report.get(\"anomalo\", {})\n",
        "    precision_anom = metrics_anom.get(\"precision\", 0.0)\n",
        "    recall_anom = metrics_anom.get(\"recall\", 0.0)\n",
        "    f1_anom = metrics_anom.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # M√©tricas generales (macro avg)\n",
        "    macro_avg = report.get(\"macro avg\", {})\n",
        "    precision_macro = macro_avg.get(\"precision\", 0.0)\n",
        "    recall_macro = macro_avg.get(\"recall\", 0.0)\n",
        "    f1_macro = macro_avg.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_val, y_pred, labels=[0, 1])\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / max(1, tn + fp)  # TNR\n",
        "    \n",
        "    return {\n",
        "        # M√©tricas generales\n",
        "        \"accuracy\": accuracy,\n",
        "        \"specificity\": specificity,\n",
        "        # M√©tricas para clase normal\n",
        "        \"precision_normal\": precision_normal,\n",
        "        \"recall_normal\": recall_normal,\n",
        "        \"f1_normal\": f1_normal,\n",
        "        # M√©tricas para clase an√≥mala\n",
        "        \"precision_anom\": precision_anom,\n",
        "        \"recall_anom\": recall_anom,\n",
        "        \"f1_anom\": f1_anom,\n",
        "        # M√©tricas macro promedio\n",
        "        \"precision_macro\": precision_macro,\n",
        "        \"recall_macro\": recall_macro,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        # Matriz de confusi√≥n\n",
        "        \"confusion_matrix\": cm,\n",
        "        # Errores (para an√°lisis)\n",
        "        \"errors_norm\": errors_norm,\n",
        "        \"errors_anom\": errors_anom,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n de evaluaci√≥n en test (con m√©tricas para ambas clases)\n",
        "# ========================================\n",
        "def evaluate_on_test(\n",
        "    model: LSTMAutoencoder,\n",
        "    test_norm_loader: DataLoader,\n",
        "    test_anom_loader: DataLoader,\n",
        "    y_test: np.ndarray,\n",
        "    device: torch.device,\n",
        "    threshold: float,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Eval√∫a el modelo en el conjunto de test.\n",
        "    Calcula precision, recall y f1 para ambas clases (normal y an√≥malo).\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con m√©tricas completas y matriz de confusi√≥n\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Calcular errores de reconstrucci√≥n\n",
        "    errors_norm = compute_reconstruction_error(model, test_norm_loader, device)\n",
        "    errors_anom = compute_reconstruction_error(model, test_anom_loader, device) if test_anom_loader else np.array([])\n",
        "    \n",
        "    # Combinar errores y etiquetas\n",
        "    all_errors = np.concatenate([errors_norm, errors_anom]) if len(errors_anom) > 0 else errors_norm\n",
        "    \n",
        "    # Predicciones basadas en umbral\n",
        "    y_pred = (all_errors > threshold).astype(int)\n",
        "    \n",
        "    # Calcular m√©tricas generales\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    # Calcular m√©tricas por clase usando classification_report\n",
        "    report = classification_report(y_test, y_pred, target_names=[\"normal\", \"anomalo\"], output_dict=True, zero_division=0)\n",
        "    \n",
        "    # M√©tricas para clase normal (0)\n",
        "    metrics_normal = report.get(\"normal\", {})\n",
        "    precision_normal = metrics_normal.get(\"precision\", 0.0)\n",
        "    recall_normal = metrics_normal.get(\"recall\", 0.0)\n",
        "    f1_normal = metrics_normal.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # M√©tricas para clase an√≥mala (1)\n",
        "    metrics_anom = report.get(\"anomalo\", {})\n",
        "    precision_anom = metrics_anom.get(\"precision\", 0.0)\n",
        "    recall_anom = metrics_anom.get(\"recall\", 0.0)\n",
        "    f1_anom = metrics_anom.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # M√©tricas generales (macro avg)\n",
        "    macro_avg = report.get(\"macro avg\", {})\n",
        "    precision_macro = macro_avg.get(\"precision\", 0.0)\n",
        "    recall_macro = macro_avg.get(\"recall\", 0.0)\n",
        "    f1_macro = macro_avg.get(\"f1-score\", 0.0)\n",
        "    \n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / max(1, tn + fp)  # TNR\n",
        "    \n",
        "    return {\n",
        "        # M√©tricas generales\n",
        "        \"accuracy\": accuracy,\n",
        "        \"specificity\": specificity,\n",
        "        # M√©tricas para clase normal\n",
        "        \"precision_normal\": precision_normal,\n",
        "        \"recall_normal\": recall_normal,\n",
        "        \"f1_normal\": f1_normal,\n",
        "        # M√©tricas para clase an√≥mala\n",
        "        \"precision_anom\": precision_anom,\n",
        "        \"recall_anom\": recall_anom,\n",
        "        \"f1_anom\": f1_anom,\n",
        "        # M√©tricas macro promedio\n",
        "        \"precision_macro\": precision_macro,\n",
        "        \"recall_macro\": recall_macro,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        # Matriz de confusi√≥n\n",
        "        \"confusion_matrix\": cm,\n",
        "        # Errores (para an√°lisis)\n",
        "        \"errors_norm\": errors_norm,\n",
        "        \"errors_anom\": errors_anom,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. üìä Integraci√≥n con MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Configuraci√≥n de MLflow\n",
        "# ========================================\n",
        "def setup_mlflow(config: Dict) -> str:\n",
        "    \"\"\"\n",
        "    Configura MLflow y crea/obtiene el experimento.\n",
        "    \n",
        "    Returns:\n",
        "        ID del experimento\n",
        "    \"\"\"\n",
        "    # Configurar tracking URI (similar al notebook de referencia)\n",
        "    if config.get(\"MLFLOW_TRACKING_URI\") is not None:\n",
        "        mlflow.set_tracking_uri(config[\"MLFLOW_TRACKING_URI\"])\n",
        "    else:\n",
        "        # Usar sqlite en el directorio padre (como en el notebook de referencia)\n",
        "        PARENT_DIR = Path.cwd().parent.resolve()\n",
        "        TRACKING_DB = (PARENT_DIR / \"mlflow.db\").resolve()\n",
        "        mlflow.set_tracking_uri(f\"sqlite:///{TRACKING_DB.as_posix()}\")\n",
        "        print(f\"‚úì MLflow tracking URI: sqlite:///{TRACKING_DB.as_posix()}\")\n",
        "    \n",
        "    # Crear o obtener experimento\n",
        "    experiment_name = config[\"MLFLOW_EXPERIMENT_NAME\"]\n",
        "    \n",
        "    try:\n",
        "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "        if experiment is None:\n",
        "            # Crear directorio de artefactos\n",
        "            PARENT_DIR = Path.cwd().parent.resolve()\n",
        "            ARTIFACT_ROOT = (PARENT_DIR / \"mlflow_artifacts\").resolve()\n",
        "            ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "            experiment_id = mlflow.create_experiment(experiment_name, artifact_location=ARTIFACT_ROOT.as_uri())\n",
        "            print(f\"‚úì Experimento MLflow creado: {experiment_name} (ID: {experiment_id})\")\n",
        "            print(f\"  Artifact root: {ARTIFACT_ROOT.as_uri()}\")\n",
        "        else:\n",
        "            experiment_id = experiment.experiment_id\n",
        "            print(f\"‚úì Experimento MLflow existente: {experiment_name} (ID: {experiment_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error al configurar MLflow: {e}\")\n",
        "        experiment_id = mlflow.set_experiment(experiment_name)\n",
        "    \n",
        "    return experiment_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n para guardar matriz de confusi√≥n como artefacto\n",
        "# ========================================\n",
        "def save_confusion_matrix(\n",
        "    cm: np.ndarray,\n",
        "    output_dir: Path,\n",
        "    tag: str,\n",
        ") -> Tuple[Path, Path]:\n",
        "    \"\"\"\n",
        "    Guarda la matriz de confusi√≥n como PNG y CSV.\n",
        "    \n",
        "    Returns:\n",
        "        Tupla con rutas (png_path, csv_path)\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Guardar como CSV\n",
        "    csv_path = output_dir / f\"confusion_matrix_{tag}.csv\"\n",
        "    df_cm = pd.DataFrame(cm, index=[\"Normal\", \"An√≥malo\"], columns=[\"Normal\", \"An√≥malo\"])\n",
        "    df_cm.to_csv(csv_path)\n",
        "    \n",
        "    # Guardar como PNG\n",
        "    png_path = output_dir / f\"confusion_matrix_{tag}.png\"\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    \n",
        "    # Etiquetas\n",
        "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]))\n",
        "    ax.set_xticklabels([\"Normal\", \"An√≥malo\"])\n",
        "    ax.set_yticklabels([\"Normal\", \"An√≥malo\"])\n",
        "    ax.set_xlabel(\"Predicci√≥n\")\n",
        "    ax.set_ylabel(\"Real\")\n",
        "    ax.set_title(f\"Matriz de Confusi√≥n - {tag.upper()}\")\n",
        "    \n",
        "    # A√±adir valores en las celdas\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(\n",
        "                j, i, f\"{cm[i, j]}\",\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
        "            )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(png_path, dpi=150)\n",
        "    plt.close()\n",
        "    \n",
        "    return png_path, csv_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Funci√≥n para guardar gr√°ficos de curvas de entrenamiento\n",
        "# ========================================\n",
        "def save_training_curves(\n",
        "    train_losses: List[float],\n",
        "    train_recon_errors: List[float],\n",
        "    val_f1_scores: List[float],\n",
        "    output_dir: Path,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Guarda gr√°ficos de curvas de entrenamiento.\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo PNG guardado\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    \n",
        "    # Loss\n",
        "    axes[0].plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\")\n",
        "    axes[0].set_xlabel(\"√âpoca\")\n",
        "    axes[0].set_ylabel(\"Loss (MSE)\")\n",
        "    axes[0].set_title(\"Loss de Entrenamiento\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Reconstruction Error\n",
        "    axes[1].plot(epochs, train_recon_errors, label=\"Train Recon Error\", color=\"green\")\n",
        "    axes[1].set_xlabel(\"√âpoca\")\n",
        "    axes[1].set_ylabel(\"Error de Reconstrucci√≥n\")\n",
        "    axes[1].set_title(\"Error de Reconstrucci√≥n\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend()\n",
        "    \n",
        "    # F1 Score en validaci√≥n\n",
        "    if val_f1_scores:\n",
        "        axes[2].plot(epochs, val_f1_scores, label=\"Val F1\", color=\"red\")\n",
        "        axes[2].set_xlabel(\"√âpoca\")\n",
        "        axes[2].set_ylabel(\"F1-Score\")\n",
        "        axes[2].set_title(\"F1-Score en Validaci√≥n\")\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        axes[2].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    png_path = output_dir / \"training_curves.png\"\n",
        "    plt.savefig(png_path, dpi=150)\n",
        "    plt.close()\n",
        "    \n",
        "    return png_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. ü™Ñ Orquestaci√≥n con Prefect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "\n",
        "# Flujo principal de Prefect\n",
        "# ========================================\n",
        "@flow(name=\"lstm_autoencoder_training_flow\", log_prints=True)\n",
        "def lstm_autoencoder_training_flow(config: Dict = None):\n",
        "    \"\"\"\n",
        "    Flujo principal de Prefect que orquesta todo el proceso:\n",
        "    1. Carga y preparaci√≥n de datos\n",
        "    2. Creaci√≥n del modelo\n",
        "    3. Entrenamiento\n",
        "    4. Evaluaci√≥n en test\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = CONFIG\n",
        "    \n",
        "    print(\"üöÄ Iniciando flujo de entrenamiento LSTM Autoencoder...\")\n",
        "    print(f\"Experimento MLflow: {config['MLFLOW_EXPERIMENT_NAME']}\")\n",
        "    \n",
        "    # Configurar MLflow\n",
        "    experiment_id = setup_mlflow(config)\n",
        "    \n",
        "    # Cargar y preparar datos\n",
        "    dataloaders = task_load_data(config)\n",
        "    train_loader, val_norm_loader, val_anom_loader, y_val, test_norm_loader, test_anom_loader, y_test = dataloaders\n",
        "    \n",
        "    # Crear modelo\n",
        "    print(\"üß† Creando modelo...\")\n",
        "    model = create_model(config)\n",
        "    \n",
        "    # Entrenar\n",
        "    model, threshold, train_losses, train_recon_errors, val_f1_scores, best_f1 = task_train_model(\n",
        "        model, train_loader, val_norm_loader, val_anom_loader, y_val, config, DEVICE, experiment_id\n",
        "    )\n",
        "    \n",
        "    # Evaluar en test\n",
        "    test_metrics = task_evaluate_test(\n",
        "        model, test_norm_loader, test_anom_loader, y_test, DEVICE, threshold, config, experiment_id\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ FLUJO COMPLETADO\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Mejor F1 en validaci√≥n: {best_f1:.4f}\")\n",
        "    print(f\"F1 en test: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"Umbral usado: {threshold:.6f}\")\n",
        "    print(f\"\\nRevisa MLflow para ver todos los artefactos y m√©tricas.\")\n",
        "    \n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"threshold\": threshold,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"best_f1\": best_f1,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 22:36:49 INFO  [prefect.flow_runs] Beginning flow run 'imaginary-macaque' for flow 'lstm_autoencoder_training_flow'\n",
            "2025-11-21 22:36:49 INFO  [prefect.flow_runs] üöÄ Iniciando flujo de entrenamiento LSTM Autoencoder...\n",
            "2025-11-21 22:36:49 INFO  [prefect.flow_runs] Experimento MLflow: LSTM_AE_Anomalias_ECG_v1\n",
            "2025-11-21 22:36:49 INFO  [prefect.flow_runs] ‚úì MLflow tracking URI: sqlite:///S:/Proyecto final/mlflow.db\n",
            "2025-11-21 22:36:49 INFO  [prefect.flow_runs] ‚úì Experimento MLflow existente: LSTM_AE_Anomalias_ECG_v1 (ID: 3)\n",
            "2025-11-21 22:36:49 INFO  [prefect.task_runs] üìÇ Cargando datos...\n",
            "2025-11-21 22:36:49 INFO  [prefect.task_runs] ‚ö†Ô∏è ADVERTENCIA: Usando datos dummy. Reemplaza esta funci√≥n con tu l√≥gica de carga.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 22:36:49 INFO  [prefect.task_runs] ======================================================================\n",
            "2025-11-21 22:36:49 INFO  [prefect.task_runs] üìä PREPARACI√ìN DE DATOS\n",
            "2025-11-21 22:36:49 INFO  [prefect.task_runs] ======================================================================\n",
            "2025-11-21 22:36:49 INFO  [prefect.task_runs] ‚úì Los datos de 'Datos_no_supervisados' ya vienen procesados y listos.\n",
            "2025-11-21 22:36:49 INFO  [prefect.task_runs]   ‚Üí Pasando datos directamente SIN modificaciones (sin limpieza ni normalizaci√≥n)\n",
            "2025-11-21 22:36:49 INFO  [prefect.task_runs] ======================================================================\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] \n",
            "üìà Estad√≠sticas de referencia (solo para logging):\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs]   Mean: [ 1.6704485e-04 -1.6632090e-04 -8.0895676e-05]\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs]   Std: [0.99752015 0.996813   0.9971317 ]\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] \n",
            "‚úì Datos listos para usar (sin modificaciones)\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] ======================================================================\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] \n",
            "‚úì DataLoaders creados:\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs]   Train normales: 16 batches (1000 muestras)\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs]   Val normales: 4 batches (200 muestras)\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs]   Val an√≥malos: 1 batches (50 muestras)\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs]   Test normales: 4 batches (200 muestras)\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs]   Test an√≥malos: 1 batches (50 muestras)\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] ‚úì Datos cargados y preparados\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] Finished in state Completed()\n",
            "2025-11-21 22:36:50 INFO  [prefect.flow_runs] üß† Creando modelo...\n",
            "c:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:990: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  super().__init__(\"LSTM\", *args, **kwargs)\n",
            "2025-11-21 22:36:50 INFO  [prefect.flow_runs] ‚úì Modelo creado:\n",
            "2025-11-21 22:36:50 INFO  [prefect.flow_runs]   Par√°metros totales: 57,196 (0.06M)\n",
            "2025-11-21 22:36:50 INFO  [prefect.flow_runs]   Par√°metros entrenables: 57,196\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] üèãÔ∏è Iniciando entrenamiento...\n",
            "2025-11-21 22:36:50 INFO  [prefect.task_runs] Epoch 001/30 | Train Loss: 1.011399 | Train Recon Error: 1.011399\n",
            "2025-11-21 22:36:51 INFO  [prefect.task_runs] Epoch 002/30 | Train Loss: 1.000546 | Train Recon Error: 1.000546\n",
            "2025-11-21 22:36:51 INFO  [prefect.task_runs] Epoch 003/30 | Train Loss: 1.000187 | Train Recon Error: 1.000187\n",
            "2025-11-21 22:36:52 INFO  [prefect.task_runs] Epoch 004/30 | Train Loss: 1.000103 | Train Recon Error: 1.000103\n",
            "2025-11-21 22:36:52 INFO  [prefect.task_runs] Epoch 005/30 | Train Loss: 1.000085 | Train Recon Error: 1.000085 | Val F1 (anom): 0.9174 | Val F1 (normal): 0.9770\n",
            "2025-11-21 22:36:53 INFO  [prefect.task_runs] Epoch 006/30 | Train Loss: 1.000080 | Train Recon Error: 1.000080\n",
            "2025-11-21 22:36:54 INFO  [prefect.task_runs] Epoch 007/30 | Train Loss: 1.000081 | Train Recon Error: 1.000081\n",
            "2025-11-21 22:36:54 INFO  [prefect.task_runs] Epoch 008/30 | Train Loss: 1.000080 | Train Recon Error: 1.000080\n",
            "2025-11-21 22:36:55 INFO  [prefect.task_runs] Epoch 009/30 | Train Loss: 1.000079 | Train Recon Error: 1.000079\n",
            "2025-11-21 22:36:56 INFO  [prefect.task_runs] Epoch 010/30 | Train Loss: 1.000079 | Train Recon Error: 1.000079 | Val F1 (anom): 0.9174 | Val F1 (normal): 0.9770\n",
            "2025-11-21 22:36:57 INFO  [prefect.task_runs] Epoch 011/30 | Train Loss: 1.000083 | Train Recon Error: 1.000083\n",
            "2025-11-21 22:36:57 INFO  [prefect.task_runs] Epoch 012/30 | Train Loss: 1.000082 | Train Recon Error: 1.000082\n",
            "2025-11-21 22:36:58 INFO  [prefect.task_runs] Epoch 013/30 | Train Loss: 1.000082 | Train Recon Error: 1.000082\n",
            "2025-11-21 22:36:58 INFO  [prefect.task_runs] Epoch 014/30 | Train Loss: 1.000083 | Train Recon Error: 1.000083\n",
            "2025-11-21 22:36:59 INFO  [prefect.task_runs] Epoch 015/30 | Train Loss: 1.000082 | Train Recon Error: 1.000082 | Val F1 (anom): 0.9174 | Val F1 (normal): 0.9770\n",
            "2025-11-21 22:37:00 INFO  [prefect.task_runs] Epoch 016/30 | Train Loss: 1.000081 | Train Recon Error: 1.000081\n",
            "2025-11-21 22:37:00 INFO  [prefect.task_runs] Epoch 017/30 | Train Loss: 1.000080 | Train Recon Error: 1.000080\n",
            "2025-11-21 22:37:01 INFO  [prefect.task_runs] Epoch 018/30 | Train Loss: 1.000080 | Train Recon Error: 1.000080\n",
            "2025-11-21 22:37:01 INFO  [prefect.task_runs] Epoch 019/30 | Train Loss: 1.000084 | Train Recon Error: 1.000084\n",
            "2025-11-21 22:37:02 INFO  [prefect.task_runs] Epoch 020/30 | Train Loss: 1.000082 | Train Recon Error: 1.000082 | Val F1 (anom): 0.9174 | Val F1 (normal): 0.9770\n",
            "2025-11-21 22:37:03 INFO  [prefect.task_runs] Epoch 021/30 | Train Loss: 1.000085 | Train Recon Error: 1.000085\n",
            "2025-11-21 22:37:03 INFO  [prefect.task_runs] Epoch 022/30 | Train Loss: 1.000088 | Train Recon Error: 1.000088\n",
            "2025-11-21 22:37:04 INFO  [prefect.task_runs] Epoch 023/30 | Train Loss: 1.000083 | Train Recon Error: 1.000083\n",
            "2025-11-21 22:37:04 INFO  [prefect.task_runs] Epoch 024/30 | Train Loss: 1.000083 | Train Recon Error: 1.000083\n",
            "2025-11-21 22:37:05 INFO  [prefect.task_runs] Epoch 025/30 | Train Loss: 1.000080 | Train Recon Error: 1.000080 | Val F1 (anom): 0.9174 | Val F1 (normal): 0.9770\n",
            "2025-11-21 22:37:06 INFO  [prefect.task_runs] Epoch 026/30 | Train Loss: 1.000083 | Train Recon Error: 1.000083\n",
            "2025-11-21 22:37:06 INFO  [prefect.task_runs] Epoch 027/30 | Train Loss: 1.000084 | Train Recon Error: 1.000084\n",
            "2025-11-21 22:37:07 INFO  [prefect.task_runs] Epoch 028/30 | Train Loss: 1.000083 | Train Recon Error: 1.000083\n",
            "2025-11-21 22:37:07 INFO  [prefect.task_runs] Epoch 029/30 | Train Loss: 1.000083 | Train Recon Error: 1.000083\n",
            "2025-11-21 22:37:08 INFO  [prefect.task_runs] Epoch 030/30 | Train Loss: 1.000081 | Train Recon Error: 1.000081 | Val F1 (anom): 0.9174 | Val F1 (normal): 0.9770\n",
            "2025/11/21 22:37:09 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/11/21 22:37:13 WARNING mlflow.utils.requirements_utils: Found torch version (2.10.0.dev20251121+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.10.0.dev20251121' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/11/21 22:37:23 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\tomas\\AppData\\Local\\Temp\\tmparqcjpa0\\model\\data, flavor: pytorch). Fall back to return ['torch==2.10.0.dev20251121', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
            "2025/11/21 22:37:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs] ‚úì Entrenamiento completado. Mejor F1 (an√≥malos): 0.9174\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs] Finished in state Completed()\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs] üìä Evaluando en conjunto de test...\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs] ‚úì Evaluaci√≥n en test completada:\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs]   Accuracy: 0.9640\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs]   Precision (normal): 1.0000 | Recall: 0.9550 | F1: 0.9770\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs]   Precision (an√≥malo): 0.8475 | Recall: 1.0000 | F1: 0.9174\n",
            "2025-11-21 22:37:23 INFO  [prefect.task_runs] Finished in state Completed()\n",
            "2025-11-21 22:37:23 INFO  [prefect.flow_runs] \n",
            "============================================================\n",
            "2025-11-21 22:37:23 INFO  [prefect.flow_runs] ‚úÖ FLUJO COMPLETADO\n",
            "2025-11-21 22:37:23 INFO  [prefect.flow_runs] ============================================================\n",
            "2025-11-21 22:37:23 INFO  [prefect.flow_runs] Mejor F1 en validaci√≥n: 0.9174\n",
            "2025-11-21 22:37:23 ERROR [prefect.flow_runs] Encountered exception during execution: KeyError('f1')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py\", line 788, in run_context\n",
            "    yield self\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py\", line 1409, in run_flow_sync\n",
            "    engine.call_flow_fn()\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py\", line 808, in call_flow_fn\n",
            "    result = call_with_parameters(self.flow.fn, self.parameters)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_8152\\3116310422.py\", line 44, in lstm_autoencoder_training_flow\n",
            "    print(f\"F1 en test: {test_metrics['f1']:.4f}\")\n",
            "                         ~~~~~~~~~~~~^^^^^^\n",
            "KeyError: 'f1'\n",
            "2025-11-21 22:37:23 INFO  [prefect.flow_runs] Finished in state Failed(\"Flow run encountered an exception: KeyError: 'f1'\")\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'f1'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[227]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Ejecutar el flujo completo\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     results = \u001b[43mlstm_autoencoder_training_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Proceso finalizado exitosamente\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flows.py:1702\u001b[39m, in \u001b[36mFlow.__call__\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m track_viz_task(\u001b[38;5;28mself\u001b[39m.isasync, \u001b[38;5;28mself\u001b[39m.name, parameters)\n\u001b[32m   1700\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mprefect\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflow_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_flow\n\u001b[32m-> \u001b[39m\u001b[32m1702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_flow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:1566\u001b[39m, in \u001b[36mrun_flow\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, error_logger, context)\u001b[39m\n\u001b[32m   1564\u001b[39m         ret_val = run_flow_async(**kwargs)\n\u001b[32m   1565\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1566\u001b[39m         ret_val = \u001b[43mrun_flow_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (Abort, Pause):\n\u001b[32m   1568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:1411\u001b[39m, in \u001b[36mrun_flow_sync\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1408\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m   1409\u001b[39m             engine.call_flow_fn()\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:361\u001b[39m, in \u001b[36mFlowRunEngine.result\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# This is a fall through case which leans on the existing state result mechanics to get the\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# return value. This is necessary because we currently will return a State object if the\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# the State was Prefect-created.\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# TODO: Remove the need to get the result from a State except in cases where the return value\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# is a State object.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:788\u001b[39m, in \u001b[36mFlowRunEngine.run_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m timeout_context(\n\u001b[32m    782\u001b[39m         seconds=\u001b[38;5;28mself\u001b[39m.flow.timeout_seconds,\n\u001b[32m    783\u001b[39m         timeout_exc_type=FlowRunTimeoutError,\n\u001b[32m    784\u001b[39m     ):\n\u001b[32m    785\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.debug(\n\u001b[32m    786\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuting flow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for flow run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow_run.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    787\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_timeout(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:1409\u001b[39m, in \u001b[36mrun_flow_sync\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1407\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m engine.is_running():\n\u001b[32m   1408\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m-> \u001b[39m\u001b[32m1409\u001b[39m             \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_flow_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\flow_engine.py:808\u001b[39m, in \u001b[36mFlowRunEngine.call_flow_fn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_flow_fn()\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m     result = \u001b[43mcall_with_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_success(result)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\prefect\\utilities\\callables.py:210\u001b[39m, in \u001b[36mcall_with_parameters\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03mCall a function with parameters extracted with `get_call_parameters`\u001b[39;00m\n\u001b[32m    204\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \u001b[33;03mthe args/kwargs using `parameters_to_positional_and_keyword` directly\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[226]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mlstm_autoencoder_training_flow\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMejor F1 en validaci√≥n: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mF1 en test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtest_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUmbral usado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRevisa MLflow para ver todos los artefactos y m√©tricas.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: 'f1'"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Ejecutar el flujo completo\n",
        "# ========================================\n",
        "if __name__ == \"__main__\":\n",
        "    results = lstm_autoencoder_training_flow(CONFIG)\n",
        "    print(\"\\n‚úì Proceso finalizado exitosamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Checklist Final\n",
        "\n",
        "Antes de ejecutar el notebook completo:\n",
        "\n",
        "1. ‚úÖ **Completa la funci√≥n `load_raw_data()`** con tu l√≥gica de carga de datos\n",
        "2. ‚úÖ **Ajusta los hiperpar√°metros** en la secci√≥n de configuraci√≥n (CONFIG)\n",
        "3. ‚úÖ **Cambia el nombre del experimento MLflow** (`MLFLOW_EXPERIMENT_NAME`)\n",
        "4. ‚úÖ **Verifica que los datos tengan la forma correcta**: (N, T, INPUT_SIZE)\n",
        "5. ‚úÖ **Ejecuta todas las celdas en orden**\n",
        "\n",
        "### üìù Notas importantes:\n",
        "\n",
        "- El modelo se entrena **solo con ejemplos normales** (entrenamiento no supervisado)\n",
        "- El umbral de detecci√≥n se calcula autom√°ticamente usando el percentil especificado en `THRESHOLD_PERCENTILE`\n",
        "- Todos los artefactos (modelo, gr√°ficos, matrices de confusi√≥n) se guardan en `OUTPUT_DIR` y en MLflow\n",
        "- Las m√©tricas se registran en MLflow: `train_loss`, `train_reconstruction_error`, `val_precision`, `val_recall`, `val_f1`, `test_precision`, `test_recall`, `test_f1`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
