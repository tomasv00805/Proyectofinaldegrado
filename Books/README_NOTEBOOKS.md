# üìì Resumen de Notebooks

Este documento proporciona un resumen r√°pido de cada notebook en el proyecto para facilitar la navegaci√≥n y selecci√≥n del notebook adecuado.

> üìñ Para documentaci√≥n completa, ver [DOCUMENTACION_GENERAL.md](DOCUMENTACION_GENERAL.md)

---

## üìä Construcci√≥n de Datos

### `build_supervised_ecg_dataset.ipynb`

**Prop√≥sito**: Crear dataset binario supervisado (NORMAL vs AN√ìMALO) desde datos crudos.

**Input**: 
- PTB-XL (archivos `.hea`, `.dat`, CSV)
- MIMIC-IV-ECG (archivos `.hea`, `.dat`, CSV)

**Output**: 
- `data/Datos_supervisados/` con arrays numpy y tensores PyTorch
- Splits train/val/test balanceados (70/15/15)
- 10 folds estratificados para cross-validation

**Tiempo estimado**: 1-2 horas

**Requisitos**: 
- Datasets PTB-XL y MIMIC-IV-ECG descargados
- ~50GB espacio en disco

**Documentaci√≥n detallada**: Ver `Documentacion Datos Supervisados.md`

---

### `build_unsupervised_ecg_dataset.ipynb`

**Prop√≥sito**: Crear dataset para entrenamiento de autoencoders (solo normales en train).

**Input**: 
- Datos ya procesados desde `Datos_supervisados/`

**Output**: 
- `data/Datos_no_supervisados/`
- Train: Solo normales (label=0)
- Val/Test: Mezcla de normales y an√≥malos (con labels)

**Tiempo estimado**: 10-20 minutos

**Requisitos**: 
- Dataset supervisado ya construido

---

## üîΩ Downsampling

### `downsample_supervised_data.ipynb`

**Prop√≥sito**: Reducir frecuencia de muestreo de datos supervisados de 500Hz ‚Üí 200Hz.

**Input**: 
- `Datos_supervisados/numpy/` y `Datos_supervisados/tensors/` (500Hz)

**Output**: 
- `Datos_supervisados/numpy_200hz/` y `Datos_supervisados/tensors_200hz/` (200Hz)
- Reduce de 5000 ‚Üí 2000 muestras por se√±al

**Tiempo estimado**: 30-60 minutos

**Ventajas**: 
- Reduce tama√±o de archivos ~2.5x
- Acelera entrenamiento
- Mantiene preprocesado original

---

### `downsample_unsupervised_data.ipynb`

**Prop√≥sito**: Reducir frecuencia de muestreo de datos no supervisados de 500Hz ‚Üí 200Hz y convertir a tensores.

**Input**: 
- `Datos_no_supervisados/numpy/` (500Hz)

**Output**: 
- `Datos_no_supervisados/numpy_200hz/` (200Hz)
- `Datos_no_supervisados/tensors_200hz/` (tensores PyTorch listos)

**Tiempo estimado**: 30-60 minutos

**Caracter√≠sticas**: 
- Guardado incremental constante (checkpoints cada 5 chunks)
- Genera tensores listos para entrenamiento

---

## üß† Clasificaci√≥n Supervisada

### `cnn1d_classification_supervised.ipynb`

**Prop√≥sito**: Clasificaci√≥n binaria con CNN1D puro.

**Arquitectura**: CNN1D para extracci√≥n de caracter√≠sticas locales.

**Input**: 
- `Datos_supervisados/tensors_200hz/` (archivos `.pt`)

**Output**: 
- Modelo entrenado guardado en `models/`
- M√©tricas en MLflow
- Gr√°ficos de entrenamiento y evaluaci√≥n

**Tiempo estimado**: 2-3 horas

**GPU**: Recomendada (RTX 5080 compatible)

**Recomendado para**: Baseline r√°pido, comparaci√≥n con otros modelos

---

### `cnn1d_lstm_classification_supervised.ipynb` ‚≠ê **RECOMENDADO**

**Prop√≥sito**: Clasificaci√≥n binaria con arquitectura h√≠brida CNN1D + LSTM.

**Arquitectura**: CNN1D para caracter√≠sticas locales + LSTM para dependencias temporales.

**Input**: 
- `Datos_supervisados/tensors_200hz/` (archivos `.pt`)

**Output**: 
- Modelo entrenado guardado en `models/`
- M√©tricas en MLflow
- Gr√°ficos de entrenamiento y evaluaci√≥n

**Tiempo estimado**: 2-4 horas

**GPU**: Recomendada (RTX 5080 compatible)

**Recomendado para**: **Uso general** - mejor balance rendimiento/complejidad

**Ventajas**: 
- Combina lo mejor de CNN y LSTM
- Buen rendimiento sin excesiva complejidad

---

### `cnn1d_transformer_classification_supervised.ipynb`

**Prop√≥sito**: Clasificaci√≥n binaria con CNN1D + Transformer.

**Arquitectura**: CNN1D + Transformer con self-attention para relaciones globales.

**Input**: 
- `Datos_supervisados/tensors_200hz/` (archivos `.pt`)

**Output**: 
- Modelo entrenado guardado en `models/`
- M√©tricas en MLflow
- Gr√°ficos de entrenamiento y evaluaci√≥n

**Tiempo estimado**: 3-5 horas

**GPU**: Requerida (m√°s lento que otros modelos)

**Recomendado para**: M√°ximo rendimiento cuando el tiempo no es limitante

**Ventajas**: 
- Mejor rendimiento potencial
- Captura dependencias complejas globales

---

### `lstm_classification_supervised.ipynb`

**Prop√≥sito**: Clasificaci√≥n binaria con LSTM puro.

**Arquitectura**: M√∫ltiples capas LSTM para secuencias temporales.

**Input**: 
- `Datos_supervisados/tensors_200hz/` (archivos `.pt`)

**Output**: 
- Modelo entrenado guardado en `models/`
- M√©tricas en MLflow
- Gr√°ficos de entrenamiento y evaluaci√≥n

**Tiempo estimado**: 2-4 horas

**GPU**: Recomendada

**Recomendado para**: Comparaci√≥n con arquitecturas h√≠bridas

---

## üîç Detecci√≥n de Anomal√≠as (Autoencoders)

### `cnn1d_autoencoder_anomaly_detection.ipynb`

**Prop√≥sito**: Detecci√≥n de anomal√≠as con autoencoder CNN1D puro.

**Arquitectura**: Encoder-decoder CNN1D.

**Entrenamiento**: Solo con ejemplos normales (no supervisado)

**Input**: 
- `Datos_no_supervisados/tensors_200hz/` (archivos `.pt`)
- Train: Solo normales
- Val/Test: Mezcla con labels

**Output**: 
- Modelo autoencoder entrenado
- Umbral √≥ptimo seleccionado autom√°ticamente
- M√©tricas de detecci√≥n (precision, recall, F1)
- Gr√°ficos de distribuci√≥n de errores

**Tiempo estimado**: 2-3 horas

**GPU**: Recomendada

**Recomendado para**: Baseline r√°pido para detecci√≥n de anomal√≠as

---

### `cnn1d_lstm_autoencoder_anomaly_detection.ipynb` ‚≠ê **RECOMENDADO**

**Prop√≥sito**: Detecci√≥n de anomal√≠as con autoencoder h√≠brido CNN1D + LSTM.

**Arquitectura**: Encoder-decoder h√≠brido (CNN1D + LSTM).

**Entrenamiento**: Solo con ejemplos normales (no supervisado)

**Input**: 
- `Datos_no_supervisados/tensors_200hz/` (archivos `.pt`)
- Train: Solo normales
- Val/Test: Mezcla con labels

**Output**: 
- Modelo autoencoder entrenado
- Umbral √≥ptimo seleccionado autom√°ticamente
- M√©tricas de detecci√≥n (precision, recall, F1)
- Gr√°ficos de distribuci√≥n de errores

**Tiempo estimado**: 2-4 horas

**GPU**: Recomendada

**Recomendado para**: **Uso general** - mejor balance rendimiento/complejidad para detecci√≥n de anomal√≠as

**Ventajas**: 
- Mejor captura de patrones temporales complejos
- Reconstrucci√≥n m√°s precisa que CNN puro

---

### `lstm_autoencoder_pipeline.ipynb`

**Prop√≥sito**: Detecci√≥n de anomal√≠as con autoencoder LSTM puro.

**Arquitectura**: Encoder-decoder LSTM.

**Entrenamiento**: Solo con ejemplos normales (no supervisado)

**Input**: 
- `Datos_no_supervisados/tensors_200hz/` (archivos `.pt`)
- Train: Solo normales
- Val/Test: Mezcla con labels

**Output**: 
- Modelo autoencoder entrenado
- Umbral √≥ptimo seleccionado autom√°ticamente
- M√©tricas de detecci√≥n (precision, recall, F1)
- Gr√°ficos de distribuci√≥n de errores

**Tiempo estimado**: 2-4 horas

**GPU**: Recomendada

**Recomendado para**: Comparaci√≥n con arquitecturas h√≠bridas

---

## üîÑ Flujo de Trabajo Recomendado

### Para Clasificaci√≥n Supervisada

1. ‚úÖ `build_supervised_ecg_dataset.ipynb` - Construir datos
2. ‚úÖ `downsample_supervised_data.ipynb` - (Opcional) Reducir a 200Hz
3. ‚úÖ `cnn1d_lstm_classification_supervised.ipynb` - Entrenar modelo ‚≠ê

### Para Detecci√≥n de Anomal√≠as

1. ‚úÖ `build_supervised_ecg_dataset.ipynb` - Construir datos base
2. ‚úÖ `build_unsupervised_ecg_dataset.ipynb` - Preparar datos no supervisados
3. ‚úÖ `downsample_unsupervised_data.ipynb` - (Opcional) Reducir a 200Hz
4. ‚úÖ `cnn1d_lstm_autoencoder_anomaly_detection.ipynb` - Entrenar modelo ‚≠ê

---

## ‚öôÔ∏è Configuraci√≥n Com√∫n

Todos los notebooks de entrenamiento requieren:

1. **Setup CUDA (Windows)**: Ejecutar celda de setup antes de imports
2. **Configurar DATA_DIR**: Ajustar ruta a tus datos
3. **GPU**: Recomendada para entrenamiento r√°pido

Ver [DOCUMENTACION_GENERAL.md](DOCUMENTACION_GENERAL.md) para detalles completos.

---

## üìä Comparaci√≥n R√°pida

| Notebook | Tipo | Arquitectura | Tiempo | Recomendado |
|----------|------|--------------|--------|-------------|
| `cnn1d_classification_supervised` | Clasificaci√≥n | CNN1D | 2-3h | Baseline |
| `cnn1d_lstm_classification_supervised` | Clasificaci√≥n | CNN1D+LSTM | 2-4h | ‚≠ê **S√≠** |
| `cnn1d_transformer_classification_supervised` | Clasificaci√≥n | CNN1D+Transformer | 3-5h | M√°ximo rendimiento |
| `lstm_classification_supervised` | Clasificaci√≥n | LSTM | 2-4h | Comparaci√≥n |
| `cnn1d_autoencoder_anomaly_detection` | Anomal√≠as | CNN1D AE | 2-3h | Baseline |
| `cnn1d_lstm_autoencoder_anomaly_detection` | Anomal√≠as | CNN1D+LSTM AE | 2-4h | ‚≠ê **S√≠** |
| `lstm_autoencoder_pipeline` | Anomal√≠as | LSTM AE | 2-4h | Comparaci√≥n |

---

**√öltima actualizaci√≥n**: 2025-01-XX

