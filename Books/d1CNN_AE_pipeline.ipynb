{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü´Ä 1D CNN Autoencoder ‚Äî RTX 5080 Ready\n",
        "\n",
        "Este notebook reconstruye el flujo completo para entrenar y evaluar un **Autoencoder 1D CNN** sobre los datos preparados (PTB-XL + MIMIC normalizados, derivaciones II/V1/V5 a 500‚ÄØHz). Est√° pensado para funcionar sin sobresaltos en Windows con una **RTX 5080 (Blackwell)**:\n",
        "\n",
        "- Celda √∫nica de instalaci√≥n que se asegura de tener la build correcta de **PyTorch nightly cu128** y a√±ade rutas DLL indispensables.\n",
        "- Configuraci√≥n centralizada y auto documentada (JSON opcional) para hiperpar√°metros, rutas, MLflow y umbrales.\n",
        "- Pipeline con **Prefect** + **MLflow** minimalista pero robusto: entrenamiento, logging, artefactos y evaluaci√≥n.\n",
        "- Fallback autom√°tico a CPU si CUDA no queda disponible tras la instalaci√≥n.\n",
        "\n",
        "> ‚ñ∂Ô∏è Recomendaci√≥n: ejecuta cada celda en orden. Si instalas PyTorch, reinicia el kernel antes de continuar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß≠ Gu√≠a r√°pida\n",
        "\n",
        "1. **Instala dependencias** (celda \"Setup RTX 5080\"), reinicia el kernel si instal√≥ PyTorch.\n",
        "2. **Configura** rutas y par√°metros en la celda \"Config\" (o crea/edita `../config/ae1d_config.json`).\n",
        "3. **Carga datos** con la celda de DataLoaders apuntando al dataset combinado PTB-XL + MIMIC (`../data/combined_ptbxl_mimic_500hz_iv1v5`).\n",
        "4. **Define y revisa** el modelo Autoencoder.\n",
        "5. **Entrena** con Prefect + MLflow (celda de entrenamiento).\n",
        "6. **Eval√∫a** m√©tricas de reconstrucci√≥n y detecci√≥n de anomal√≠as.\n",
        "\n",
        "Durante todo el flujo se guardan artefactos y par√°metros en `../mlflow.db` y `../mlflow_artifacts/`.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Configuraci√≥n R√°pida\n",
        "\n",
        "### üéØ Cambiar Umbral de Anomal√≠as\n",
        "**Celda 10** - B√∫squeda de umbral √≥ptimo:\n",
        "```python\n",
        "# Percentiles m√°s altos = menos falsos positivos\n",
        "percentiles=[80, 85, 90, 92, 94, 96, 98, 99]\n",
        "\n",
        "# FPR m√°ximo permitido (5% = 0.05)\n",
        "max_fpr=0.05\n",
        "```\n",
        "\n",
        "### ‚öôÔ∏è Cambiar Pesos/Hiperpar√°metros del Modelo\n",
        "**Celda 2** - Configuraci√≥n:\n",
        "```python\n",
        "# En el archivo JSON o en la celda:\n",
        "{\n",
        "  \"model\": {\n",
        "    \"base_filters\": 32,      # M√°s = modelo m√°s grande\n",
        "    \"kernels\": [11, 7, 9, 11], # Tama√±os de kernel\n",
        "    \"leak\": 0.1              # Pendiente LeakyReLU\n",
        "  },\n",
        "  \"training\": {\n",
        "    \"epochs\": 30,            # N√∫mero de √©pocas\n",
        "    \"batch_size\": 64,        # Tama√±o de batch\n",
        "    \"lr\": 0.0003,            # Learning rate\n",
        "    \"weight_decay\": 1e-5     # Regularizaci√≥n L2\n",
        "  }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: c:\\Python311\\python.exe\n",
            "Working dir: S:\\Proyecto final\\Books\n",
            "DLL directories a√±adidos:\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\n",
            "  - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp\n",
            "‚è≥ Instalando mlflow>=2.16 ...\n",
            "‚è≥ Instalando prefect>=3 ...\n",
            "‚è≥ Instalando scikit-learn ...\n",
            "‚úî matplotlib ya instalado\n",
            "‚úî pandas ya instalado\n",
            "‚úî numpy ya instalado\n",
            "Torch info:\n",
            "  - torch_version: 2.10.0.dev20251113+cu128\n",
            "  - cuda_version: 12.8\n",
            "  - cuda_available: True\n",
            "GPU detectada: NVIDIA GeForce RTX 5080 | SM 120\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üîß Setup RTX 5080 ‚Äî dependencias + CUDA DLL\n",
        "# Ejecuta una sola vez (o tras actualizar drivers/librer√≠as)\n",
        "# ========================================\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from textwrap import dedent\n",
        "\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"Working dir: {Path.cwd().resolve()}\")\n",
        "\n",
        "CUDA_CANDIDATES = [\n",
        "    os.environ.get(\"CUDA_PATH\"),\n",
        "    os.environ.get(\"CUDA_PATH_V12_8\"),\n",
        "    r\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\bin\",\n",
        "    r\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\libnvvp\",\n",
        "    r\"C:\\\\Program Files\\\\NVIDIA\\\\CUDNN\",\n",
        "]\n",
        "\n",
        "added = []\n",
        "if hasattr(os, \"add_dll_directory\"):\n",
        "    for candidate in CUDA_CANDIDATES:\n",
        "        if not candidate:\n",
        "            continue\n",
        "        path = Path(candidate)\n",
        "        if path.is_dir():\n",
        "            try:\n",
        "                os.add_dll_directory(str(path))\n",
        "                added.append(str(path))\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "if added:\n",
        "    print(\"DLL directories a√±adidos:\")\n",
        "    for path in added:\n",
        "        print(\"  -\", path)\n",
        "\n",
        "BASE_PACKAGES = [\n",
        "    \"mlflow>=2.16\",\n",
        "    \"prefect>=3\",\n",
        "    \"scikit-learn\",\n",
        "    \"matplotlib\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "]\n",
        "\n",
        "def pip_install(spec: str) -> None:\n",
        "    module_name = spec.split(\"==\")[0].split(\"[\")[0].replace(\"-\", \"_\")\n",
        "    try:\n",
        "        __import__(module_name)\n",
        "        print(f\"‚úî {spec} ya instalado\")\n",
        "    except Exception:\n",
        "        print(f\"‚è≥ Instalando {spec} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", spec])\n",
        "\n",
        "for pkg in BASE_PACKAGES:\n",
        "    pip_install(pkg)\n",
        "\n",
        "TORCH_INSTALL_CMD = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"pip\",\n",
        "    \"install\",\n",
        "    \"--upgrade\",\n",
        "    \"--pre\",\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"torchaudio\",\n",
        "    \"--index-url\",\n",
        "    \"https://download.pytorch.org/whl/nightly/cu128\",\n",
        "]\n",
        "\n",
        "\n",
        "def ensure_torch_cuda() -> \"tuple[object | None, dict]\":\n",
        "    \"\"\"Importa torch, o instala la nightly cu128 si hace falta.\"\"\"\n",
        "    info: dict[str, str | float | bool] = {}\n",
        "    try:\n",
        "        import torch  # type: ignore\n",
        "        info[\"torch_version\"] = getattr(torch, \"__version__\", \"desconocida\")\n",
        "        info[\"cuda_version\"] = getattr(getattr(torch, \"version\", object()), \"cuda\", \"desconocida\")\n",
        "        info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "        if \"cu128\" not in info[\"torch_version\"] and not str(info[\"cuda_version\"]).startswith(\"12.8\"):\n",
        "            raise RuntimeError(\n",
        "                f\"Build {info['torch_version']} no es cu128. Se reinstalar√° la nightly para RTX 5080.\"\n",
        "            )\n",
        "        return torch, info\n",
        "    except Exception as err:\n",
        "        print(\"‚ö†Ô∏è Torch no usable todav√≠a:\", err)\n",
        "        print(\"   Instalando nightly cu128 desde PyTorch (puede tardar).\")\n",
        "        subprocess.check_call(TORCH_INSTALL_CMD)\n",
        "        import importlib\n",
        "        import time\n",
        "        time.sleep(2)\n",
        "        importlib.invalidate_caches()\n",
        "        import torch  # type: ignore\n",
        "        info[\"torch_version\"] = getattr(torch, \"__version__\", \"desconocida\")\n",
        "        info[\"cuda_version\"] = getattr(getattr(torch, \"version\", object()), \"cuda\", \"desconocida\")\n",
        "        info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "        return torch, info\n",
        "\n",
        "\n",
        "torch, torch_info = ensure_torch_cuda()\n",
        "\n",
        "print(\"Torch info:\")\n",
        "for k, v in torch_info.items():\n",
        "    print(f\"  - {k}: {v}\")\n",
        "\n",
        "if torch_info.get(\"cuda_available\"):\n",
        "    try:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        cc = torch.cuda.get_device_properties(0)\n",
        "        print(f\"GPU detectada: {gpu_name} | SM {cc.major}{cc.minor}\")\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è CUDA disponible pero no se pudo consultar GPU:\", e)\n",
        "else:\n",
        "    print(dedent(\n",
        "        \"\"\"\n",
        "        ‚ö†Ô∏è CUDA sigue inactiva. Revisa drivers / reinicia kernel tras la instalaci√≥n.\n",
        "        Si el problema contin√∫a, ejecuta manualmente:\n",
        "          pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
        "        \"\"\"\n",
        "    ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando configuraci√≥n cargada desde ..\\config\\ae1d_config.json\n",
            "Dataset utilizado: S:\\Proyecto final\\data\\combined_ptbxl_mimic_500hz_iv1v5\n",
            "Configuraci√≥n activa:\n",
            "{'mlflow': {'experiment_name': 'ptbxl_ae_1dcnn_iv1v5', 'tracking_uri': None},\n",
            " 'model': {'base_filters': 32,\n",
            "           'in_channels': 3,\n",
            "           'kernels': [11, 7, 9, 11],\n",
            "           'leak': 0.1},\n",
            " 'paths': {'output_root': 'S:\\\\Proyecto '\n",
            "                          'final\\\\data\\\\combined_ptbxl_mimic_500hz_iv1v5'},\n",
            " 'threshold': {'k_std': 3.0, 'percentile': 98.0, 'use_mean_std': False},\n",
            " 'training': {'batch_size': 64,\n",
            "              'clip_grad': None,\n",
            "              'device': 'auto',\n",
            "              'epochs': 25,\n",
            "              'loss_fn': 'mse',\n",
            "              'lr': 0.0003,\n",
            "              'seed': 42,\n",
            "              'weight_decay': 1e-05}}\n",
            "Dispositivo seleccionado: cuda\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# ‚öôÔ∏è Config ‚Äî hiperpar√°metros centralizados\n",
        "# ========================================\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import random\n",
        "from dataclasses import dataclass, asdict, field\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "CONFIG_JSON = Path(\"../config/ae1d_config.json\")\n",
        "COMBINED_DEFAULT = Path(\"../data/combined_ptbxl_mimic_500hz_iv1v5\").resolve()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PathsConfig:\n",
        "    output_root: str = str(COMBINED_DEFAULT)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    seed: int = 42\n",
        "    device: str = \"auto\"  # \"auto\" -> cuda si disponible, sino cpu\n",
        "    epochs: int = 30\n",
        "    batch_size: int = 64\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    clip_grad: float | None = None\n",
        "    loss_fn: str = \"mse\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    in_channels: int = 3\n",
        "    base_filters: int = 32\n",
        "    leak: float = 0.1\n",
        "    kernels: tuple[int, int, int, int] = (11, 7, 9, 11)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ThresholdConfig:\n",
        "    use_mean_std: bool = False\n",
        "    k_std: float = 3.0\n",
        "    percentile: float = 98.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MlflowConfig:\n",
        "    experiment_name: str = \"ae1d\"\n",
        "    tracking_uri: str | None = None  # se asigna en runtime al sqlite ../mlflow.db\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class NotebookConfig:\n",
        "    paths: PathsConfig = field(default_factory=PathsConfig)\n",
        "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    threshold: ThresholdConfig = field(default_factory=ThresholdConfig)\n",
        "    mlflow: MlflowConfig = field(default_factory=MlflowConfig)\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"paths\": asdict(self.paths),\n",
        "            \"training\": asdict(self.training),\n",
        "            \"model\": asdict(self.model),\n",
        "            \"threshold\": asdict(self.threshold),\n",
        "            \"mlflow\": asdict(self.mlflow),\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: dict) -> \"NotebookConfig\":\n",
        "        def _get(section: str, dataclass_type):\n",
        "            return dataclass_type(**data.get(section, {}))\n",
        "\n",
        "        return cls(\n",
        "            paths=_get(\"paths\", PathsConfig),\n",
        "            training=_get(\"training\", TrainingConfig),\n",
        "            model=_get(\"model\", ModelConfig),\n",
        "            threshold=_get(\"threshold\", ThresholdConfig),\n",
        "            mlflow=_get(\"mlflow\", MlflowConfig),\n",
        "        )\n",
        "\n",
        "\n",
        "DEFAULT_CONFIG = NotebookConfig()\n",
        "\n",
        "user_cfg: NotebookConfig = DEFAULT_CONFIG\n",
        "if CONFIG_JSON.exists():\n",
        "    try:\n",
        "        with open(CONFIG_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        user_cfg = NotebookConfig.from_dict({**DEFAULT_CONFIG.to_dict(), **data})\n",
        "        print(f\"Usando configuraci√≥n cargada desde {CONFIG_JSON}\")\n",
        "    except Exception as err:\n",
        "        print(f\"‚ö†Ô∏è No se pudo leer {CONFIG_JSON}: {err}. Se usar√° la configuraci√≥n por defecto.\")\n",
        "else:\n",
        "    CONFIG_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(CONFIG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(DEFAULT_CONFIG.to_dict(), f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Plantilla de configuraci√≥n creada en {CONFIG_JSON}\")\n",
        "\n",
        "CONFIG = user_cfg\n",
        "\n",
        "if not Path(CONFIG.paths.output_root).exists():\n",
        "    print(\n",
        "        f\"‚ö†Ô∏è Ruta {CONFIG.paths.output_root} no existe. Se usar√° el dataset combinado por defecto: {COMBINED_DEFAULT}\"\n",
        "    )\n",
        "    CONFIG.paths.output_root = str(COMBINED_DEFAULT)\n",
        "\n",
        "# --- helpers ---\n",
        "def resolve_device(device_setting: str) -> str:\n",
        "    if device_setting.lower() == \"auto\":\n",
        "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return device_setting.lower()\n",
        "\n",
        "\n",
        "def set_seed_everywhere(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "TRAINING = CONFIG.training\n",
        "DEVICE = resolve_device(TRAINING.device)\n",
        "set_seed_everywhere(TRAINING.seed)\n",
        "\n",
        "OUTPUT_ROOT = Path(CONFIG.paths.output_root).resolve()\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Dataset utilizado: {OUTPUT_ROOT}\")\n",
        "X_NORM_MM_DAT = OUTPUT_ROOT / \"X_norm_mm.dat\"\n",
        "X_NORM_RAW_DAT = OUTPUT_ROOT / \"X_norm_raw.dat\"\n",
        "X_NORM_FILT_DAT = OUTPUT_ROOT / \"X_norm_filt.dat\"\n",
        "X_ANOM_MM_NPY = OUTPUT_ROOT / \"X_anom_mm.npy\"\n",
        "DIMS_JSON = OUTPUT_ROOT / \"dims.json\"\n",
        "SPLITS_DIR = OUTPUT_ROOT / \"splits\"\n",
        "IDX_NORM_TRAIN = SPLITS_DIR / \"idx_norm_train.npy\"\n",
        "IDX_NORM_VAL = SPLITS_DIR / \"idx_norm_val.npy\"\n",
        "IDX_NORM_TEST = SPLITS_DIR / \"idx_norm_test.npy\"\n",
        "IDX_ANOM_VAL = SPLITS_DIR / \"idx_anom_val.npy\"\n",
        "IDX_ANOM_TEST = SPLITS_DIR / \"idx_anom_test.npy\"\n",
        "\n",
        "print(\"Configuraci√≥n activa:\")\n",
        "pprint(CONFIG.to_dict())\n",
        "print(f\"Dispositivo seleccionado: {DEVICE}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recreando datasets con limpieza autom√°tica de NaN/Inf...\n",
            "‚úì Datasets recreados con limpieza autom√°tica de NaN/Inf\n",
            "  Train: 121284, Val: 15160, Test: 15162\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üîß CORRECCI√ìN: Limpiar NaN/Inf en los datasets\n",
        "# ========================================\n",
        "# Redefinir las clases Dataset para limpiar autom√°ticamente valores no finitos\n",
        "\n",
        "class MemmapDatasetClean(Dataset):\n",
        "    \"\"\"Versi√≥n de MemmapDataset que limpia autom√°ticamente NaN/Inf\"\"\"\n",
        "    def __init__(self, x_memmap: np.memmap, indices: np.ndarray):\n",
        "        self.x = x_memmap\n",
        "        self.indices = np.array(indices, dtype=np.int64)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        sample = self.x[self.indices[idx]]  # [T, C]\n",
        "        sample = np.transpose(sample, (1, 0)).copy()  # [C, T]\n",
        "        sample_tensor = torch.from_numpy(sample).float()\n",
        "        \n",
        "        # Limpiar valores no finitos (NaN/Inf) autom√°ticamente\n",
        "        if not torch.isfinite(sample_tensor).all():\n",
        "            # Reemplazar NaN con 0 e Inf con valores grandes pero finitos\n",
        "            sample_tensor = torch.where(torch.isfinite(sample_tensor), sample_tensor, torch.zeros_like(sample_tensor))\n",
        "            # Clamp para asegurar que no haya valores extremos\n",
        "            sample_tensor = torch.clamp(sample_tensor, min=-1e6, max=1e6)\n",
        "        \n",
        "        return sample_tensor\n",
        "\n",
        "\n",
        "class NpyDatasetClean(Dataset):\n",
        "    \"\"\"Versi√≥n de NpyDataset que limpia autom√°ticamente NaN/Inf\"\"\"\n",
        "    def __init__(self, arr: np.ndarray, indices: np.ndarray):\n",
        "        self.arr = arr\n",
        "        self.indices = np.array(indices, dtype=np.int64)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        sample = self.arr[self.indices[idx]]  # [T, C]\n",
        "        sample = np.transpose(sample, (1, 0)).copy()\n",
        "        sample_tensor = torch.from_numpy(sample).float()\n",
        "        \n",
        "        # Limpiar valores no finitos (NaN/Inf) autom√°ticamente\n",
        "        if not torch.isfinite(sample_tensor).all():\n",
        "            # Reemplazar NaN con 0 e Inf con valores grandes pero finitos\n",
        "            sample_tensor = torch.where(torch.isfinite(sample_tensor), sample_tensor, torch.zeros_like(sample_tensor))\n",
        "            # Clamp para asegurar que no haya valores extremos\n",
        "            sample_tensor = torch.clamp(sample_tensor, min=-1e6, max=1e6)\n",
        "        \n",
        "        return sample_tensor\n",
        "\n",
        "\n",
        "# Recrear datasets con limpieza autom√°tica\n",
        "print(\"Recreando datasets con limpieza autom√°tica de NaN/Inf...\")\n",
        "ds_train = MemmapDatasetClean(X_norm_mm, idx_norm_train)\n",
        "ds_val_n = MemmapDatasetClean(X_norm_mm, idx_norm_val)\n",
        "ds_test_n = MemmapDatasetClean(X_norm_mm, idx_norm_test)\n",
        "\n",
        "if len(idx_anom_val) > 0 and X_anom_mm.shape[0] > 0:\n",
        "    ds_val_a = NpyDatasetClean(X_anom_mm, idx_anom_val)\n",
        "else:\n",
        "    ds_val_a = None\n",
        "\n",
        "if len(idx_anom_test) > 0 and X_anom_mm.shape[0] > 0:\n",
        "    ds_test_a = NpyDatasetClean(X_anom_mm, idx_anom_test)\n",
        "else:\n",
        "    ds_test_a = None\n",
        "\n",
        "# Recrear dataloaders\n",
        "dl_train = make_dataloader(ds_train, BATCH_SIZE, shuffle=True)\n",
        "dl_val_n = make_dataloader(ds_val_n, BATCH_SIZE, shuffle=False)\n",
        "dl_test_n = make_dataloader(ds_test_n, BATCH_SIZE, shuffle=False)\n",
        "dl_val_a = make_dataloader(ds_val_a, BATCH_SIZE, shuffle=False) if ds_val_a else None\n",
        "dl_test_a = make_dataloader(ds_test_a, BATCH_SIZE, shuffle=False) if ds_test_a else None\n",
        "\n",
        "print(\"‚úì Datasets recreados con limpieza autom√°tica de NaN/Inf\")\n",
        "print(f\"  Train: {len(ds_train)}, Val: {len(ds_val_n)}, Test: {len(ds_test_n)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples disponibles:\n",
            "  Train normales: 121284\n",
            "  Val normales: 15160 | Val an√≥malos: 100\n",
            "  Test normales: 15162 | Test an√≥malos: 900\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üìÇ Datos ‚Äî memmap combinado (PTB-XL + MIMIC)\n",
        "# ========================================\n",
        "from typing import Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "def open_memmap_known_tc(path: Path, T: int, C: int, mode: str = \"r\", dtype=np.float32):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"No se encontr√≥ {path}. Verifica la extracci√≥n previa.\")\n",
        "    bytes_total = os.path.getsize(path)\n",
        "    if bytes_total % 4 != 0:\n",
        "        raise RuntimeError(f\"{path} no es m√∫ltiplo de 4 bytes (float32)\")\n",
        "    n_float32 = bytes_total // 4\n",
        "    if n_float32 % (T * C) != 0:\n",
        "        raise RuntimeError(f\"Tama√±o inconsistente con T={T}, C={C}\")\n",
        "    N = n_float32 // (T * C)\n",
        "    return np.memmap(path, dtype=dtype, mode=mode, shape=(N, T, C))\n",
        "\n",
        "\n",
        "if DIMS_JSON.exists():\n",
        "    with open(DIMS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        dims = json.load(f)\n",
        "elif (OUTPUT_ROOT / \"dims.npy\").exists():\n",
        "    arr = np.load(OUTPUT_ROOT / \"dims.npy\")\n",
        "    dims = {\"T\": int(arr[0]), \"C\": int(arr[1])}\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No se encontraron dims.json ni dims.npy en {OUTPUT_ROOT}. Ejecuta la preparaci√≥n de datos combinados.\"\n",
        "    )\n",
        "\n",
        "T = int(dims[\"T\"])\n",
        "C = int(dims[\"C\"])\n",
        "assert C == CONFIG.model.in_channels, f\"Esperaba {CONFIG.model.in_channels} derivaciones, pero C={C}\"\n",
        "\n",
        "\n",
        "class MemmapDataset(Dataset):\n",
        "    def __init__(self, x_memmap: np.memmap, indices: np.ndarray):\n",
        "        self.x = x_memmap\n",
        "        self.indices = np.array(indices, dtype=np.int64)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        sample = self.x[self.indices[idx]]  # [T, C]\n",
        "        sample = np.transpose(sample, (1, 0)).copy()  # [C, T]\n",
        "        return torch.from_numpy(sample).float()\n",
        "\n",
        "\n",
        "X_norm_mm = open_memmap_known_tc(X_NORM_MM_DAT, T, C)\n",
        "X_anom_mm = np.load(X_ANOM_MM_NPY) if X_ANOM_MM_NPY.exists() else np.zeros((0, T, C), dtype=np.float32)\n",
        "\n",
        "idx_norm_train = np.load(IDX_NORM_TRAIN)\n",
        "idx_norm_val = np.load(IDX_NORM_VAL)\n",
        "idx_norm_test = np.load(IDX_NORM_TEST)\n",
        "idx_anom_val = np.load(IDX_ANOM_VAL) if IDX_ANOM_VAL.exists() else np.zeros((0,), dtype=np.int64)\n",
        "idx_anom_test = np.load(IDX_ANOM_TEST) if IDX_ANOM_TEST.exists() else np.zeros((0,), dtype=np.int64)\n",
        "\n",
        "\n",
        "def make_dataloader(dataset: Dataset, batch_size: int, shuffle: bool) -> DataLoader:\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0, drop_last=False)\n",
        "\n",
        "\n",
        "ds_train = MemmapDataset(X_norm_mm, idx_norm_train)\n",
        "ds_val_n = MemmapDataset(X_norm_mm, idx_norm_val)\n",
        "ds_test_n = MemmapDataset(X_norm_mm, idx_norm_test)\n",
        "\n",
        "\n",
        "class NpyDataset(Dataset):\n",
        "    def __init__(self, arr: np.ndarray, indices: np.ndarray):\n",
        "        self.arr = arr\n",
        "        self.indices = np.array(indices, dtype=np.int64)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        sample = self.arr[self.indices[idx]]  # [T, C]\n",
        "        sample = np.transpose(sample, (1, 0)).copy()\n",
        "        return torch.from_numpy(sample).float()\n",
        "\n",
        "\n",
        "ds_val_a = NpyDataset(X_anom_mm, idx_anom_val) if len(idx_anom_val) else None\n",
        "ds_test_a = NpyDataset(X_anom_mm, idx_anom_test) if len(idx_anom_test) else None\n",
        "\n",
        "BATCH_SIZE = TRAINING.batch_size\n",
        "\n",
        "dl_train = make_dataloader(ds_train, BATCH_SIZE, shuffle=True)\n",
        "dl_val_n = make_dataloader(ds_val_n, BATCH_SIZE, shuffle=False)\n",
        "dl_test_n = make_dataloader(ds_test_n, BATCH_SIZE, shuffle=False)\n",
        "dl_val_a = make_dataloader(ds_val_a, BATCH_SIZE, shuffle=False) if ds_val_a else None\n",
        "dl_test_a = make_dataloader(ds_test_a, BATCH_SIZE, shuffle=False) if ds_test_a else None\n",
        "\n",
        "print(\"Samples disponibles:\")\n",
        "print(f\"  Train normales: {len(ds_train)}\")\n",
        "print(f\"  Val normales: {len(ds_val_n)} | Val an√≥malos: {0 if ds_val_a is None else len(ds_val_a)}\")\n",
        "print(f\"  Test normales: {len(ds_test_n)} | Test an√≥malos: {0 if ds_test_a is None else len(ds_test_a)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CORRECCI√ìN: Recargando an√≥malos\n",
            "============================================================\n",
            "‚úì idx_anom_val.npy: 100 √≠ndices cargados\n",
            "‚úì idx_anom_test.npy: 900 √≠ndices cargados\n",
            "‚úì X_anom_mm ya cargado: shape (1000, 5000, 3)\n",
            "X_anom_mm.shape: (1000, 5000, 3)\n",
            "‚úì ds_val_a recreado: 100 muestras\n",
            "‚úì ds_test_a recreado: 900 muestras\n",
            "‚úì dl_val_a recreado\n",
            "‚úì dl_test_a recreado\n",
            "\n",
            "============================================================\n",
            "RESUMEN FINAL:\n",
            "============================================================\n",
            "  Train normales: 121284\n",
            "  Val normales: 15160 | Val an√≥malos: 100\n",
            "  Test normales: 15162 | Test an√≥malos: 900\n",
            "\n",
            "‚úì An√≥malos cargados correctamente\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üîß CORRECCI√ìN: Recargar an√≥malos correctamente\n",
        "# ========================================\n",
        "# IMPORTANTE: Ejecuta primero la celda 3 (carga de datos) antes de esta celda\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CORRECCI√ìN: Recargando an√≥malos\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verificar que las variables de configuraci√≥n est√©n definidas\n",
        "required_vars = ['IDX_ANOM_VAL', 'IDX_ANOM_TEST', 'X_ANOM_MM_NPY', 'DIMS_JSON']\n",
        "missing_vars = [v for v in required_vars if v not in globals()]\n",
        "if missing_vars:\n",
        "    raise NameError(f\"Variables no definidas: {missing_vars}. Ejecuta primero la celda 3 (carga de datos).\")\n",
        "\n",
        "# Verificar que las funciones necesarias est√©n definidas\n",
        "if 'NpyDataset' not in globals():\n",
        "    raise NameError(\"NpyDataset no est√° definido. Ejecuta primero la celda 3 (carga de datos).\")\n",
        "if 'make_dataloader' not in globals():\n",
        "    raise NameError(\"make_dataloader no est√° definido. Ejecuta primero la celda 3 (carga de datos).\")\n",
        "\n",
        "# Asegurar que NpyDataset y make_dataloader est√©n disponibles localmente\n",
        "NpyDataset = globals()['NpyDataset']\n",
        "make_dataloader = globals()['make_dataloader']\n",
        "\n",
        "# Recargar √≠ndices con verificaci√≥n\n",
        "if IDX_ANOM_VAL.exists():\n",
        "    idx_anom_val = np.load(IDX_ANOM_VAL)\n",
        "    print(f\"‚úì {IDX_ANOM_VAL.name}: {len(idx_anom_val)} √≠ndices cargados\")\n",
        "else:\n",
        "    idx_anom_val = np.zeros((0,), dtype=np.int64)\n",
        "    print(f\"‚úó {IDX_ANOM_VAL.name}: NO EXISTE\")\n",
        "\n",
        "if IDX_ANOM_TEST.exists():\n",
        "    idx_anom_test = np.load(IDX_ANOM_TEST)\n",
        "    print(f\"‚úì {IDX_ANOM_TEST.name}: {len(idx_anom_test)} √≠ndices cargados\")\n",
        "else:\n",
        "    idx_anom_test = np.zeros((0,), dtype=np.int64)\n",
        "    print(f\"‚úó {IDX_ANOM_TEST.name}: NO EXISTE\")\n",
        "\n",
        "# Cargar o recargar X_anom_mm si es necesario\n",
        "if 'X_anom_mm' not in globals():\n",
        "    # X_anom_mm no est√° definido, cargarlo\n",
        "    if X_ANOM_MM_NPY.exists():\n",
        "        X_anom_mm = np.load(X_ANOM_MM_NPY)\n",
        "        print(f\"‚úì X_anom_mm cargado: shape {X_anom_mm.shape}\")\n",
        "    else:\n",
        "        # Cargar T y C desde dims.json\n",
        "        if not DIMS_JSON.exists():\n",
        "            raise FileNotFoundError(f\"No se encontr√≥ {DIMS_JSON}. Ejecuta primero la celda 3.\")\n",
        "        with open(DIMS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "            dims = json.load(f)\n",
        "        T = int(dims[\"T\"])\n",
        "        C = int(dims[\"C\"])\n",
        "        X_anom_mm = np.zeros((0, T, C), dtype=np.float32)\n",
        "        print(f\"‚úó {X_ANOM_MM_NPY.name}: NO EXISTE - usando array vac√≠o (T={T}, C={C})\")\n",
        "else:\n",
        "    # X_anom_mm est√° definido en globals(), obtenerlo localmente\n",
        "    X_anom_mm = globals()['X_anom_mm']\n",
        "    if hasattr(X_anom_mm, 'shape') and X_anom_mm.shape[0] == 0:\n",
        "        # X_anom_mm est√° vac√≠o, intentar recargarlo\n",
        "        if X_ANOM_MM_NPY.exists():\n",
        "            X_anom_mm = np.load(X_ANOM_MM_NPY)\n",
        "            print(f\"‚úì X_anom_mm recargado: shape {X_anom_mm.shape}\")\n",
        "        else:\n",
        "            print(f\"‚ö† {X_ANOM_MM_NPY.name}: NO EXISTE - X_anom_mm sigue vac√≠o\")\n",
        "    else:\n",
        "        # X_anom_mm ya est√° cargado y tiene datos\n",
        "        print(f\"‚úì X_anom_mm ya cargado: shape {X_anom_mm.shape}\")\n",
        "\n",
        "print(f\"X_anom_mm.shape: {X_anom_mm.shape}\")\n",
        "\n",
        "# Recrear datasets de an√≥malos con verificaci√≥n\n",
        "if len(idx_anom_val) > 0 and X_anom_mm.shape[0] > 0:\n",
        "    if np.max(idx_anom_val) < X_anom_mm.shape[0]:\n",
        "        ds_val_a = NpyDataset(X_anom_mm, idx_anom_val)\n",
        "        print(f\"‚úì ds_val_a recreado: {len(ds_val_a)} muestras\")\n",
        "    else:\n",
        "        print(f\"‚ö† ERROR: √çndices val exceden tama√±o ({np.max(idx_anom_val)} >= {X_anom_mm.shape[0]})\")\n",
        "        ds_val_a = None\n",
        "else:\n",
        "    ds_val_a = None\n",
        "    if len(idx_anom_val) == 0:\n",
        "        print(\"‚ö† No hay √≠ndices de an√≥malos para val\")\n",
        "    if X_anom_mm.shape[0] == 0:\n",
        "        print(\"‚ö† X_anom_mm est√° vac√≠o\")\n",
        "\n",
        "if len(idx_anom_test) > 0 and X_anom_mm.shape[0] > 0:\n",
        "    if np.max(idx_anom_test) < X_anom_mm.shape[0]:\n",
        "        ds_test_a = NpyDataset(X_anom_mm, idx_anom_test)\n",
        "        print(f\"‚úì ds_test_a recreado: {len(ds_test_a)} muestras\")\n",
        "    else:\n",
        "        print(f\"‚ö† ERROR: √çndices test exceden tama√±o ({np.max(idx_anom_test)} >= {X_anom_mm.shape[0]})\")\n",
        "        ds_test_a = None\n",
        "else:\n",
        "    ds_test_a = None\n",
        "    if len(idx_anom_test) == 0:\n",
        "        print(\"‚ö† No hay √≠ndices de an√≥malos para test\")\n",
        "    if X_anom_mm.shape[0] == 0:\n",
        "        print(\"‚ö† X_anom_mm est√° vac√≠o\")\n",
        "\n",
        "# Recrear dataloaders (verificar que BATCH_SIZE est√© definido)\n",
        "if 'BATCH_SIZE' not in globals():\n",
        "    if 'TRAINING' in globals():\n",
        "        BATCH_SIZE = TRAINING.batch_size\n",
        "        print(f\"‚úì BATCH_SIZE obtenido de TRAINING: {BATCH_SIZE}\")\n",
        "    else:\n",
        "        BATCH_SIZE = 64  # valor por defecto\n",
        "        print(\"‚ö† BATCH_SIZE no definido, usando 64 por defecto\")\n",
        "\n",
        "if ds_val_a is not None:\n",
        "    dl_val_a = make_dataloader(ds_val_a, BATCH_SIZE, shuffle=False)\n",
        "    print(f\"‚úì dl_val_a recreado\")\n",
        "if ds_test_a is not None:\n",
        "    dl_test_a = make_dataloader(ds_test_a, BATCH_SIZE, shuffle=False)\n",
        "    print(f\"‚úì dl_test_a recreado\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMEN FINAL:\")\n",
        "print(\"=\"*60)\n",
        "if 'ds_train' in globals():\n",
        "    ds_train = globals()['ds_train']\n",
        "    print(f\"  Train normales: {len(ds_train)}\")\n",
        "if 'ds_val_n' in globals():\n",
        "    ds_val_n = globals()['ds_val_n']\n",
        "    print(f\"  Val normales: {len(ds_val_n)} | Val an√≥malos: {0 if ds_val_a is None else len(ds_val_a)}\")\n",
        "if 'ds_test_n' in globals():\n",
        "    ds_test_n = globals()['ds_test_n']\n",
        "    print(f\"  Test normales: {len(ds_test_n)} | Test an√≥malos: {0 if ds_test_a is None else len(ds_test_a)}\")\n",
        "\n",
        "if ds_val_a is None and ds_test_a is None:\n",
        "    print(\"\\n‚ö† PROBLEMA: No se pudieron cargar an√≥malos\")\n",
        "    print(\"  SOLUCI√ìN:\")\n",
        "    print(\"  1. Ve a 01_ecg_preprocessing_demo.ipynb\")\n",
        "    print(\"  2. Ejecuta la celda de train_valid_test_split()\")\n",
        "    print(\"  3. Verifica que se generen los archivos idx_anom_val.npy y idx_anom_test.npy\")\n",
        "    print(\"  4. Vuelve a ejecutar esta celda\")\n",
        "else:\n",
        "    print(\"\\n‚úì An√≥malos cargados correctamente\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AE1DCNN(\n",
            "  (encoder): Sequential(\n",
            "    (0): Conv1d(3, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "    (4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
            "    (7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    (8): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "    (9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (1): Conv1d(32, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
            "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    (3): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (4): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "    (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    (6): Conv1d(32, 3, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "    (7): Sigmoid()\n",
            "  )\n",
            "  (crop): Cropping1D()\n",
            ")\n",
            "Par√°metros totales: 0.071 M | Entrenables: 0.071 M\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üß† Modelo ‚Äî Autoencoder 1D CNN\n",
        "# ========================================\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Cropping1D(nn.Module):\n",
        "    def __init__(self, target_len: int):\n",
        "        super().__init__()\n",
        "        self.target_len = target_len\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        current = x.shape[-1]\n",
        "        if current == self.target_len:\n",
        "            return x\n",
        "        if current > self.target_len:\n",
        "            diff = current - self.target_len\n",
        "            start = diff // 2\n",
        "            end = start + self.target_len\n",
        "            return x[..., start:end]\n",
        "        # pad reflect\n",
        "        pad_total = self.target_len - current\n",
        "        left = pad_total // 2\n",
        "        right = pad_total - left\n",
        "        return nn.functional.pad(x, (left, right), mode=\"reflect\")\n",
        "\n",
        "\n",
        "class AE1DCNN(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig, in_len: int):\n",
        "        super().__init__()\n",
        "        c_in = cfg.in_channels\n",
        "        base = cfg.base_filters\n",
        "        leak = cfg.leak\n",
        "        k1, k2, k3, k4 = cfg.kernels\n",
        "        act = nn.LeakyReLU(leak, inplace=True)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(c_in, base * 2, kernel_size=k1, padding=k1 // 2),\n",
        "            act,\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(base * 2, base, kernel_size=k2, padding=k2 // 2),\n",
        "            act,\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(base, base, kernel_size=k3, padding=k3 // 2),\n",
        "            act,\n",
        "            nn.Conv1d(base, base, kernel_size=k4, padding=k4 // 2),\n",
        "            act,\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Conv1d(base, base * 2, kernel_size=k3, padding=k3 // 2),\n",
        "            act,\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Conv1d(base * 2, base, kernel_size=k2, padding=k2 // 2),\n",
        "            act,\n",
        "            nn.Conv1d(base, c_in, kernel_size=k1, padding=k1 // 2),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.crop = Cropping1D(in_len)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "        return self.crop(y)\n",
        "\n",
        "\n",
        "MODEL = AE1DCNN(CONFIG.model, in_len=T).to(DEVICE)\n",
        "PARAMS_TOTAL = sum(p.numel() for p in MODEL.parameters())\n",
        "PARAMS_TRAINABLE = sum(p.numel() for p in MODEL.parameters() if p.requires_grad)\n",
        "print(MODEL)\n",
        "print(f\"Par√°metros totales: {PARAMS_TOTAL/1e6:.3f} M | Entrenables: {PARAMS_TRAINABLE/1e6:.3f} M\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking URI: sqlite:///S:/Proyecto final/mlflow.db\n",
            "Experiment ID: 2\n",
            "Artifact root: file:///S:/Proyecto%20final/mlflow_artifacts\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üìü MLflow ‚Äî tracking en sqlite + artefactos locales\n",
        "# ========================================\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "PARENT_DIR = Path.cwd().parent.resolve()\n",
        "TRACKING_DB = (PARENT_DIR / \"mlflow.db\").resolve()\n",
        "ARTIFACT_ROOT = (PARENT_DIR / \"mlflow_artifacts\").resolve()\n",
        "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "mlflow.set_tracking_uri(f\"sqlite:///{TRACKING_DB.as_posix()}\")\n",
        "client = MlflowClient()\n",
        "EXPERIMENT_NAME = CONFIG.mlflow.experiment_name\n",
        "exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
        "if exp is None:\n",
        "    EXPERIMENT_ID = client.create_experiment(EXPERIMENT_NAME, artifact_location=ARTIFACT_ROOT.as_uri())\n",
        "else:\n",
        "    EXPERIMENT_ID = exp.experiment_id\n",
        "\n",
        "if mlflow.active_run() is not None:\n",
        "    print(\"Cerrando run previo colgado:\", mlflow.active_run().info.run_id)\n",
        "    mlflow.end_run()\n",
        "\n",
        "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
        "print(\"Experiment ID:\", EXPERIMENT_ID)\n",
        "print(\"Artifact root:\", ARTIFACT_ROOT.as_uri())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì M√≥dulo de evaluaci√≥n importado correctamente\n",
            "\n",
            "Funciones disponibles:\n",
            "  - compute_reconstruction_errors_with_labels(): Calcula errores con etiquetas\n",
            "  - find_optimal_threshold(): Busca umbral √≥ptimo basado en percentiles\n",
            "  - predict_with_threshold(): Predice etiquetas usando un umbral\n",
            "  - evaluate_test_set(): Eval√∫a en test con m√©tricas completas\n",
            "  - full_evaluation_pipeline(): Pipeline completo de evaluaci√≥n\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üîç EVALUACI√ìN MEJORADA: B√∫squeda de umbral √≥ptimo\n",
        "# ========================================\n",
        "#\n",
        "# Esta celda importa el m√≥dulo de evaluaci√≥n mejorado que implementa:\n",
        "# - B√∫squeda de umbral basada en percentiles\n",
        "# - Evaluaci√≥n completa con m√©tricas detalladas\n",
        "# - Visualizaci√≥n de resultados\n",
        "#\n",
        "# üìö FUNCIONES DISPONIBLES:\n",
        "#   - compute_reconstruction_errors_with_labels(): Calcula errores con etiquetas\n",
        "#   - find_optimal_threshold(): Busca umbral √≥ptimo basado en percentiles\n",
        "#   - predict_with_threshold(): Predice etiquetas usando un umbral\n",
        "#   - evaluate_test_set(): Eval√∫a en test con m√©tricas completas\n",
        "#   - full_evaluation_pipeline(): Pipeline completo de evaluaci√≥n\n",
        "#\n",
        "# üìñ DOCUMENTACI√ìN COMPLETA:\n",
        "#   Ver el archivo evaluation_threshold_tuning.py para m√°s detalles\n",
        "#\n",
        "# ========================================\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Agregar el directorio actual al path para importar el m√≥dulo\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "from evaluation_threshold_tuning import (\n",
        "    compute_reconstruction_errors_with_labels,\n",
        "    evaluate_test_set,\n",
        "    find_optimal_threshold,\n",
        "    full_evaluation_pipeline,\n",
        "    predict_with_threshold,\n",
        ")\n",
        "\n",
        "print(\"‚úì M√≥dulo de evaluaci√≥n importado correctamente\")\n",
        "print(\"\\nFunciones disponibles:\")\n",
        "print(\"  - compute_reconstruction_errors_with_labels(): Calcula errores con etiquetas\")\n",
        "print(\"  - find_optimal_threshold(): Busca umbral √≥ptimo basado en percentiles\")\n",
        "print(\"  - predict_with_threshold(): Predice etiquetas usando un umbral\")\n",
        "print(\"  - evaluate_test_set(): Eval√∫a en test con m√©tricas completas\")\n",
        "print(\"  - full_evaluation_pipeline(): Pipeline completo de evaluaci√≥n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "B√öSQUEDA DE UMBRAL √ìPTIMO\n",
            "================================================================================\n",
            "Modelo cargado desde: S:\\Proyecto final\\data\\combined_ptbxl_mimic_500hz_iv1v5\\ae_best.pt\n",
            "Dispositivo: cuda\n",
            "\n",
            "Calculando errores de reconstrucci√≥n en validaci√≥n...\n",
            "‚úì Errores calculados: 15260 muestras\n",
            "  - Normales (0): 15160\n",
            "  - An√≥malos (1): 100\n",
            "  - Error m√≠nimo: 0.000012\n",
            "  - Error m√°ximo: 0.002433\n",
            "  - Error promedio: 0.000032\n",
            "\n",
            "Buscando umbral √≥ptimo basado en percentiles...\n",
            "\n",
            "================================================================================\n",
            "B√öSQUEDA DE UMBRAL √ìPTIMO\n",
            "================================================================================\n",
            "\n",
            "Candidatos evaluados: 8 umbrales\n",
            "Filtro aplicado: FPR <= 0.05\n",
            "\n",
            "Resultados por umbral:\n",
            "--------------------------------------------------------------------------------\n",
            " threshold  recall_anom  precision_anom  fpr_normal  f2_score\n",
            "  0.000056     0.330000        0.054010    0.038127  0.163205\n",
            "  0.000068     0.270000        0.088235    0.018404  0.191218\n",
            "  0.000091     0.200000        0.130719    0.008773  0.180832\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚úì Mejor umbral seleccionado: 0.000068\n",
            "  - F2-score: 0.191218\n",
            "  - Recall (an√≥malos): 0.270000\n",
            "  - Precision (an√≥malos): 0.088235\n",
            "  - FPR (normales): 0.018404\n",
            "================================================================================\n",
            "\n",
            "‚úì Mejor umbral seleccionado: 0.000068\n",
            "  Este umbral se usar√° para evaluar en test.\n",
            "\n",
            "‚úì Resultados de b√∫squeda guardados en: S:\\Proyecto final\\data\\combined_ptbxl_mimic_500hz_iv1v5\\threshold_search_results.csv\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üéØ B√öSQUEDA DE UMBRAL √ìPTIMO EN VALIDACI√ìN\n",
        "# ========================================\n",
        "#\n",
        "# ‚ö†Ô∏è IMPORTANTE: ESTA ES LA CELDA DONDE SE DEFINE EL UMBRAL PARA DETECTAR ANOMAL√çAS\n",
        "#\n",
        "# Esta celda busca autom√°ticamente el mejor umbral basado en:\n",
        "# 1. Percentiles del error de reconstrucci√≥n en validaci√≥n\n",
        "# 2. Maximizaci√≥n del F2-score (da m√°s peso al recall de anomal√≠as)\n",
        "# 3. Filtro por FPR m√°ximo (evita demasiados falsos positivos)\n",
        "#\n",
        "# üìù C√ìMO CAMBIAR EL UMBRAL/PERCENTIL PARA DETECTAR ANOMAL√çAS:\n",
        "#\n",
        "# OPCI√ìN 1: Cambiar los percentiles a probar\n",
        "#   - Modifica la lista: percentiles=[80, 85, 90, 92, 94, 96, 98, 99]\n",
        "#   - Percentiles m√°s altos = umbral m√°s alto = menos falsos positivos\n",
        "#   - Percentiles m√°s bajos = umbral m√°s bajo = detecta m√°s anomal√≠as\n",
        "#\n",
        "# OPCI√ìN 2: Cambiar el FPR m√°ximo permitido\n",
        "#   - Modifica: max_fpr=0.05 (5% de falsos positivos m√°ximo)\n",
        "#   - max_fpr m√°s bajo = umbral m√°s alto = menos falsos positivos\n",
        "#   - max_fpr m√°s alto = umbral m√°s bajo = m√°s detecciones\n",
        "#\n",
        "# OPCI√ìN 3: Usar un umbral fijo manual\n",
        "#   - Comenta la b√∫squeda autom√°tica\n",
        "#   - Define: BEST_THR = 0.0001  # Tu valor fijo\n",
        "#   - El modelo clasificar√° como an√≥malo si error > BEST_THR\n",
        "#\n",
        "# üìä RESULTADO:\n",
        "#   - BEST_THR: Umbral √≥ptimo seleccionado (se usa en la siguiente celda)\n",
        "#   - best_val_metrics: M√©tricas en validaci√≥n con ese umbral\n",
        "#   - df_thresholds: Tabla con todos los umbrales probados\n",
        "#\n",
        "# ========================================\n",
        "\n",
        "# Cargar el mejor modelo entrenado\n",
        "best_ckpt = OUTPUT_ROOT / \"ae_best.pt\"\n",
        "if not best_ckpt.exists():\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ el checkpoint: {best_ckpt}. Ejecuta primero el entrenamiento.\")\n",
        "\n",
        "MODEL.load_state_dict(torch.load(best_ckpt, map_location=DEVICE))\n",
        "MODEL.to(DEVICE)\n",
        "MODEL.eval()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"B√öSQUEDA DE UMBRAL √ìPTIMO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Modelo cargado desde: {best_ckpt}\")\n",
        "print(f\"Dispositivo: {DEVICE}\")\n",
        "\n",
        "# Calcular errores en validaci√≥n (normales + an√≥malos)\n",
        "print(\"\\nCalculando errores de reconstrucci√≥n en validaci√≥n...\")\n",
        "val_errors, val_labels = compute_reconstruction_errors_with_labels(\n",
        "    model=MODEL,\n",
        "    normal_loader=dl_val_n,\n",
        "    anomalous_loader=dl_val_a,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "print(f\"‚úì Errores calculados: {len(val_errors)} muestras\")\n",
        "print(f\"  - Normales (0): {(val_labels == 0).sum()}\")\n",
        "print(f\"  - An√≥malos (1): {(val_labels == 1).sum()}\")\n",
        "print(f\"  - Error m√≠nimo: {val_errors.min():.6f}\")\n",
        "print(f\"  - Error m√°ximo: {val_errors.max():.6f}\")\n",
        "print(f\"  - Error promedio: {val_errors.mean():.6f}\")\n",
        "\n",
        "# Buscar umbral √≥ptimo\n",
        "print(\"\\nBuscando umbral √≥ptimo basado en percentiles...\")\n",
        "BEST_THR, best_val_metrics, df_thresholds = find_optimal_threshold(\n",
        "    val_errors=val_errors,\n",
        "    val_labels=val_labels,\n",
        "    percentiles=[80, 85, 90, 92, 94, 96, 98, 99],  # Percentiles a probar\n",
        "    max_fpr=0.05,  # FPR m√°ximo permitido (5%)\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Mejor umbral seleccionado: {BEST_THR:.6f}\")\n",
        "print(f\"  Este umbral se usar√° para evaluar en test.\")\n",
        "\n",
        "# Guardar resultados\n",
        "if OUTPUT_ROOT:\n",
        "    thresholds_path = OUTPUT_ROOT / \"threshold_search_results.csv\"\n",
        "    df_thresholds.to_csv(thresholds_path, index=False)\n",
        "    print(f\"\\n‚úì Resultados de b√∫squeda guardados en: {thresholds_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EVALUACI√ìN EN CONJUNTO DE TEST\n",
            "================================================================================\n",
            "Usando umbral √≥ptimo: 0.000068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/11/14 16:05:01 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/11/14 16:05:01 INFO mlflow.store.db.utils: Updating database tables\n",
            "2025-11-14 16:05:01 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "2025-11-14 16:05:01 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EVALUACI√ìN EN CONJUNTO DE TEST\n",
            "================================================================================\n",
            "\n",
            "Umbral utilizado: 0.000068\n",
            "\n",
            "Muestras totales: 16062\n",
            "  - Normales (0): 15162\n",
            "  - An√≥malos (1): 900\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MATRIZ DE CONFUSI√ìN\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "                Pred: Normal    Pred: An√≥malo  \n",
            "Real: Normal    14867           295            \n",
            "Real: An√≥malo   669             231            \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "M√âTRICAS DE CLASIFICACI√ìN\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "M√©tricas generales:\n",
            "  Accuracy:           0.939983\n",
            "  Specificity (TNR):  0.980543\n",
            "  FPR:                0.019457\n",
            "\n",
            "M√©tricas para clase NORMAL (0):\n",
            "  Precision:          0.956939\n",
            "  Recall:             0.980543\n",
            "  F1-score:           0.968597\n",
            "\n",
            "M√©tricas para clase AN√ìMALA (1):\n",
            "  Precision:          0.439163\n",
            "  Recall:             0.256667\n",
            "  F1-score:           0.323983\n",
            "  F2-score:           0.279932\n",
            "\n",
            "M√©tricas de ranking:\n",
            "  AUROC:              0.740068\n",
            "  AUPRC:              0.274369\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "REPORTE DE CLASIFICACI√ìN (sklearn)\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal     0.9569    0.9805    0.9686     15162\n",
            "     An√≥malo     0.4392    0.2567    0.3240       900\n",
            "\n",
            "    accuracy                         0.9400     16062\n",
            "   macro avg     0.6981    0.6186    0.6463     16062\n",
            "weighted avg     0.9279    0.9400    0.9325     16062\n",
            "\n",
            "================================================================================\n",
            "‚úì Matriz de confusi√≥n guardada en: S:\\Proyecto final\\data\\combined_ptbxl_mimic_500hz_iv1v5\\confusion_matrix_test.png\n",
            "\n",
            "‚úì M√©tricas guardadas en: S:\\Proyecto final\\data\\combined_ptbxl_mimic_500hz_iv1v5\\test_metrics.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-14 16:05:01 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "2025-11-14 16:05:01 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì M√©tricas guardadas en MLflow\n",
            "\n",
            "================================================================================\n",
            "‚úÖ EVALUACI√ìN COMPLETA FINALIZADA\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üìä EVALUACI√ìN FINAL EN TEST CON UMBRAL √ìPTIMO\n",
        "# ========================================\n",
        "#\n",
        "# Esta celda eval√∫a el modelo en el conjunto de TEST usando el umbral √≥ptimo\n",
        "# encontrado en la celda anterior (BEST_THR).\n",
        "#\n",
        "# ‚ö†Ô∏è IMPORTANTE: Si quieres usar un umbral diferente al encontrado autom√°ticamente:\n",
        "#   1. Define manualmente: BEST_THR = 0.0001  # Tu valor personalizado\n",
        "#   2. O modifica BEST_THR despu√©s de la celda 10\n",
        "#\n",
        "# üìä M√âTRICAS CALCULADAS:\n",
        "#   - Accuracy: Precisi√≥n general\n",
        "#   - Precision/Recall: Para clase normal y an√≥mala\n",
        "#   - F1-score y F2-score: Para clase an√≥mala (F2 da m√°s peso al recall)\n",
        "#   - Specificity (TNR): Tasa de verdaderos negativos\n",
        "#   - FPR: Tasa de falsos positivos\n",
        "#   - AUROC y AUPRC: M√©tricas de ranking\n",
        "#\n",
        "# üìÅ ARCHIVOS GENERADOS:\n",
        "#   - confusion_matrix_test.png: Visualizaci√≥n de la matriz de confusi√≥n\n",
        "#   - test_metrics.csv: Todas las m√©tricas en formato CSV\n",
        "#\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUACI√ìN EN CONJUNTO DE TEST\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Usando umbral √≥ptimo: {BEST_THR:.6f}\")\n",
        "\n",
        "# Evaluar en test con el umbral √≥ptimo\n",
        "test_metrics = evaluate_test_set(\n",
        "    model=MODEL,\n",
        "    test_normal_loader=dl_test_n,\n",
        "    test_anomalous_loader=dl_test_a,\n",
        "    device=DEVICE,\n",
        "    threshold=BEST_THR,\n",
        "    output_dir=OUTPUT_ROOT,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Guardar umbral √≥ptimo y m√©tricas en MLflow si est√° disponible\n",
        "try:\n",
        "    import mlflow\n",
        "    import time\n",
        "\n",
        "    if mlflow.active_run() is None:\n",
        "        # Crear un nuevo run para la evaluaci√≥n\n",
        "        mlflow.set_tracking_uri(f\"sqlite:///{(Path.cwd().parent / 'mlflow.db').as_posix()}\")\n",
        "        with mlflow.start_run(run_name=f\"eval_threshold_{int(time.time())}\", experiment_id=EXPERIMENT_ID):\n",
        "            mlflow.log_param(\"best_threshold\", BEST_THR)\n",
        "            mlflow.log_param(\"threshold_search_method\", \"percentile_based_f2_maximization\")\n",
        "            mlflow.log_metrics({f\"test_{k}\": float(v) for k, v in test_metrics.items() if isinstance(v, (int, float))})\n",
        "            print(\"\\n‚úì M√©tricas guardadas en MLflow\")\n",
        "    else:\n",
        "        # Usar el run activo\n",
        "        mlflow.log_param(\"best_threshold\", BEST_THR)\n",
        "        mlflow.log_metrics({f\"test_{k}\": float(v) for k, v in test_metrics.items() if isinstance(v, (int, float))})\n",
        "        print(\"\\n‚úì M√©tricas guardadas en MLflow\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö† No se pudo guardar en MLflow: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ EVALUACI√ìN COMPLETA FINALIZADA\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí° Para usar el pipeline completo, descomenta el c√≥digo en esta celda.\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üöÄ ALTERNATIVA: Pipeline completo en una sola llamada\n",
        "# ========================================\n",
        "# Si prefieres ejecutar todo el proceso de evaluaci√≥n en una sola funci√≥n:\n",
        "\n",
        "# Descomenta las siguientes l√≠neas para usar el pipeline completo:\n",
        "\"\"\"\n",
        "best_threshold, val_metrics, test_metrics = full_evaluation_pipeline(\n",
        "    model=MODEL,\n",
        "    val_normal_loader=dl_val_n,\n",
        "    val_anomalous_loader=dl_val_a,\n",
        "    test_normal_loader=dl_test_n,\n",
        "    test_anomalous_loader=dl_test_a,\n",
        "    device=DEVICE,\n",
        "    percentiles=[80, 85, 90, 92, 94, 96, 98, 99],\n",
        "    max_fpr=0.05,\n",
        "    output_dir=OUTPUT_ROOT,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Pipeline completo ejecutado\")\n",
        "print(f\"  Umbral √≥ptimo: {best_threshold:.6f}\")\n",
        "print(f\"  F2-score en test: {test_metrics['f2_anom']:.6f}\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"üí° Para usar el pipeline completo, descomenta el c√≥digo en esta celda.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs] Beginning flow run 'imposing-swift' for flow 'train_autoencoder_1d'\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs] Verificando datos de entrenamiento...\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   Shape del batch: torch.Size([64, 3, 5000])\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   Min: 0.000000, Max: 1.000000\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   Mean: 0.410954, Std: 0.291378\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   ‚úì Datos de entrenamiento son finitos\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs] \n",
            "Verificando datos de validaci√≥n...\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   Shape del batch: torch.Size([64, 3, 5000])\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   Min: 0.000000, Max: 1.000000\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   Mean: 0.406845, Std: 0.292782\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs]   ‚úì Datos de validaci√≥n son finitos\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs] MLflow run: eace69ca2e9c486cb7e20abdbdf2b8d1\n",
            "2025-11-14 16:05:44 INFO  [prefect.flow_runs] Usando dispositivo: cuda\n",
            "2025-11-14 16:06:19 INFO  [prefect.flow_runs] Epoch 001/25 | train=0.002228 | val=0.000191\n",
            "2025-11-14 16:07:01 INFO  [prefect.flow_runs] Epoch 002/25 | train=0.000143 | val=0.000104\n",
            "2025-11-14 16:07:36 INFO  [prefect.flow_runs] Epoch 003/25 | train=0.000099 | val=0.000080\n",
            "2025-11-14 16:08:10 INFO  [prefect.flow_runs] Epoch 004/25 | train=0.000079 | val=0.000063\n",
            "2025-11-14 16:08:44 INFO  [prefect.flow_runs] Epoch 005/25 | train=0.000067 | val=0.000117\n",
            "2025-11-14 16:09:18 INFO  [prefect.flow_runs] Epoch 006/25 | train=0.000060 | val=0.000075\n",
            "2025-11-14 16:09:52 INFO  [prefect.flow_runs] Epoch 007/25 | train=0.000056 | val=0.000072\n",
            "2025-11-14 16:10:26 INFO  [prefect.flow_runs] Epoch 008/25 | train=0.000052 | val=0.000041\n",
            "2025-11-14 16:11:00 INFO  [prefect.flow_runs] Epoch 009/25 | train=0.000050 | val=0.000042\n",
            "2025-11-14 16:11:40 INFO  [prefect.flow_runs] Epoch 010/25 | train=0.000049 | val=0.000055\n",
            "2025-11-14 16:12:21 INFO  [prefect.flow_runs] Epoch 011/25 | train=0.000047 | val=0.000041\n",
            "2025-11-14 16:13:03 INFO  [prefect.flow_runs] Epoch 012/25 | train=0.000046 | val=0.000041\n",
            "2025-11-14 16:13:44 INFO  [prefect.flow_runs] Epoch 013/25 | train=0.000046 | val=0.000035\n",
            "2025-11-14 16:14:26 INFO  [prefect.flow_runs] Epoch 014/25 | train=0.000045 | val=0.000035\n",
            "2025-11-14 16:15:05 INFO  [prefect.flow_runs] Epoch 015/25 | train=0.000044 | val=0.000034\n",
            "2025-11-14 16:15:40 INFO  [prefect.flow_runs] Epoch 016/25 | train=0.000043 | val=0.000073\n",
            "2025-11-14 16:16:14 INFO  [prefect.flow_runs] Epoch 017/25 | train=0.000043 | val=0.000032\n",
            "2025-11-14 16:16:48 INFO  [prefect.flow_runs] Epoch 018/25 | train=0.000043 | val=0.000062\n",
            "2025-11-14 16:17:23 INFO  [prefect.flow_runs] Epoch 019/25 | train=0.000042 | val=0.000054\n",
            "2025-11-14 16:17:57 INFO  [prefect.flow_runs] Epoch 020/25 | train=0.000042 | val=0.000052\n",
            "2025-11-14 16:18:31 INFO  [prefect.flow_runs] Epoch 021/25 | train=0.000042 | val=0.000031\n",
            "2025-11-14 16:19:06 INFO  [prefect.flow_runs] Epoch 022/25 | train=0.000041 | val=0.000100\n",
            "2025-11-14 16:19:40 INFO  [prefect.flow_runs] Epoch 023/25 | train=0.000041 | val=0.000029\n",
            "2025-11-14 16:20:14 INFO  [prefect.flow_runs] Epoch 024/25 | train=0.000041 | val=0.000060\n",
            "2025-11-14 16:20:49 INFO  [prefect.flow_runs] Epoch 025/25 | train=0.000041 | val=0.000036\n",
            "2025/11/14 16:20:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/11/14 16:20:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.10.0.dev20251113+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.10.0.dev20251113' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/11/14 16:20:55 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\tomas\\AppData\\Local\\Temp\\tmpmnyo3l_q\\model\\data, flavor: pytorch). Fall back to return ['torch==2.10.0.dev20251113', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
            "2025/11/14 16:20:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-11-14 16:20:55 INFO  [prefect.flow_runs] Finished in state Completed()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen entrenamiento:\n",
            "{'best_ckpt': 'S:\\\\Proyecto '\n",
            "              'final\\\\data\\\\combined_ptbxl_mimic_500hz_iv1v5\\\\ae_best.pt',\n",
            " 'best_val': 2.8826565134856912e-05,\n",
            " 'curves_png': 'S:\\\\Proyecto '\n",
            "               'final\\\\data\\\\combined_ptbxl_mimic_500hz_iv1v5\\\\loss_curves.png',\n",
            " 'device_used': 'cuda',\n",
            " 'run_id': 'eace69ca2e9c486cb7e20abdbdf2b8d1'}\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üèãÔ∏è Entrenamiento ‚Äî Prefect + MLflow\n",
        "# ========================================\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow.pytorch\n",
        "import numpy as np\n",
        "from prefect import task, flow\n",
        "from prefect.tasks import NO_CACHE\n",
        "\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "LOSS_FN = TRAINING.loss_fn.lower()\n",
        "\n",
        "\n",
        "def loss_function(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    if LOSS_FN == \"mse\":\n",
        "        return nn.functional.mse_loss(pred, target)\n",
        "    raise NotImplementedError(f\"Funci√≥n de p√©rdida no soportada: {LOSS_FN}\")\n",
        "\n",
        "\n",
        "def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \"__\") -> dict:\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else str(k)\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "\n",
        "def collect_run_params() -> dict:\n",
        "    return {\n",
        "        \"training\": asdict(TRAINING),\n",
        "        \"model\": asdict(CONFIG.model),\n",
        "        \"threshold\": asdict(CONFIG.threshold),\n",
        "        \"paths\": {\"output_root\": str(OUTPUT_ROOT)},\n",
        "        \"dims\": {\"T\": T, \"C\": C},\n",
        "    }\n",
        "\n",
        "\n",
        "@task(name=\"train_epoch\", log_prints=False, cache_policy=NO_CACHE)\n",
        "def train_epoch(model: AE1DCNN, loader: DataLoader, optimizer: torch.optim.Optimizer) -> float:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_samples = 0\n",
        "    model_device = next(model.parameters()).device\n",
        "    non_blocking = model_device.type == \"cuda\"\n",
        "    for xb in loader:\n",
        "        xb = xb.to(model_device, non_blocking=non_blocking)\n",
        "        \n",
        "        # Verificar que los datos sean finitos\n",
        "        if not torch.isfinite(xb).all():\n",
        "            print(\"‚ö†Ô∏è ADVERTENCIA: Datos de entrada contienen valores no finitos\")\n",
        "            return float('nan')\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        recon = model(xb)\n",
        "        \n",
        "        # Verificar que la reconstrucci√≥n sea finita\n",
        "        if not torch.isfinite(recon).all():\n",
        "            print(\"‚ö†Ô∏è ADVERTENCIA: Reconstrucci√≥n contiene valores no finitos\")\n",
        "            return float('nan')\n",
        "        \n",
        "        loss = loss_function(recon, xb)\n",
        "        \n",
        "        # Verificar que la p√©rdida sea finita\n",
        "        if not torch.isfinite(loss):\n",
        "            print(f\"‚ö†Ô∏è ADVERTENCIA: P√©rdida no finita: {loss.item()}\")\n",
        "            return float('nan')\n",
        "        \n",
        "        loss.backward()\n",
        "        if TRAINING.clip_grad is not None:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), TRAINING.clip_grad)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        n_samples += xb.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / max(1, n_samples)\n",
        "    if not np.isfinite(avg_loss):\n",
        "        print(f\"‚ö†Ô∏è ADVERTENCIA: P√©rdida promedio no finita: {avg_loss}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "@task(name=\"eval_epoch\", log_prints=False, cache_policy=NO_CACHE)\n",
        "def eval_epoch(model: AE1DCNN, loader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    n_samples = 0\n",
        "    model_device = next(model.parameters()).device\n",
        "    non_blocking = model_device.type == \"cuda\"\n",
        "    with torch.no_grad():\n",
        "        for xb in loader:\n",
        "            xb = xb.to(model_device, non_blocking=non_blocking)\n",
        "            \n",
        "            # Verificar que los datos sean finitos\n",
        "            if not torch.isfinite(xb).all():\n",
        "                print(\"‚ö†Ô∏è ADVERTENCIA: Datos de validaci√≥n contienen valores no finitos\")\n",
        "                return float('nan')\n",
        "            \n",
        "            recon = model(xb)\n",
        "            \n",
        "            # Verificar que la reconstrucci√≥n sea finita\n",
        "            if not torch.isfinite(recon).all():\n",
        "                print(\"‚ö†Ô∏è ADVERTENCIA: Reconstrucci√≥n de validaci√≥n contiene valores no finitos\")\n",
        "                return float('nan')\n",
        "            \n",
        "            loss = loss_function(recon, xb)\n",
        "            \n",
        "            # Verificar que la p√©rdida sea finita\n",
        "            if not torch.isfinite(loss):\n",
        "                print(f\"‚ö†Ô∏è ADVERTENCIA: P√©rdida de validaci√≥n no finita: {loss.item()}\")\n",
        "                return float('nan')\n",
        "            \n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            n_samples += xb.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / max(1, n_samples)\n",
        "    if not np.isfinite(avg_loss):\n",
        "        print(f\"‚ö†Ô∏è ADVERTENCIA: P√©rdida promedio de validaci√≥n no finita: {avg_loss}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "@flow(name=\"train_autoencoder_1d\", log_prints=True)\n",
        "def train_flow() -> dict:\n",
        "    global DEVICE\n",
        "    set_seed_everywhere(TRAINING.seed)\n",
        "    model = AE1DCNN(CONFIG.model, in_len=T).to(DEVICE)\n",
        "    local_device = DEVICE\n",
        "\n",
        "    # Warm-up para detectar problemas de CUDA\n",
        "    if DEVICE.startswith(\"cuda\"):\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                dummy_len = T\n",
        "                dummy = torch.randn(1, CONFIG.model.in_channels, dummy_len, device=DEVICE)\n",
        "                _ = model(dummy)\n",
        "        except Exception as err:\n",
        "            print(\"‚ö†Ô∏è CUDA no respondi√≥ (\", err, \") -> cambiando a CPU\")\n",
        "            model = model.to(\"cpu\")\n",
        "            local_device = \"cpu\"\n",
        "\n",
        "    DEVICE = local_device\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=TRAINING.lr, weight_decay=TRAINING.weight_decay)\n",
        "\n",
        "    # Verificaci√≥n inicial de los datos\n",
        "    print(\"Verificando datos de entrenamiento...\")\n",
        "    sample_batch = next(iter(dl_train))\n",
        "    print(f\"  Shape del batch: {sample_batch.shape}\")\n",
        "    print(f\"  Min: {sample_batch.min().item():.6f}, Max: {sample_batch.max().item():.6f}\")\n",
        "    print(f\"  Mean: {sample_batch.mean().item():.6f}, Std: {sample_batch.std().item():.6f}\")\n",
        "    \n",
        "    # Verificar y limpiar valores no finitos\n",
        "    if not torch.isfinite(sample_batch).all():\n",
        "        print(\"  ‚ö†Ô∏è ADVERTENCIA: El batch contiene valores no finitos (NaN o Inf)\")\n",
        "        nan_count = (~torch.isfinite(sample_batch)).sum().item()\n",
        "        print(f\"  Valores no finitos: {nan_count} de {sample_batch.numel()}\")\n",
        "        print(\"  PROBLEMA: Los datos contienen valores no finitos. Esto causar√° NaN en el entrenamiento.\")\n",
        "        print(\"  SOLUCI√ìN: Verifica que los datos est√©n correctamente normalizados.\")\n",
        "        print(\"  Los datos deber√≠an estar en el rango [0, 1] despu√©s de la normalizaci√≥n min-max.\")\n",
        "        print(\"  Revisa el pipeline de preprocesamiento en 01_ecg_preprocessing_demo.ipynb\")\n",
        "    else:\n",
        "        print(\"  ‚úì Datos de entrenamiento son finitos\")\n",
        "    \n",
        "    print(\"\\nVerificando datos de validaci√≥n...\")\n",
        "    val_sample_batch = next(iter(dl_val_n))\n",
        "    print(f\"  Shape del batch: {val_sample_batch.shape}\")\n",
        "    print(f\"  Min: {val_sample_batch.min().item():.6f}, Max: {val_sample_batch.max().item():.6f}\")\n",
        "    print(f\"  Mean: {val_sample_batch.mean().item():.6f}, Std: {val_sample_batch.std().item():.6f}\")\n",
        "    \n",
        "    if not torch.isfinite(val_sample_batch).all():\n",
        "        print(\"  ‚ö†Ô∏è ADVERTENCIA: El batch de validaci√≥n contiene valores no finitos (NaN o Inf)\")\n",
        "        nan_count = (~torch.isfinite(val_sample_batch)).sum().item()\n",
        "        print(f\"  Valores no finitos: {nan_count} de {val_sample_batch.numel()}\")\n",
        "        print(\"  PROBLEMA: Los datos contienen valores no finitos. Esto causar√° NaN en el entrenamiento.\")\n",
        "        print(\"  SOLUCI√ìN: Verifica que los datos est√©n correctamente normalizados.\")\n",
        "    else:\n",
        "        print(\"  ‚úì Datos de validaci√≥n son finitos\")\n",
        "    \n",
        "    # Verificar el rango de los datos (deber√≠an estar normalizados entre 0 y 1)\n",
        "    if torch.isfinite(sample_batch).all():\n",
        "        if sample_batch.min() < -1.0 or sample_batch.max() > 1.0:\n",
        "            print(f\"\\n  ‚ö†Ô∏è ADVERTENCIA: Datos fuera del rango esperado [-1, 1]\")\n",
        "            print(f\"     Rango actual: [{sample_batch.min().item():.6f}, {sample_batch.max().item():.6f}]\")\n",
        "            print(f\"     Los datos deber√≠an estar normalizados entre 0 y 1 (min-max) o -1 y 1 (z-score)\")\n",
        "\n",
        "    params_dict = collect_run_params()\n",
        "    params_path = OUTPUT_ROOT / \"run_params.json\"\n",
        "    with open(params_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(params_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Asegurar que el directorio de salida existe\n",
        "    OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    best_ckpt = OUTPUT_ROOT / \"ae_best.pt\"\n",
        "    curves_png = OUTPUT_ROOT / \"loss_curves.png\"\n",
        "\n",
        "    with mlflow.start_run(run_name=f\"train_{int(time.time())}\", experiment_id=EXPERIMENT_ID) as run:\n",
        "        run_id = run.info.run_id\n",
        "        print(\"MLflow run:\", run_id)\n",
        "        print(\"Usando dispositivo:\", local_device)\n",
        "\n",
        "        flat_params = flatten_dict(params_dict)\n",
        "        for k, v in flat_params.items():\n",
        "            if isinstance(v, (str, int, float, bool)):\n",
        "                mlflow.log_param(k, v)\n",
        "            else:\n",
        "                mlflow.log_param(k, str(v))\n",
        "        mlflow.log_artifact(str(params_path), artifact_path=\"config\")\n",
        "\n",
        "        train_losses: list[float] = []\n",
        "        val_losses: list[float] = []\n",
        "        best_val = float(\"inf\")\n",
        "\n",
        "        for epoch in range(1, TRAINING.epochs + 1):\n",
        "            # Llamar directamente a las funciones sin decoradores Prefect para evitar problemas de serializaci√≥n\n",
        "            tr_loss = train_epoch.fn(model, dl_train, optimizer)\n",
        "            va_loss = eval_epoch.fn(model, dl_val_n)\n",
        "\n",
        "            train_losses.append(tr_loss)\n",
        "            val_losses.append(va_loss)\n",
        "\n",
        "            mlflow.log_metrics(\n",
        "                {\n",
        "                    \"recon_mse_train\": float(tr_loss),\n",
        "                    \"recon_mse_val\": float(va_loss),\n",
        "                },\n",
        "                step=epoch,\n",
        "            )\n",
        "            print(f\"Epoch {epoch:03d}/{TRAINING.epochs} | train={tr_loss:.6f} | val={va_loss:.6f}\")\n",
        "            \n",
        "            # Verificar si hay NaN o Inf\n",
        "            if not (np.isfinite(tr_loss) and np.isfinite(va_loss)):\n",
        "                print(f\"‚ö†Ô∏è ADVERTENCIA: P√©rdidas no finitas en epoch {epoch} (train={tr_loss}, val={va_loss})\")\n",
        "                print(\"   Esto puede indicar problemas con los datos o el modelo.\")\n",
        "                # Continuar pero no guardar checkpoint si hay NaN\n",
        "                if np.isfinite(va_loss) and va_loss < best_val:\n",
        "                    best_val = va_loss\n",
        "                    torch.save(model.state_dict(), best_ckpt)\n",
        "                    mlflow.log_artifact(str(best_ckpt), artifact_path=\"checkpoints\")\n",
        "            elif va_loss < best_val:\n",
        "                best_val = va_loss\n",
        "                torch.save(model.state_dict(), best_ckpt)\n",
        "                mlflow.log_artifact(str(best_ckpt), artifact_path=\"checkpoints\")\n",
        "\n",
        "        # Curvas\n",
        "        plt.figure(figsize=(7, 4))\n",
        "        plt.plot(train_losses, label=\"train MSE\")\n",
        "        plt.plot(val_losses, label=\"val MSE\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Reconstr. MSE\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(curves_png, dpi=130)\n",
        "        plt.close()\n",
        "        mlflow.log_artifact(str(curves_png), artifact_path=\"plots\")\n",
        "\n",
        "        # Guardar mejor modelo en formato MLflow\n",
        "        if best_ckpt.exists():\n",
        "            best_state = torch.load(best_ckpt, map_location=\"cpu\")\n",
        "            model.load_state_dict(best_state)\n",
        "            model.eval()\n",
        "            mlflow.pytorch.log_model(model, artifact_path=\"pytorch_model\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è ADVERTENCIA: No se guard√≥ ning√∫n checkpoint (best_ckpt no existe)\")\n",
        "            print(\"   Esto puede deberse a que todas las p√©rdidas fueron NaN o no hubo mejora.\")\n",
        "            # Guardar el modelo actual de todas formas\n",
        "            model.eval()\n",
        "            mlflow.pytorch.log_model(model, artifact_path=\"pytorch_model\")\n",
        "\n",
        "    return {\n",
        "        \"run_id\": run_id,\n",
        "        \"best_val\": best_val,\n",
        "        \"best_ckpt\": str(best_ckpt),\n",
        "        \"curves_png\": str(curves_png),\n",
        "        \"device_used\": local_device,\n",
        "    }\n",
        "\n",
        "\n",
        "train_summary = train_flow()\n",
        "print(\"Resumen entrenamiento:\")\n",
        "pprint(train_summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluaci√≥n completa. Revisa MLflow para artefactos y m√©tricas.\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# üìà Evaluaci√≥n ‚Äî reconstrucci√≥n + detecci√≥n anomal√≠as\n",
        "# ========================================\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    average_precision_score,\n",
        "    balanced_accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    r2_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "\n",
        "\n",
        "def reconstruction_errors(model: AE1DCNN, loader: DataLoader) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    errs, xs, ys = [], [], []\n",
        "    model_device = next(model.parameters()).device\n",
        "    non_blocking = model_device.type == \"cuda\"\n",
        "    with torch.no_grad():\n",
        "        for xb in loader:\n",
        "            xb = xb.to(model_device, non_blocking=non_blocking)\n",
        "            recon = model(xb)\n",
        "            mse = torch.mean((recon - xb) ** 2, dim=(1, 2)).cpu().numpy()\n",
        "            errs.append(mse)\n",
        "            xs.append(xb.cpu().numpy())\n",
        "            ys.append(recon.cpu().numpy())\n",
        "    return np.concatenate(errs), np.concatenate(xs), np.concatenate(ys)\n",
        "\n",
        "\n",
        "def basic_metrics(x_true: np.ndarray, x_pred: np.ndarray) -> dict:\n",
        "    a = x_true.reshape(-1)\n",
        "    b = x_pred.reshape(-1)\n",
        "    mse = mean_squared_error(a, b)\n",
        "    return {\n",
        "        \"mae\": mean_absolute_error(a, b),\n",
        "        \"mse\": mse,\n",
        "        \"rmse\": float(np.sqrt(mse)),\n",
        "        \"r2\": r2_score(a, b),\n",
        "    }\n",
        "\n",
        "\n",
        "def pick_threshold(errs_norm: np.ndarray) -> tuple[float, str]:\n",
        "    thr_cfg = CONFIG.threshold\n",
        "    if thr_cfg.use_mean_std:\n",
        "        mu = float(np.mean(errs_norm))\n",
        "        sigma = float(np.std(errs_norm))\n",
        "        return mu + thr_cfg.k_std * sigma, f\"mean+{thr_cfg.k_std}*std\"\n",
        "    return float(np.percentile(errs_norm, thr_cfg.percentile)), f\"p{thr_cfg.percentile}\"\n",
        "\n",
        "\n",
        "def log_confusion_artifacts(cm: np.ndarray, labels: list[str], tag: str) -> None:\n",
        "    df = pd.DataFrame(\n",
        "        cm,\n",
        "        index=[f\"Real:{lbl}\" for lbl in labels],\n",
        "        columns=[f\"Pred:{lbl}\" for lbl in labels],\n",
        "    )\n",
        "    csv_path = OUTPUT_ROOT / f\"cm_{tag}.csv\"\n",
        "    png_path = OUTPUT_ROOT / f\"cm_{tag}.png\"\n",
        "    df.to_csv(csv_path)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)))\n",
        "    ax.set_xticklabels([f\"Pred:{lbl}\" for lbl in labels])\n",
        "    ax.set_yticklabels([f\"Real:{lbl}\" for lbl in labels])\n",
        "    ax.set_xlabel(\"Predicci√≥n\")\n",
        "    ax.set_ylabel(\"Real\")\n",
        "    ax.set_title(f\"Matriz de confusi√≥n ({tag})\")\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, f\"{cm[i, j]}\", ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(png_path, dpi=140)\n",
        "    plt.close(fig)\n",
        "\n",
        "    mlflow.log_artifact(str(csv_path), artifact_path=f\"eval_{tag}\")\n",
        "    mlflow.log_artifact(str(png_path), artifact_path=f\"eval_{tag}\")\n",
        "\n",
        "\n",
        "best_ckpt = OUTPUT_ROOT / \"ae_best.pt\"\n",
        "state_dict = torch.load(best_ckpt, map_location=DEVICE)\n",
        "MODEL.load_state_dict(state_dict)\n",
        "MODEL.to(DEVICE)\n",
        "MODEL.eval()\n",
        "\n",
        "with mlflow.start_run(run_name=\"eval\", experiment_id=EXPERIMENT_ID):\n",
        "    val_err_n, val_x_n, val_y_n = reconstruction_errors(MODEL, dl_val_n)\n",
        "    metrics_val = basic_metrics(val_x_n, val_y_n)\n",
        "    thr, thr_label = pick_threshold(val_err_n)\n",
        "\n",
        "    if dl_val_a:\n",
        "        val_err_a, val_x_a, val_y_a = reconstruction_errors(MODEL, dl_val_a)\n",
        "        y_true_val = np.concatenate([np.zeros_like(val_err_n), np.ones_like(val_err_a)])\n",
        "        y_score_val = np.concatenate([val_err_n, val_err_a])\n",
        "    else:\n",
        "        val_err_a = np.zeros((0,), dtype=np.float32)\n",
        "        y_true_val = np.zeros_like(val_err_n)\n",
        "        y_score_val = val_err_n\n",
        "\n",
        "    y_pred_val = (y_score_val > thr).astype(int)\n",
        "    cm_val = confusion_matrix(y_true_val, y_pred_val, labels=[0, 1])\n",
        "    tn_val, fp_val, fn_val, tp_val = cm_val.ravel()\n",
        "\n",
        "    def safe_metric(fn, *args, **kwargs):\n",
        "        try:\n",
        "            return float(fn(*args, **kwargs))\n",
        "        except Exception:\n",
        "            return float(\"nan\")\n",
        "\n",
        "    specificity_val = tn_val / max(1, tn_val + fp_val)\n",
        "\n",
        "    metrics_clf_val = {\n",
        "        \"accuracy\": accuracy_score(y_true_val, y_pred_val),\n",
        "        \"precision\": precision_score(y_true_val, y_pred_val, zero_division=0),\n",
        "        \"recall\": recall_score(y_true_val, y_pred_val, zero_division=0),\n",
        "        \"specificity\": specificity_val,\n",
        "        \"f1\": f1_score(y_true_val, y_pred_val, zero_division=0),\n",
        "        \"balanced_accuracy\": balanced_accuracy_score(y_true_val, y_pred_val),\n",
        "        \"auroc\": safe_metric(roc_auc_score, y_true_val, y_score_val),\n",
        "        \"auprc\": safe_metric(average_precision_score, y_true_val, y_score_val),\n",
        "    }\n",
        "\n",
        "    mlflow.log_param(\"threshold_value\", thr)\n",
        "    mlflow.log_param(\"threshold_strategy\", thr_label)\n",
        "    mlflow.log_metrics({f\"val_{k}\": float(v) for k, v in {**metrics_val, **metrics_clf_val}.items()})\n",
        "\n",
        "    log_confusion_artifacts(cm_val, [\"Normal\", \"An√≥malo\"], \"val\")\n",
        "    report_val = classification_report(y_true_val, y_pred_val, target_names=[\"normal\", \"anomalo\"], digits=4)\n",
        "    report_val_path = OUTPUT_ROOT / \"classification_report_val.txt\"\n",
        "    with open(report_val_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report_val)\n",
        "    mlflow.log_artifact(str(report_val_path), artifact_path=\"eval_val\")\n",
        "\n",
        "    test_err_n, test_x_n, test_y_n = reconstruction_errors(MODEL, dl_test_n)\n",
        "    metrics_test = basic_metrics(test_x_n, test_y_n)\n",
        "\n",
        "    if dl_test_a:\n",
        "        test_err_a, test_x_a, test_y_a = reconstruction_errors(MODEL, dl_test_a)\n",
        "        y_true_test = np.concatenate([np.zeros_like(test_err_n), np.ones_like(test_err_a)])\n",
        "        y_score_test = np.concatenate([test_err_n, test_err_a])\n",
        "    else:\n",
        "        y_true_test = np.zeros_like(test_err_n)\n",
        "        y_score_test = test_err_n\n",
        "\n",
        "    y_pred_test = (y_score_test > thr).astype(int)\n",
        "    cm_test = confusion_matrix(y_true_test, y_pred_test, labels=[0, 1])\n",
        "    tn_test, fp_test, fn_test, tp_test = cm_test.ravel()\n",
        "\n",
        "    metrics_clf_test = {\n",
        "        \"accuracy\": accuracy_score(y_true_test, y_pred_test),\n",
        "        \"precision\": precision_score(y_true_test, y_pred_test, zero_division=0),\n",
        "        \"recall\": recall_score(y_true_test, y_pred_test, zero_division=0),\n",
        "        \"specificity\": tn_test / max(1, tn_test + fp_test),\n",
        "        \"f1\": f1_score(y_true_test, y_pred_test, zero_division=0),\n",
        "        \"balanced_accuracy\": balanced_accuracy_score(y_true_test, y_pred_test),\n",
        "        \"auroc\": safe_metric(roc_auc_score, y_true_test, y_score_test),\n",
        "        \"auprc\": safe_metric(average_precision_score, y_true_test, y_score_test),\n",
        "    }\n",
        "\n",
        "    mlflow.log_metrics({f\"test_{k}\": float(v) for k, v in {**metrics_test, **metrics_clf_test}.items()})\n",
        "\n",
        "    log_confusion_artifacts(cm_test, [\"Normal\", \"An√≥malo\"], \"test\")\n",
        "    report_test = classification_report(y_true_test, y_pred_test, target_names=[\"normal\", \"anomalo\"], digits=4)\n",
        "    report_test_path = OUTPUT_ROOT / \"classification_report_test.txt\"\n",
        "    with open(report_test_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report_test)\n",
        "    mlflow.log_artifact(str(report_test_path), artifact_path=\"eval_test\")\n",
        "\n",
        "    # Ejemplos de reconstrucci√≥n\n",
        "    if test_x_n.shape[0] > 0:\n",
        "        leads = [\"II\", \"V1\", \"V5\"]\n",
        "        t_axis = np.arange(test_x_n.shape[2]) / 500.0\n",
        "        for i, lead in enumerate(leads):\n",
        "            fig, ax = plt.subplots(figsize=(10, 3))\n",
        "            ax.plot(t_axis, test_x_n[0, i], label=\"input\")\n",
        "            ax.plot(t_axis, test_y_n[0, i], label=\"recon\", alpha=0.85)\n",
        "            ax.set_title(f\"Reconstrucci√≥n Test ‚Äî {lead}\")\n",
        "            ax.set_xlabel(\"Tiempo (s)\")\n",
        "            ax.legend()\n",
        "            fig.tight_layout()\n",
        "            img_path = OUTPUT_ROOT / f\"recon_{lead}_test.png\"\n",
        "            fig.savefig(img_path, dpi=130)\n",
        "            plt.close(fig)\n",
        "            mlflow.log_artifact(str(img_path), artifact_path=\"recon_examples\")\n",
        "\n",
        "print(\"Evaluaci√≥n completa. Revisa MLflow para artefactos y m√©tricas.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Checklist final\n",
        "\n",
        "- Ajusta cualquier hiperpar√°metro en la celda de **Config** o edita `../config/ae1d_config.json`.\n",
        "- Tras instalar PyTorch cu128 por primera vez, **reinicia el kernel** para que tome las DLL.\n",
        "- Si el entrenamiento se ejecuta en CPU, revisa drivers CUDA / reinicia y vuelve a correr la celda de setup.\n",
        "- Los artefactos (checkpoints, curvas, reportes) quedan en `OUTPUT_ROOT` y en MLflow (`../mlflow_artifacts`).\n",
        "- Para ejecutar desde terminal: `prefect deployment run train_autoencoder_1d/train_flow` (opcional).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
