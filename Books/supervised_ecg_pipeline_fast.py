"""
Versión optimizada del pipeline con paralelización para mayor velocidad.

Este módulo añade funciones paralelizadas para acelerar el procesamiento.
"""

from __future__ import annotations

import multiprocessing as mp
from functools import partial
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

# Importar funciones base del pipeline original
import numpy as np
from supervised_ecg_pipeline import (
    BATCH_SIZE,
    MIMIC_ROOT,
    N_WORKERS,
    PTB_ROOT,
    TARGET_LEADS,
    EXPECTED_SAMPLES,
    PTB_REJECT_COLUMNS,
)


# =====================================================================================
# Funciones auxiliares para procesamiento paralelo
# =====================================================================================

def _process_ptbxl_single(args: Tuple) -> Optional[Dict]:
    """Procesa un único registro de PTB-XL (para multiprocessing)."""
    (ecg_id, row_dict, filename_hr, apply_quality_check, apply_notch, notch_freq, 
     normalize_method, reject_unvalidated) = args
    
    try:
        import pandas as pd
        import numpy as np
        from supervised_ecg_pipeline import (
            label_ptbxl_record,
            process_single_record,
            PTB_ROOT,
            PTB_REJECT_COLUMNS,
            get_sig_names,
            map_lead_names,
            TARGET_LEADS,
        )
        import wfdb
        
        row = pd.Series(row_dict)
        
        # 1. Etiquetar (ya fue pre-filtrado, pero verificar por seguridad)
        quality_cols = row[PTB_REJECT_COLUMNS] if all(c in row.index for c in PTB_REJECT_COLUMNS) else None
        label, reason = label_ptbxl_record(
            row["scp_codes"],
            quality_cols,
            reject_unvalidated=reject_unvalidated,
            validated_by_human=row.get("validated_by_human"),
            initial_autogenerated=row.get("initial_autogenerated_report"),
        )
        
        if label == -1:
            return None
        
        # 2. Cargar señal directamente desde el path (optimizado)
        try:
            record_path = PTB_ROOT / filename_hr
            # Cargar solo los campos necesarios (más rápido)
            signal, meta = wfdb.rdsamp(str(record_path), channels=None)
            
            # Extraer leads II, V1, V5
            sig_names = get_sig_names(meta)
            lead_mapping = map_lead_names(sig_names, TARGET_LEADS)
            indices = [lead_mapping[lead] for lead in TARGET_LEADS]
            signal = signal[:, indices].astype(np.float32)
            
            # Obtener frecuencia de muestreo (filename_hr es siempre 500 Hz)
            original_fs = 500  # Optimización: PTB-XL filename_hr siempre es 500 Hz
        except (KeyError, FileNotFoundError, ValueError, Exception):
            return None
        
        # 3. Procesar
        processed, proc_reason = process_single_record(
            signal,
            original_fs,
            apply_quality_check=apply_quality_check,
            apply_notch=apply_notch,
            notch_freq=notch_freq,
            normalize_method=normalize_method,
        )
        
        if processed is None:
            return None
        
        # 4. Retornar resultado
        return {
            "record_id": f"ptbxl_{ecg_id}",
            "source": "PTB-XL",
            "original_id": int(ecg_id),
            "label": int(label),
            "label_reason": reason,
            "shape": f"{processed.shape[0]},{processed.shape[1]}",
            "signal": processed,
        }
    
    except Exception as e:
        return None


def _process_mimic_single(args: Tuple) -> Optional[Dict]:
    """Procesa un único registro de MIMIC (para multiprocessing)."""
    (row_dict, record_path_str, apply_quality_check, apply_notch, 
     notch_freq, normalize_method) = args
    
    try:
        import pandas as pd
        import numpy as np
        from supervised_ecg_pipeline import (
            label_mimic_record,
            load_mimic_record,
            process_single_record,
            MIMIC_ROOT,
        )
        
        row = pd.Series(row_dict)
        subject_id = int(row["subject_id"])
        study_id = int(row["study_id"])
        
        # 1. Verificar path
        if not record_path_str:
            return None
        
        record_path = MIMIC_ROOT / record_path_str
        
        # 2. Etiquetar (ya fue pre-filtrado, pero verificar por seguridad)
        report_cols = {k: v for k, v in row_dict.items() if k.startswith("report_")}
        if not report_cols:
            return None
        
        from supervised_ecg_pipeline import label_mimic_record
        reports_series = pd.Series(report_cols)
        label, reason = label_mimic_record(reports_series)
        
        if label == -1:
            return None
        
        # 3. Cargar señal
        try:
            signal, meta = load_mimic_record(record_path)
            original_fs = meta.get("fs", 250) if isinstance(meta, dict) else getattr(meta, "fs", 250)
        except (KeyError, FileNotFoundError, ValueError, Exception):
            return None
        
        # 4. Procesar
        processed, proc_reason = process_single_record(
            signal,
            original_fs,
            apply_quality_check=apply_quality_check,
            apply_notch=apply_notch,
            notch_freq=notch_freq,
            normalize_method=normalize_method,
        )
        
        if processed is None:
            return None
        
        # 5. Retornar resultado
        return {
            "record_id": f"mimic_{subject_id}_{study_id}",
            "source": "MIMIC",
            "subject_id": subject_id,
            "study_id": study_id,
            "label": int(label),
            "label_reason": reason,
            "shape": f"{processed.shape[0]},{processed.shape[1]}",
            "signal": processed,
        }
    
    except Exception:
        return None


# =====================================================================================
# Funciones optimizadas de procesamiento
# =====================================================================================

def process_ptbxl_dataset_fast(
    overwrite: bool = False,
    apply_quality_check: bool = True,
    apply_notch: bool = True,
    notch_freq: float = 50.0,
    normalize_method: str = "minmax",
    reject_unvalidated: bool = False,
    max_records: Optional[int] = None,
    n_workers: int = N_WORKERS,
    verbose: bool = True,
    prefilter_labels: bool = False,  # Deshabilitado por defecto: procesar todos directamente
) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:
    """
    Versión optimizada y paralelizada para procesar PTB-XL.
    
    Optimizaciones:
    - Pre-filtra por etiquetas ANTES de cargar señales (muy rápido)
    - Procesamiento paralelo
    - Retorna directamente arrays numpy
    
    Args:
        prefilter_labels: Si True, etiqueta primero y solo carga señales válidas (mucho más rápido)
    """
    import ast
    from supervised_ecg_pipeline import label_ptbxl_record, PTB_REJECT_COLUMNS
    
    db_csv = PTB_ROOT / "ptbxl_database.csv"
    if not db_csv.exists():
        raise FileNotFoundError(f"No se encontró {db_csv}")
    
    metadata = pd.read_csv(db_csv, index_col="ecg_id")
    metadata["scp_codes"] = metadata["scp_codes"].apply(ast.literal_eval)
    
    if max_records:
        metadata = metadata.iloc[:max_records]
    
    total = len(metadata)
    if verbose:
        print(f"[PTB-XL] Procesando {total} registros directamente en paralelo ({n_workers} workers)...")
    
    # Preparar argumentos para paralelización
    # Convertir a lista de tuplas con información necesaria
    tasks = []
    for ecg_id, row in metadata.iterrows():
        row_dict = row.to_dict()
        filename_hr = row.get("filename_hr", "")
        tasks.append((
            ecg_id,
            row_dict,
            filename_hr,
            apply_quality_check,
            apply_notch,
            notch_freq,
            normalize_method,
            reject_unvalidated,
        ))
    
    processed_results = []
    skipped_labels = {"rejected": 0, "quality": 0, "processing_error": 0, "missing_leads": 0}
    
    # Procesar TODO de una vez usando un solo pool (máxima velocidad)
    # El pool maneja internamente la distribución entre workers
    if verbose:
        print(f"  Iniciando procesamiento paralelo de {total} registros...")
    
    with mp.Pool(processes=n_workers) as pool:
        # Usar imap_unordered para procesar todo y mostrar progreso
        batch_results = list(pool.imap_unordered(_process_ptbxl_single, tasks, chunksize=100))
    
    # Filtrar resultados válidos
    for result in batch_results:
        if result is not None:
            processed_results.append(result)
        else:
            skipped_labels["rejected"] += 1
    
    if verbose:
        print(f"  ✓ Completado: {len(processed_results)} válidos, {sum(skipped_labels.values())} rechazados")
    
    if verbose:
        print(f"\n[PTB-XL] Completado:")
        print(f"  Procesados exitosamente: {len(processed_results)}")
        print(f"  Rechazados: {sum(skipped_labels.values())}")
    
    if len(processed_results) == 0:
        return np.array([]), np.array([]), pd.DataFrame()
    
    if verbose:
        print(f"  Construyendo arrays finales de {len(processed_results)} registros...")
    
    # Separar señales y metadatos (optimizado para memoria)
    # Extraer señales primero
    if verbose:
        print(f"  Extrayendo señales...")
    signals_list = [r["signal"] for r in processed_results]
    signals = np.stack(signals_list, axis=0)
    del signals_list  # Liberar memoria
    if verbose:
        print(f"  ✓ Señales extraídas: shape {signals.shape}")
    
    # Extraer labels
    if verbose:
        print(f"  Extrayendo labels...")
    labels = np.array([r["label"] for r in processed_results])
    if verbose:
        print(f"  ✓ Labels extraídos: {len(labels)} labels")
    
    # Crear DataFrame sin señales (ahorrar memoria)
    if verbose:
        print(f"  Creando metadatos...")
    metadata_df = pd.DataFrame([
        {k: v for k, v in r.items() if k != "signal"}
        for r in processed_results
    ])
    if verbose:
        print(f"  ✓ Metadatos creados: {len(metadata_df)} registros")
    
    # Liberar memoria de processed_results
    del processed_results
    
    if verbose:
        print(f"  ✓ Arrays finales listos: signals={signals.shape}, labels={labels.shape}")
    
    return signals, labels, metadata_df


def process_mimic_dataset_fast(
    overwrite: bool = False,
    apply_quality_check: bool = True,
    apply_notch: bool = True,
    notch_freq: float = 50.0,
    normalize_method: str = "minmax",
    max_records: Optional[int] = None,
    n_workers: int = N_WORKERS,
    verbose: bool = True,
    prefilter_labels: bool = False,  # Deshabilitado por defecto: procesar todos directamente
) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:
    """
    Versión optimizada y paralelizada para procesar MIMIC-IV-ECG.
    
    Retorna directamente arrays numpy en lugar de DataFrame con señales embebidas.
    """
    machine_csv = MIMIC_ROOT / "machine_measurements.csv"
    record_list_csv = MIMIC_ROOT / "record_list.csv"
    
    if not machine_csv.exists():
        raise FileNotFoundError(f"No se encontró {machine_csv}")
    if not record_list_csv.exists():
        raise FileNotFoundError(f"No se encontró {record_list_csv}")
    
    record_df = pd.read_csv(record_list_csv)
    path_map = {
        (int(row.subject_id), int(row.study_id)): row.path
        for row in record_df.itertuples(index=False)
    }
    
    if verbose:
        print(f"[MIMIC] Leyendo registros y preparando procesamiento paralelo ({n_workers} workers)...")
    
    # Leer CSV completo o en chunks y preparar TODOS para procesamiento directo
    chunksize = 10000
    valid_rows = []
    total_processed = 0
    
    for chunk in pd.read_csv(machine_csv, chunksize=chunksize):
        for idx, row in chunk.iterrows():
            if max_records and total_processed >= max_records:
                break
            
            subject_id = int(row["subject_id"])
            study_id = int(row["study_id"])
            key = (subject_id, study_id)
            
            # Solo verificar si tiene path (sin pre-filtrar por etiquetas)
            if key not in path_map:
                total_processed += 1
                continue
            
            record_path_str = path_map[key]
            valid_rows.append((row.to_dict(), record_path_str))
            total_processed += 1
        if max_records and total_processed >= max_records:
            break
    
    total = len(valid_rows)
    if verbose:
        print(f"[MIMIC] Procesando {total} registros directamente en paralelo...")
    
    # Crear lista de tareas con todos los parámetros
    tasks = [
        (
            row_dict,
            record_path_str,
            apply_quality_check,
            apply_notch,
            notch_freq,
            normalize_method,
        )
        for row_dict, record_path_str in valid_rows
    ]
    
    processed_results = []
    skipped_labels = {"rejected": 0, "quality": 0, "processing_error": 0, "missing_leads": 0, "missing_path": 0}
    
    # Procesar TODO de una vez usando un solo pool (máxima velocidad)
    # El pool maneja internamente la distribución entre workers
    if verbose:
        print(f"  Iniciando procesamiento paralelo de {total} registros...")
    
    with mp.Pool(processes=n_workers) as pool:
        # Usar imap_unordered para procesar todo y mostrar progreso
        # chunksize=100: cada worker recibe 100 tareas a la vez
        batch_results = list(pool.imap_unordered(_process_mimic_single, tasks, chunksize=100))
    
    # Filtrar resultados válidos
    for result in batch_results:
        if result is not None:
            processed_results.append(result)
        else:
            skipped_labels["rejected"] += 1
    
    if verbose:
        print(f"  ✓ Completado: {len(processed_results)} válidos, {sum(skipped_labels.values())} rechazados")
    
    if verbose:
        print(f"\n[MIMIC] Completado:")
        print(f"  Procesados exitosamente: {len(processed_results)}")
        print(f"  Rechazados: {sum(skipped_labels.values())}")
    
    if len(processed_results) == 0:
        return np.array([]), np.array([]), pd.DataFrame()
    
    if verbose:
        print(f"  Construyendo arrays finales de {len(processed_results)} registros...")
    
    # Separar señales y metadatos (optimizado para memoria)
    # Extraer señales primero
    if verbose:
        print(f"  Extrayendo señales...")
    signals_list = [r["signal"] for r in processed_results]
    signals = np.stack(signals_list, axis=0)
    del signals_list  # Liberar memoria
    if verbose:
        print(f"  ✓ Señales extraídas: shape {signals.shape}")
    
    # Extraer labels
    if verbose:
        print(f"  Extrayendo labels...")
    labels = np.array([r["label"] for r in processed_results])
    if verbose:
        print(f"  ✓ Labels extraídos: {len(labels)} labels")
    
    # Crear DataFrame sin señales (ahorrar memoria)
    if verbose:
        print(f"  Creando metadatos...")
    metadata_df = pd.DataFrame([
        {k: v for k, v in r.items() if k != "signal"}
        for r in processed_results
    ])
    if verbose:
        print(f"  ✓ Metadatos creados: {len(metadata_df)} registros")
    
    # Liberar memoria de processed_results
    del processed_results
    
    if verbose:
        print(f"  ✓ Arrays finales listos: signals={signals.shape}, labels={labels.shape}")
    
    return signals, labels, metadata_df

